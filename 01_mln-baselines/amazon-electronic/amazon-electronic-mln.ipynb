{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Amazon Electronic MLN Recommendation via PyTorch\n",
    "\n",
    "adapted from https://github.com/fastai/fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "figure_path = '/home/weiss/git/thesis/doc/figures/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "\n",
    "RANDOM_STATE = 2021\n",
    "set_random_seed(RANDOM_STATE)\n",
    "\n",
    "DIR = '/home/weiss/rs_data/amazon-electronic-product-recommendation/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# pick one of the available folders\n",
    "ratings = pd.read_pickle(DIR+'amazon-electronic-ratings.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "            user_id     item_id  rating\n222  A3MV1KKHX51FYT  0380709473     4.0\n306  A2I2KPNJDQ9SL0  0511189877     5.0\n380  A2DFM26VLNVYNY  0511189877     5.0\n649  A34GB2ZA1JLGND  0594033926     5.0\n743   AT09WGFUM934H  0594481902     3.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>222</th>\n      <td>A3MV1KKHX51FYT</td>\n      <td>0380709473</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>306</th>\n      <td>A2I2KPNJDQ9SL0</td>\n      <td>0511189877</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>A2DFM26VLNVYNY</td>\n      <td>0511189877</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>649</th>\n      <td>A34GB2ZA1JLGND</td>\n      <td>0594033926</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>743</th>\n      <td>AT09WGFUM934H</td>\n      <td>0594481902</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 5. 3. 2. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(ratings['rating'].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 5143 users, 115961 items\n",
      "Dataset shape: (394059, 2)\n",
      "Target shape: (394059,)\n",
      "     user_id  item_id\n",
      "222        0        0\n",
      "306        1        1\n",
      "380        2        1\n",
      "649        3        2\n",
      "743        4        3\n",
      "222    4\n",
      "306    5\n",
      "380    5\n",
      "649    5\n",
      "743    3\n",
      "Name: rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(ratings, top=None):\n",
    "    unique_users = ratings.user_id.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.user_id.map(user_to_index)\n",
    "\n",
    "    unique_items = ratings.item_id.unique()\n",
    "    item_to_index = {old: new for new, old in enumerate(unique_items)}\n",
    "    new_items = ratings.item_id.map(item_to_index)\n",
    "\n",
    "    n_users = unique_users.shape[0]\n",
    "    n_items = unique_items.shape[0]\n",
    "\n",
    "    X = pd.DataFrame({'user_id': new_users, 'item_id': new_items})\n",
    "    y = ratings['rating'].astype(np.int)\n",
    "    return (n_users, n_items), (X, y), (user_to_index, item_to_index)\n",
    "\n",
    "(n, m), (X, y), _ = create_dataset(ratings)\n",
    "print(f'Embeddings: {n} users, {m} items')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')\n",
    "print(X.head())\n",
    "print(y.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=RANDOM_STATE)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, random_state=RANDOM_STATE)\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid), 'test': (X_test, y_test)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_valid), 'test': len(X_test)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "class RatingsIterator:\n",
    "\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in RatingsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "class RecommenderNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates dense MLN with embedding layers.\n",
    "\n",
    "    Args:\n",
    "        n_users:\n",
    "            Number of unique users in the dataset.\n",
    "\n",
    "        n_items:\n",
    "            Number of unique items in the dataset.\n",
    "\n",
    "        n_factors:\n",
    "            Number of columns in the embeddings matrix.\n",
    "\n",
    "        embedding_dropout:\n",
    "            Dropout rate to apply right after embeddings layer.\n",
    "\n",
    "        hidden:\n",
    "            A single integer or a list of integers defining the number of\n",
    "            units in hidden layer(s).\n",
    "\n",
    "        dropouts:\n",
    "            A single integer or a list of integers defining the dropout\n",
    "            layers rates applied right after each of hidden layers.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_items,\n",
    "                 n_factors=50, embedding_dropout=0.3,\n",
    "                 hidden=10, dropouts=0.3):\n",
    "\n",
    "        super().__init__()\n",
    "        hidden = get_list(hidden)\n",
    "        dropouts = get_list(dropouts)\n",
    "        n_last = hidden[-1]\n",
    "\n",
    "        def gen_layers(n_in):\n",
    "            \"\"\"\n",
    "            A generator that yields a sequence of hidden layers and\n",
    "            their activations/dropouts.\n",
    "\n",
    "            Note that the function captures `hidden` and `dropouts`\n",
    "            values from the outer scope.\n",
    "            \"\"\"\n",
    "            nonlocal hidden, dropouts\n",
    "            assert len(dropouts) <= len(hidden)\n",
    "\n",
    "            for n_out, rate in zip_longest(hidden, dropouts):\n",
    "                yield nn.Linear(n_in, n_out)\n",
    "                yield self.activation\n",
    "                if rate is not None and rate > 0.:\n",
    "                    yield nn.Dropout(rate)\n",
    "                n_in = n_out\n",
    "\n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        self.m = nn.Embedding(n_items, n_factors)\n",
    "        self.drop = nn.Dropout(embedding_dropout)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.hidden = nn.Sequential(*list(gen_layers(n_factors * 2)))\n",
    "        self.fc = nn.Linear(n_last, 1)\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, users, items, minmax=None):\n",
    "        features = torch.cat([self.u(users), self.m(items)], dim=1)\n",
    "        x = self.drop(features)\n",
    "        x = self.hidden(x)\n",
    "        out = self.activation(self.fc(x))\n",
    "        #out = self.relu(self.fc(x))  # relu delivers worse rmse\n",
    "        if minmax is not None:\n",
    "            min_rating, max_rating = minmax\n",
    "            out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n",
    "        return out\n",
    "\n",
    "    def _init(self):\n",
    "        \"\"\"\n",
    "        Setup embeddings and hidden layers with reasonable initial values.\n",
    "        \"\"\"\n",
    "\n",
    "        def init(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        self.u.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.m.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.hidden.apply(init)\n",
    "        init(self.fc)\n",
    "\n",
    "\n",
    "def get_list(n):\n",
    "    if isinstance(n, (int, float)):\n",
    "        return [n]\n",
    "    elif hasattr(n, '__iter__'):\n",
    "        return list(n)\n",
    "    raise TypeError('layers configuraiton should be a single number or a list of numbers')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "(1.0, 5.0)"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax = float(ratings.rating.min()), float(ratings.rating.max())\n",
    "minmax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "#small\n",
    "\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_items=m,\n",
    "    n_factors=20, hidden=[10, 10],\n",
    "    embedding_dropout=0.1, dropouts=[0.1, 0.1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-6\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 1.2662 - val: 1.1756\n",
      "loss improvement on epoch: 2\n",
      "[002/300] train: 1.1740 - val: 1.1586\n",
      "loss improvement on epoch: 3\n",
      "[003/300] train: 1.1426 - val: 1.1306\n",
      "loss improvement on epoch: 4\n",
      "[004/300] train: 1.0850 - val: 1.0776\n",
      "loss improvement on epoch: 5\n",
      "[005/300] train: 0.9907 - val: 1.0272\n",
      "loss improvement on epoch: 6\n",
      "[006/300] train: 0.8877 - val: 1.0098\n",
      "[007/300] train: 0.8012 - val: 1.0232\n",
      "[008/300] train: 0.7338 - val: 1.0486\n",
      "[009/300] train: 0.6840 - val: 1.0752\n",
      "[010/300] train: 0.6512 - val: 1.0944\n",
      "[011/300] train: 0.6242 - val: 1.1128\n",
      "[012/300] train: 0.6065 - val: 1.1241\n",
      "[013/300] train: 0.5917 - val: 1.1342\n",
      "[014/300] train: 0.5803 - val: 1.1400\n",
      "[015/300] train: 0.5708 - val: 1.1421\n",
      "[016/300] train: 0.5613 - val: 1.1479\n",
      "[017/300] train: 0.5550 - val: 1.1607\n",
      "[018/300] train: 0.5471 - val: 1.1612\n",
      "[019/300] train: 0.5420 - val: 1.1717\n",
      "[020/300] train: 0.5370 - val: 1.1735\n",
      "[021/300] train: 0.5321 - val: 1.1799\n",
      "[022/300] train: 0.5286 - val: 1.1783\n",
      "[023/300] train: 0.5236 - val: 1.1859\n",
      "[024/300] train: 0.5204 - val: 1.1882\n",
      "[025/300] train: 0.5181 - val: 1.1916\n",
      "[026/300] train: 0.5150 - val: 1.1942\n",
      "[027/300] train: 0.5129 - val: 1.1971\n",
      "[028/300] train: 0.5123 - val: 1.2041\n",
      "[029/300] train: 0.5081 - val: 1.2077\n",
      "[030/300] train: 0.5066 - val: 1.2096\n",
      "[031/300] train: 0.5042 - val: 1.2059\n",
      "[032/300] train: 0.5031 - val: 1.2119\n",
      "[033/300] train: 0.5010 - val: 1.2184\n",
      "[034/300] train: 0.4999 - val: 1.2170\n",
      "[035/300] train: 0.4973 - val: 1.2214\n",
      "[036/300] train: 0.4969 - val: 1.2254\n",
      "[037/300] train: 0.4957 - val: 1.2217\n",
      "[038/300] train: 0.4941 - val: 1.2302\n",
      "[039/300] train: 0.4932 - val: 1.2292\n",
      "[040/300] train: 0.4927 - val: 1.2333\n",
      "[041/300] train: 0.4907 - val: 1.2324\n",
      "[042/300] train: 0.4881 - val: 1.2343\n",
      "[043/300] train: 0.4889 - val: 1.2375\n",
      "[044/300] train: 0.4884 - val: 1.2314\n",
      "[045/300] train: 0.4864 - val: 1.2397\n",
      "[046/300] train: 0.4857 - val: 1.2383\n",
      "[047/300] train: 0.4843 - val: 1.2387\n",
      "[048/300] train: 0.4840 - val: 1.2363\n",
      "[049/300] train: 0.4836 - val: 1.2360\n",
      "[050/300] train: 0.4816 - val: 1.2418\n",
      "[051/300] train: 0.4813 - val: 1.2412\n",
      "[052/300] train: 0.4804 - val: 1.2445\n",
      "[053/300] train: 0.4788 - val: 1.2455\n",
      "[054/300] train: 0.4796 - val: 1.2435\n",
      "[055/300] train: 0.4787 - val: 1.2503\n",
      "[056/300] train: 0.4767 - val: 1.2521\n",
      "[057/300] train: 0.4772 - val: 1.2560\n",
      "[058/300] train: 0.4754 - val: 1.2514\n",
      "[059/300] train: 0.4742 - val: 1.2532\n",
      "[060/300] train: 0.4741 - val: 1.2514\n",
      "[061/300] train: 0.4722 - val: 1.2514\n",
      "[062/300] train: 0.4711 - val: 1.2540\n",
      "[063/300] train: 0.4699 - val: 1.2551\n",
      "[064/300] train: 0.4686 - val: 1.2581\n",
      "[065/300] train: 0.4690 - val: 1.2562\n",
      "[066/300] train: 0.4665 - val: 1.2603\n",
      "[067/300] train: 0.4655 - val: 1.2577\n",
      "[068/300] train: 0.4639 - val: 1.2589\n",
      "[069/300] train: 0.4622 - val: 1.2631\n",
      "[070/300] train: 0.4621 - val: 1.2586\n",
      "[071/300] train: 0.4611 - val: 1.2638\n",
      "[072/300] train: 0.4572 - val: 1.2621\n",
      "[073/300] train: 0.4549 - val: 1.2630\n",
      "[074/300] train: 0.4530 - val: 1.2650\n",
      "[075/300] train: 0.4522 - val: 1.2675\n",
      "[076/300] train: 0.4493 - val: 1.2620\n",
      "[077/300] train: 0.4466 - val: 1.2742\n",
      "[078/300] train: 0.4443 - val: 1.2705\n",
      "[079/300] train: 0.4428 - val: 1.2789\n",
      "[080/300] train: 0.4408 - val: 1.2733\n",
      "[081/300] train: 0.4365 - val: 1.2827\n",
      "[082/300] train: 0.4346 - val: 1.2834\n",
      "[083/300] train: 0.4324 - val: 1.2843\n",
      "[084/300] train: 0.4301 - val: 1.2923\n",
      "[085/300] train: 0.4268 - val: 1.2886\n",
      "[086/300] train: 0.4229 - val: 1.2969\n",
      "[087/300] train: 0.4212 - val: 1.3038\n",
      "[088/300] train: 0.4186 - val: 1.2997\n",
      "[089/300] train: 0.4148 - val: 1.3065\n",
      "[090/300] train: 0.4113 - val: 1.3132\n",
      "[091/300] train: 0.4095 - val: 1.3178\n",
      "[092/300] train: 0.4074 - val: 1.3122\n",
      "[093/300] train: 0.4038 - val: 1.3193\n",
      "[094/300] train: 0.3998 - val: 1.3235\n",
      "[095/300] train: 0.3975 - val: 1.3202\n",
      "[096/300] train: 0.3943 - val: 1.3340\n",
      "[097/300] train: 0.3924 - val: 1.3277\n",
      "[098/300] train: 0.3893 - val: 1.3337\n",
      "[099/300] train: 0.3866 - val: 1.3356\n",
      "[100/300] train: 0.3849 - val: 1.3404\n",
      "[101/300] train: 0.3810 - val: 1.3416\n",
      "[102/300] train: 0.3791 - val: 1.3528\n",
      "[103/300] train: 0.3753 - val: 1.3440\n",
      "[104/300] train: 0.3745 - val: 1.3574\n",
      "[105/300] train: 0.3707 - val: 1.3459\n",
      "[106/300] train: 0.3699 - val: 1.3562\n",
      "[107/300] train: 0.3660 - val: 1.3560\n",
      "[108/300] train: 0.3645 - val: 1.3587\n",
      "[109/300] train: 0.3636 - val: 1.3568\n",
      "[110/300] train: 0.3601 - val: 1.3644\n",
      "[111/300] train: 0.3584 - val: 1.3626\n",
      "[112/300] train: 0.3553 - val: 1.3677\n",
      "[113/300] train: 0.3552 - val: 1.3706\n",
      "[114/300] train: 0.3537 - val: 1.3774\n",
      "[115/300] train: 0.3521 - val: 1.3712\n",
      "[116/300] train: 0.3489 - val: 1.3760\n",
      "[117/300] train: 0.3482 - val: 1.3757\n",
      "[118/300] train: 0.3457 - val: 1.3820\n",
      "[119/300] train: 0.3444 - val: 1.3765\n",
      "[120/300] train: 0.3430 - val: 1.3819\n",
      "[121/300] train: 0.3425 - val: 1.3857\n",
      "[122/300] train: 0.3414 - val: 1.3833\n",
      "[123/300] train: 0.3402 - val: 1.3883\n",
      "[124/300] train: 0.3381 - val: 1.3803\n",
      "[125/300] train: 0.3377 - val: 1.3817\n",
      "[126/300] train: 0.3370 - val: 1.3906\n",
      "[127/300] train: 0.3352 - val: 1.3904\n",
      "[128/300] train: 0.3350 - val: 1.3834\n",
      "[129/300] train: 0.3329 - val: 1.3903\n",
      "[130/300] train: 0.3324 - val: 1.3911\n",
      "[131/300] train: 0.3324 - val: 1.3938\n",
      "[132/300] train: 0.3306 - val: 1.3961\n",
      "[133/300] train: 0.3294 - val: 1.3942\n",
      "[134/300] train: 0.3275 - val: 1.3949\n",
      "[135/300] train: 0.3281 - val: 1.3924\n",
      "[136/300] train: 0.3265 - val: 1.3919\n",
      "[137/300] train: 0.3261 - val: 1.3975\n",
      "[138/300] train: 0.3249 - val: 1.3961\n",
      "[139/300] train: 0.3254 - val: 1.3959\n",
      "[140/300] train: 0.3242 - val: 1.3953\n",
      "[141/300] train: 0.3235 - val: 1.3968\n",
      "[142/300] train: 0.3225 - val: 1.4034\n",
      "[143/300] train: 0.3226 - val: 1.3958\n",
      "[144/300] train: 0.3220 - val: 1.3966\n",
      "[145/300] train: 0.3207 - val: 1.4013\n",
      "[146/300] train: 0.3195 - val: 1.3982\n",
      "[147/300] train: 0.3199 - val: 1.3975\n",
      "[148/300] train: 0.3178 - val: 1.3936\n",
      "[149/300] train: 0.3177 - val: 1.4028\n",
      "[150/300] train: 0.3174 - val: 1.3954\n",
      "[151/300] train: 0.3160 - val: 1.3973\n",
      "[152/300] train: 0.3150 - val: 1.4050\n",
      "[153/300] train: 0.3160 - val: 1.3963\n",
      "[154/300] train: 0.3134 - val: 1.3984\n",
      "[155/300] train: 0.3143 - val: 1.4036\n",
      "[156/300] train: 0.3137 - val: 1.3999\n",
      "[157/300] train: 0.3109 - val: 1.4057\n",
      "[158/300] train: 0.3114 - val: 1.4059\n",
      "[159/300] train: 0.3104 - val: 1.3939\n",
      "[160/300] train: 0.3098 - val: 1.4083\n",
      "[161/300] train: 0.3094 - val: 1.3962\n",
      "[162/300] train: 0.3102 - val: 1.4042\n",
      "[163/300] train: 0.3091 - val: 1.3996\n",
      "[164/300] train: 0.3075 - val: 1.3995\n",
      "[165/300] train: 0.3071 - val: 1.4020\n",
      "[166/300] train: 0.3064 - val: 1.4030\n",
      "[167/300] train: 0.3065 - val: 1.3973\n",
      "[168/300] train: 0.3059 - val: 1.4082\n",
      "[169/300] train: 0.3045 - val: 1.4051\n",
      "[170/300] train: 0.3048 - val: 1.4031\n",
      "[171/300] train: 0.3030 - val: 1.3962\n",
      "[172/300] train: 0.3020 - val: 1.4099\n",
      "[173/300] train: 0.3037 - val: 1.4053\n",
      "[174/300] train: 0.3017 - val: 1.4009\n",
      "[175/300] train: 0.3015 - val: 1.4020\n",
      "[176/300] train: 0.3009 - val: 1.4039\n",
      "[177/300] train: 0.3015 - val: 1.4001\n",
      "[178/300] train: 0.3002 - val: 1.4033\n",
      "[179/300] train: 0.2993 - val: 1.3985\n",
      "[180/300] train: 0.2970 - val: 1.4000\n",
      "[181/300] train: 0.2982 - val: 1.3971\n",
      "[182/300] train: 0.2958 - val: 1.3980\n",
      "[183/300] train: 0.2982 - val: 1.3977\n",
      "[184/300] train: 0.2957 - val: 1.4024\n",
      "[185/300] train: 0.2943 - val: 1.4016\n",
      "[186/300] train: 0.2951 - val: 1.4019\n",
      "[187/300] train: 0.2951 - val: 1.3962\n",
      "[188/300] train: 0.2956 - val: 1.4096\n",
      "[189/300] train: 0.2940 - val: 1.4037\n",
      "[190/300] train: 0.2926 - val: 1.4057\n",
      "[191/300] train: 0.2937 - val: 1.4032\n",
      "[192/300] train: 0.2921 - val: 1.3943\n",
      "[193/300] train: 0.2921 - val: 1.3992\n",
      "[194/300] train: 0.2914 - val: 1.3987\n",
      "[195/300] train: 0.2919 - val: 1.4073\n",
      "[196/300] train: 0.2905 - val: 1.4004\n",
      "[197/300] train: 0.2912 - val: 1.4016\n",
      "[198/300] train: 0.2889 - val: 1.3972\n",
      "[199/300] train: 0.2885 - val: 1.4063\n",
      "[200/300] train: 0.2888 - val: 1.4063\n",
      "[201/300] train: 0.2871 - val: 1.4037\n",
      "[202/300] train: 0.2885 - val: 1.4031\n",
      "[203/300] train: 0.2876 - val: 1.4014\n",
      "[204/300] train: 0.2875 - val: 1.4038\n",
      "[205/300] train: 0.2860 - val: 1.4008\n",
      "[206/300] train: 0.2860 - val: 1.3951\n",
      "[207/300] train: 0.2864 - val: 1.4010\n",
      "[208/300] train: 0.2855 - val: 1.3994\n",
      "[209/300] train: 0.2858 - val: 1.4065\n",
      "[210/300] train: 0.2846 - val: 1.4079\n",
      "[211/300] train: 0.2835 - val: 1.3977\n",
      "[212/300] train: 0.2845 - val: 1.3989\n",
      "[213/300] train: 0.2825 - val: 1.4064\n",
      "[214/300] train: 0.2828 - val: 1.3996\n",
      "[215/300] train: 0.2833 - val: 1.4032\n",
      "[216/300] train: 0.2824 - val: 1.3971\n",
      "[217/300] train: 0.2812 - val: 1.4034\n",
      "[218/300] train: 0.2818 - val: 1.4001\n",
      "[219/300] train: 0.2826 - val: 1.4035\n",
      "[220/300] train: 0.2816 - val: 1.3977\n",
      "[221/300] train: 0.2821 - val: 1.4080\n",
      "[222/300] train: 0.2801 - val: 1.4052\n",
      "[223/300] train: 0.2799 - val: 1.4012\n",
      "[224/300] train: 0.2810 - val: 1.3949\n",
      "[225/300] train: 0.2800 - val: 1.4065\n",
      "[226/300] train: 0.2782 - val: 1.4024\n",
      "[227/300] train: 0.2775 - val: 1.4016\n",
      "[228/300] train: 0.2782 - val: 1.4061\n",
      "[229/300] train: 0.2779 - val: 1.3958\n",
      "[230/300] train: 0.2781 - val: 1.4016\n",
      "[231/300] train: 0.2781 - val: 1.3993\n",
      "[232/300] train: 0.2762 - val: 1.4018\n",
      "[233/300] train: 0.2776 - val: 1.4118\n",
      "[234/300] train: 0.2772 - val: 1.3979\n",
      "[235/300] train: 0.2764 - val: 1.4020\n",
      "[236/300] train: 0.2772 - val: 1.4073\n",
      "[237/300] train: 0.2755 - val: 1.4034\n",
      "[238/300] train: 0.2743 - val: 1.4071\n",
      "[239/300] train: 0.2750 - val: 1.4035\n",
      "[240/300] train: 0.2745 - val: 1.3988\n",
      "[241/300] train: 0.2748 - val: 1.4000\n",
      "[242/300] train: 0.2742 - val: 1.4052\n",
      "[243/300] train: 0.2735 - val: 1.4150\n",
      "[244/300] train: 0.2725 - val: 1.4078\n",
      "[245/300] train: 0.2732 - val: 1.4040\n",
      "[246/300] train: 0.2714 - val: 1.4075\n",
      "[247/300] train: 0.2736 - val: 1.4002\n",
      "[248/300] train: 0.2719 - val: 1.4057\n",
      "[249/300] train: 0.2718 - val: 1.4060\n",
      "[250/300] train: 0.2713 - val: 1.4053\n",
      "[251/300] train: 0.2713 - val: 1.4003\n",
      "[252/300] train: 0.2718 - val: 1.4072\n",
      "[253/300] train: 0.2718 - val: 1.3996\n",
      "[254/300] train: 0.2721 - val: 1.3987\n",
      "[255/300] train: 0.2701 - val: 1.4029\n",
      "[256/300] train: 0.2694 - val: 1.4069\n",
      "[257/300] train: 0.2689 - val: 1.4101\n",
      "[258/300] train: 0.2700 - val: 1.4043\n",
      "[259/300] train: 0.2702 - val: 1.4067\n",
      "[260/300] train: 0.2702 - val: 1.4142\n",
      "[261/300] train: 0.2699 - val: 1.4066\n",
      "[262/300] train: 0.2707 - val: 1.4138\n",
      "[263/300] train: 0.2694 - val: 1.4052\n",
      "[264/300] train: 0.2702 - val: 1.4083\n",
      "[265/300] train: 0.2686 - val: 1.4029\n",
      "[266/300] train: 0.2677 - val: 1.4034\n",
      "[267/300] train: 0.2691 - val: 1.4019\n",
      "[268/300] train: 0.2688 - val: 1.4056\n",
      "[269/300] train: 0.2677 - val: 1.4065\n",
      "[270/300] train: 0.2672 - val: 1.4034\n",
      "[271/300] train: 0.2664 - val: 1.4069\n",
      "[272/300] train: 0.2663 - val: 1.4081\n",
      "[273/300] train: 0.2671 - val: 1.4071\n",
      "[274/300] train: 0.2666 - val: 1.4044\n",
      "[275/300] train: 0.2656 - val: 1.4115\n",
      "[276/300] train: 0.2653 - val: 1.4022\n",
      "[277/300] train: 0.2663 - val: 1.4111\n",
      "[278/300] train: 0.2652 - val: 1.4073\n",
      "[279/300] train: 0.2656 - val: 1.4035\n",
      "[280/300] train: 0.2654 - val: 1.4062\n",
      "[281/300] train: 0.2653 - val: 1.4081\n",
      "[282/300] train: 0.2650 - val: 1.4141\n",
      "[283/300] train: 0.2650 - val: 1.4060\n",
      "[284/300] train: 0.2646 - val: 1.3973\n",
      "[285/300] train: 0.2646 - val: 1.4078\n",
      "[286/300] train: 0.2640 - val: 1.3929\n",
      "[287/300] train: 0.2631 - val: 1.4036\n",
      "[288/300] train: 0.2637 - val: 1.4050\n",
      "[289/300] train: 0.2642 - val: 1.4084\n",
      "[290/300] train: 0.2636 - val: 1.4010\n",
      "[291/300] train: 0.2636 - val: 1.4043\n",
      "[292/300] train: 0.2636 - val: 1.4048\n",
      "[293/300] train: 0.2624 - val: 1.4046\n",
      "[294/300] train: 0.2615 - val: 1.4133\n",
      "[295/300] train: 0.2630 - val: 1.4074\n",
      "[296/300] train: 0.2626 - val: 1.4111\n",
      "[297/300] train: 0.2629 - val: 1.4079\n",
      "[298/300] train: 0.2627 - val: 1.4070\n",
      "[299/300] train: 0.2616 - val: 1.4076\n",
      "[300/300] train: 0.2614 - val: 1.4132\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 16\n",
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')\n",
    "\n",
    "plt.savefig(figure_path + '/ammazon-small-loss.png', dpi=300, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 1.0114\n"
     ]
    }
   ],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 1.0174\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "with open('best.weights', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f57757a1",
   "language": "python",
   "display_name": "PyCharm (rs-via-gnn)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}