{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MovieLens MLN Recommendation via PyTorch\n",
    "\n",
    "adapted from https://github.com/fastai/fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "#from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import time\n",
    "\n",
    "figure_path = '/home/weiss/git/thesis/doc/figures/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "\n",
    "RANDOM_STATE = 2021\n",
    "set_random_seed(RANDOM_STATE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    files = {}\n",
    "    path = Path(path)\n",
    "    for filename in path.glob('*'):\n",
    "        if filename.suffix == '.dat':\n",
    "            if filename.stem == 'ratings':\n",
    "                columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "            elif filename.stem == 'users':\n",
    "                columns = ['userId', 'gender', 'Occupation', 'zip-code']\n",
    "            else:\n",
    "                columns = ['movieId', 'title', 'genres']\n",
    "            data = pd.read_csv(filename, sep='::', names=columns, engine='python')\n",
    "            files[filename.stem] = data\n",
    "    return files['ratings'], files['movies'], files['users']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# pick one of the available folders\n",
    "ratings, movies, users = read_data('/home/weiss/rs_data/ml-1m')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "   userId  movieId  rating  timestamp\n0       1     1193       5  978300760\n1       1      661       3  978302109\n2       1      914       3  978301968\n3       1     3408       4  978300275\n4       1     2355       5  978824291",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1193</td>\n      <td>5</td>\n      <td>978300760</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>661</td>\n      <td>3</td>\n      <td>978302109</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>914</td>\n      <td>3</td>\n      <td>978301968</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3408</td>\n      <td>4</td>\n      <td>978300275</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2355</td>\n      <td>5</td>\n      <td>978824291</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "   movieId                               title                        genres\n0        1                    Toy Story (1995)   Animation|Children's|Comedy\n1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n2        3             Grumpier Old Men (1995)                Comedy|Romance\n3        4            Waiting to Exhale (1995)                  Comedy|Drama\n4        5  Father of the Bride Part II (1995)                        Comedy",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movieId</th>\n      <th>title</th>\n      <th>genres</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Toy Story (1995)</td>\n      <td>Animation|Children's|Comedy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Jumanji (1995)</td>\n      <td>Adventure|Children's|Fantasy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Grumpier Old Men (1995)</td>\n      <td>Comedy|Romance</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Waiting to Exhale (1995)</td>\n      <td>Comedy|Drama</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Father of the Bride Part II (1995)</td>\n      <td>Comedy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 6040 users, 3706 movies\n",
      "Dataset shape: (1000209, 2)\n",
      "Target shape: (1000209,)\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(ratings, top=None):\n",
    "    if top is not None:\n",
    "        ratings.groupby('userId')['rating'].count()\n",
    "\n",
    "    unique_users = ratings.userId.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.userId.map(user_to_index)\n",
    "\n",
    "    unique_movies = ratings.movieId.unique()\n",
    "    movie_to_index = {old: new for new, old in enumerate(unique_movies)}\n",
    "    new_movies = ratings.movieId.map(movie_to_index)\n",
    "\n",
    "    n_users = unique_users.shape[0]\n",
    "    n_movies = unique_movies.shape[0]\n",
    "\n",
    "    X = pd.DataFrame({'user_id': new_users, 'movie_id': new_movies})\n",
    "    y = ratings['rating'].astype(np.float32)\n",
    "    return (n_users, n_movies), (X, y), (user_to_index, movie_to_index)\n",
    "\n",
    "(n, m), (X, y), _ = create_dataset(ratings)\n",
    "print(f'Embeddings: {n} users, {m} movies')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=RANDOM_STATE)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, random_state=RANDOM_STATE)\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid), 'test': (X_test, y_test)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_valid), 'test': len(X_test)}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class RatingsIterator:\n",
    "\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in RatingsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class RecommenderNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates dense MLN with embedding layers.\n",
    "\n",
    "    Args:\n",
    "        n_users:\n",
    "            Number of unique users in the dataset.\n",
    "\n",
    "        n_movies:\n",
    "            Number of unique movies in the dataset.\n",
    "\n",
    "        n_factors:\n",
    "            Number of columns in the embeddings matrix.\n",
    "\n",
    "        embedding_dropout:\n",
    "            Dropout rate to apply right after embeddings layer.\n",
    "\n",
    "        hidden:\n",
    "            A single integer or a list of integers defining the number of\n",
    "            units in hidden layer(s).\n",
    "\n",
    "        dropouts:\n",
    "            A single integer or a list of integers defining the dropout\n",
    "            layers rates applied right after each of hidden layers.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_movies,\n",
    "                 n_factors=50, embedding_dropout=0.02,\n",
    "                 hidden=10, dropouts=0.2):\n",
    "\n",
    "        super().__init__()\n",
    "        hidden = get_list(hidden)\n",
    "        dropouts = get_list(dropouts)\n",
    "        n_last = hidden[-1]\n",
    "\n",
    "        def gen_layers(n_in):\n",
    "            \"\"\"\n",
    "            A generator that yields a sequence of hidden layers and\n",
    "            their activations/dropouts.\n",
    "\n",
    "            Note that the function captures `hidden` and `dropouts`\n",
    "            values from the outer scope.\n",
    "            \"\"\"\n",
    "            nonlocal hidden, dropouts\n",
    "            assert len(dropouts) <= len(hidden)\n",
    "\n",
    "            for n_out, rate in zip_longest(hidden, dropouts):\n",
    "                yield nn.Linear(n_in, n_out)\n",
    "                yield nn.ReLU()\n",
    "                if rate is not None and rate > 0.:\n",
    "                    yield nn.Dropout(rate)\n",
    "                n_in = n_out\n",
    "\n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        self.m = nn.Embedding(n_movies, n_factors)\n",
    "        self.drop = nn.Dropout(embedding_dropout)\n",
    "        self.hidden = nn.Sequential(*list(gen_layers(n_factors * 2)))\n",
    "        self.fc = nn.Linear(n_last, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, users, movies, minmax=None):\n",
    "        features = torch.cat([self.u(users), self.m(movies)], dim=1)\n",
    "        x = self.drop(features)\n",
    "        x = self.hidden(x)\n",
    "        out = torch.sigmoid(self.fc(x))\n",
    "        #out = self.relu(self.fc(x))  # relu delivers worse rsme\n",
    "        if minmax is not None:\n",
    "            min_rating, max_rating = minmax\n",
    "            out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n",
    "        return out\n",
    "\n",
    "    def _init(self):\n",
    "        \"\"\"\n",
    "        Setup embeddings and hidden layers with reasonable initial values.\n",
    "        \"\"\"\n",
    "\n",
    "        def init(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        self.u.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.m.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.hidden.apply(init)\n",
    "        init(self.fc)\n",
    "\n",
    "\n",
    "def get_list(n):\n",
    "    if isinstance(n, (int, float)):\n",
    "        return [n]\n",
    "    elif hasattr(n, '__iter__'):\n",
    "        return list(n)\n",
    "    raise TypeError('layers configuraiton should be a single number or a list of numbers')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def plot_lr(schedule):\n",
    "    ts = list(range(1000))\n",
    "    y = [schedule(t, 0.001) for t in ts]\n",
    "    plt.plot(ts, y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(1.0, 5.0)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax = float(ratings.rating.min()), float(ratings.rating.max())\n",
    "minmax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# small net\n",
    "\n",
    "start = time.time()\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_movies=m,\n",
    "    n_factors=10, hidden=[10],\n",
    "    embedding_dropout=0.05, dropouts=[0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "#iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "#scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 1.1976 - val: 0.9426\n",
      "loss improvement on epoch: 2\n",
      "[002/300] train: 0.9012 - val: 0.8667\n",
      "loss improvement on epoch: 3\n",
      "[003/300] train: 0.8622 - val: 0.8519\n",
      "loss improvement on epoch: 4\n",
      "[004/300] train: 0.8468 - val: 0.8432\n",
      "loss improvement on epoch: 5\n",
      "[005/300] train: 0.8368 - val: 0.8390\n",
      "loss improvement on epoch: 6\n",
      "[006/300] train: 0.8290 - val: 0.8346\n",
      "loss improvement on epoch: 7\n",
      "[007/300] train: 0.8233 - val: 0.8299\n",
      "[008/300] train: 0.8203 - val: 0.8301\n",
      "loss improvement on epoch: 9\n",
      "[009/300] train: 0.8160 - val: 0.8289\n",
      "loss improvement on epoch: 10\n",
      "[010/300] train: 0.8138 - val: 0.8273\n",
      "loss improvement on epoch: 11\n",
      "[011/300] train: 0.8118 - val: 0.8265\n",
      "[012/300] train: 0.8099 - val: 0.8266\n",
      "loss improvement on epoch: 13\n",
      "[013/300] train: 0.8088 - val: 0.8258\n",
      "[014/300] train: 0.8074 - val: 0.8259\n",
      "loss improvement on epoch: 15\n",
      "[015/300] train: 0.8059 - val: 0.8238\n",
      "[016/300] train: 0.8056 - val: 0.8245\n",
      "[017/300] train: 0.8046 - val: 0.8250\n",
      "[018/300] train: 0.8028 - val: 0.8240\n",
      "[019/300] train: 0.8018 - val: 0.8239\n",
      "loss improvement on epoch: 20\n",
      "[020/300] train: 0.8001 - val: 0.8232\n",
      "[021/300] train: 0.7989 - val: 0.8242\n",
      "loss improvement on epoch: 22\n",
      "[022/300] train: 0.7990 - val: 0.8232\n",
      "loss improvement on epoch: 23\n",
      "[023/300] train: 0.7984 - val: 0.8230\n",
      "loss improvement on epoch: 24\n",
      "[024/300] train: 0.7969 - val: 0.8230\n",
      "[025/300] train: 0.7962 - val: 0.8234\n",
      "[026/300] train: 0.7947 - val: 0.8236\n",
      "[027/300] train: 0.7939 - val: 0.8237\n",
      "[028/300] train: 0.7932 - val: 0.8231\n",
      "loss improvement on epoch: 29\n",
      "[029/300] train: 0.7919 - val: 0.8223\n",
      "[030/300] train: 0.7919 - val: 0.8233\n",
      "[031/300] train: 0.7903 - val: 0.8241\n",
      "[032/300] train: 0.7891 - val: 0.8233\n",
      "[033/300] train: 0.7882 - val: 0.8228\n",
      "[034/300] train: 0.7879 - val: 0.8232\n",
      "loss improvement on epoch: 35\n",
      "[035/300] train: 0.7881 - val: 0.8217\n",
      "[036/300] train: 0.7873 - val: 0.8227\n",
      "loss improvement on epoch: 37\n",
      "[037/300] train: 0.7863 - val: 0.8214\n",
      "[038/300] train: 0.7863 - val: 0.8250\n",
      "[039/300] train: 0.7856 - val: 0.8225\n",
      "[040/300] train: 0.7844 - val: 0.8231\n",
      "[041/300] train: 0.7839 - val: 0.8233\n",
      "loss improvement on epoch: 42\n",
      "[042/300] train: 0.7839 - val: 0.8213\n",
      "[043/300] train: 0.7834 - val: 0.8225\n",
      "[044/300] train: 0.7826 - val: 0.8230\n",
      "[045/300] train: 0.7813 - val: 0.8230\n",
      "[046/300] train: 0.7811 - val: 0.8226\n",
      "[047/300] train: 0.7806 - val: 0.8235\n",
      "[048/300] train: 0.7801 - val: 0.8217\n",
      "[049/300] train: 0.7796 - val: 0.8233\n",
      "[050/300] train: 0.7790 - val: 0.8219\n",
      "[051/300] train: 0.7792 - val: 0.8222\n",
      "[052/300] train: 0.7785 - val: 0.8214\n",
      "[053/300] train: 0.7785 - val: 0.8215\n",
      "[054/300] train: 0.7771 - val: 0.8239\n",
      "[055/300] train: 0.7774 - val: 0.8236\n",
      "[056/300] train: 0.7774 - val: 0.8227\n",
      "[057/300] train: 0.7767 - val: 0.8232\n",
      "[058/300] train: 0.7773 - val: 0.8241\n",
      "[059/300] train: 0.7763 - val: 0.8219\n",
      "[060/300] train: 0.7754 - val: 0.8243\n",
      "[061/300] train: 0.7748 - val: 0.8235\n",
      "[062/300] train: 0.7753 - val: 0.8240\n",
      "[063/300] train: 0.7747 - val: 0.8256\n",
      "[064/300] train: 0.7738 - val: 0.8231\n",
      "[065/300] train: 0.7747 - val: 0.8238\n",
      "[066/300] train: 0.7734 - val: 0.8229\n",
      "[067/300] train: 0.7731 - val: 0.8233\n",
      "[068/300] train: 0.7741 - val: 0.8242\n",
      "[069/300] train: 0.7730 - val: 0.8239\n",
      "[070/300] train: 0.7730 - val: 0.8237\n",
      "[071/300] train: 0.7735 - val: 0.8253\n",
      "[072/300] train: 0.7717 - val: 0.8242\n",
      "[073/300] train: 0.7720 - val: 0.8236\n",
      "[074/300] train: 0.7714 - val: 0.8245\n",
      "[075/300] train: 0.7720 - val: 0.8255\n",
      "[076/300] train: 0.7709 - val: 0.8252\n",
      "[077/300] train: 0.7721 - val: 0.8249\n",
      "[078/300] train: 0.7718 - val: 0.8242\n",
      "[079/300] train: 0.7711 - val: 0.8223\n",
      "[080/300] train: 0.7705 - val: 0.8239\n",
      "[081/300] train: 0.7706 - val: 0.8246\n",
      "[082/300] train: 0.7705 - val: 0.8242\n",
      "[083/300] train: 0.7701 - val: 0.8253\n",
      "[084/300] train: 0.7699 - val: 0.8238\n",
      "[085/300] train: 0.7704 - val: 0.8248\n",
      "[086/300] train: 0.7703 - val: 0.8258\n",
      "[087/300] train: 0.7702 - val: 0.8255\n",
      "[088/300] train: 0.7703 - val: 0.8252\n",
      "[089/300] train: 0.7698 - val: 0.8253\n",
      "[090/300] train: 0.7687 - val: 0.8249\n",
      "[091/300] train: 0.7688 - val: 0.8283\n",
      "[092/300] train: 0.7702 - val: 0.8269\n",
      "[093/300] train: 0.7688 - val: 0.8258\n",
      "[094/300] train: 0.7691 - val: 0.8271\n",
      "[095/300] train: 0.7696 - val: 0.8255\n",
      "[096/300] train: 0.7690 - val: 0.8232\n",
      "[097/300] train: 0.7693 - val: 0.8264\n",
      "[098/300] train: 0.7691 - val: 0.8258\n",
      "[099/300] train: 0.7692 - val: 0.8256\n",
      "[100/300] train: 0.7690 - val: 0.8252\n",
      "[101/300] train: 0.7697 - val: 0.8269\n",
      "[102/300] train: 0.7678 - val: 0.8269\n",
      "[103/300] train: 0.7679 - val: 0.8262\n",
      "[104/300] train: 0.7686 - val: 0.8265\n",
      "[105/300] train: 0.7692 - val: 0.8257\n",
      "[106/300] train: 0.7677 - val: 0.8259\n",
      "[107/300] train: 0.7676 - val: 0.8270\n",
      "[108/300] train: 0.7690 - val: 0.8242\n",
      "[109/300] train: 0.7688 - val: 0.8254\n",
      "[110/300] train: 0.7677 - val: 0.8253\n",
      "[111/300] train: 0.7679 - val: 0.8252\n",
      "[112/300] train: 0.7669 - val: 0.8265\n",
      "[113/300] train: 0.7668 - val: 0.8264\n",
      "[114/300] train: 0.7681 - val: 0.8268\n",
      "[115/300] train: 0.7671 - val: 0.8260\n",
      "[116/300] train: 0.7680 - val: 0.8264\n",
      "[117/300] train: 0.7669 - val: 0.8276\n",
      "[118/300] train: 0.7672 - val: 0.8254\n",
      "[119/300] train: 0.7674 - val: 0.8278\n",
      "[120/300] train: 0.7683 - val: 0.8286\n",
      "[121/300] train: 0.7666 - val: 0.8265\n",
      "[122/300] train: 0.7671 - val: 0.8259\n",
      "[123/300] train: 0.7663 - val: 0.8254\n",
      "[124/300] train: 0.7667 - val: 0.8272\n",
      "[125/300] train: 0.7662 - val: 0.8275\n",
      "[126/300] train: 0.7681 - val: 0.8259\n",
      "[127/300] train: 0.7671 - val: 0.8259\n",
      "[128/300] train: 0.7663 - val: 0.8268\n",
      "[129/300] train: 0.7658 - val: 0.8262\n",
      "[130/300] train: 0.7658 - val: 0.8277\n",
      "[131/300] train: 0.7665 - val: 0.8260\n",
      "[132/300] train: 0.7656 - val: 0.8286\n",
      "[133/300] train: 0.7657 - val: 0.8274\n",
      "[134/300] train: 0.7657 - val: 0.8278\n",
      "[135/300] train: 0.7660 - val: 0.8272\n",
      "[136/300] train: 0.7661 - val: 0.8282\n",
      "[137/300] train: 0.7661 - val: 0.8280\n",
      "[138/300] train: 0.7654 - val: 0.8269\n",
      "[139/300] train: 0.7660 - val: 0.8288\n",
      "[140/300] train: 0.7650 - val: 0.8262\n",
      "[141/300] train: 0.7656 - val: 0.8283\n",
      "[142/300] train: 0.7651 - val: 0.8278\n",
      "[143/300] train: 0.7653 - val: 0.8295\n",
      "[144/300] train: 0.7655 - val: 0.8277\n",
      "[145/300] train: 0.7652 - val: 0.8266\n",
      "[146/300] train: 0.7657 - val: 0.8284\n",
      "[147/300] train: 0.7654 - val: 0.8270\n",
      "[148/300] train: 0.7648 - val: 0.8279\n",
      "[149/300] train: 0.7648 - val: 0.8251\n",
      "[150/300] train: 0.7644 - val: 0.8265\n",
      "[151/300] train: 0.7659 - val: 0.8269\n",
      "[152/300] train: 0.7633 - val: 0.8270\n",
      "[153/300] train: 0.7655 - val: 0.8277\n",
      "[154/300] train: 0.7643 - val: 0.8271\n",
      "[155/300] train: 0.7653 - val: 0.8284\n",
      "[156/300] train: 0.7649 - val: 0.8268\n",
      "[157/300] train: 0.7649 - val: 0.8276\n",
      "[158/300] train: 0.7650 - val: 0.8272\n",
      "[159/300] train: 0.7652 - val: 0.8313\n",
      "[160/300] train: 0.7644 - val: 0.8295\n",
      "[161/300] train: 0.7649 - val: 0.8293\n",
      "[162/300] train: 0.7632 - val: 0.8270\n",
      "[163/300] train: 0.7643 - val: 0.8272\n",
      "[164/300] train: 0.7632 - val: 0.8298\n",
      "[165/300] train: 0.7631 - val: 0.8301\n",
      "[166/300] train: 0.7644 - val: 0.8277\n",
      "[167/300] train: 0.7645 - val: 0.8282\n",
      "[168/300] train: 0.7639 - val: 0.8286\n",
      "[169/300] train: 0.7645 - val: 0.8288\n",
      "[170/300] train: 0.7636 - val: 0.8292\n",
      "[171/300] train: 0.7648 - val: 0.8290\n",
      "[172/300] train: 0.7640 - val: 0.8279\n",
      "[173/300] train: 0.7637 - val: 0.8287\n",
      "[174/300] train: 0.7642 - val: 0.8268\n",
      "[175/300] train: 0.7637 - val: 0.8295\n",
      "[176/300] train: 0.7642 - val: 0.8259\n",
      "[177/300] train: 0.7640 - val: 0.8264\n",
      "[178/300] train: 0.7640 - val: 0.8284\n",
      "[179/300] train: 0.7637 - val: 0.8275\n",
      "[180/300] train: 0.7626 - val: 0.8286\n",
      "[181/300] train: 0.7640 - val: 0.8275\n",
      "[182/300] train: 0.7638 - val: 0.8292\n",
      "[183/300] train: 0.7627 - val: 0.8290\n",
      "[184/300] train: 0.7641 - val: 0.8302\n",
      "[185/300] train: 0.7643 - val: 0.8293\n",
      "[186/300] train: 0.7635 - val: 0.8282\n",
      "[187/300] train: 0.7638 - val: 0.8267\n",
      "[188/300] train: 0.7636 - val: 0.8297\n",
      "[189/300] train: 0.7641 - val: 0.8288\n",
      "[190/300] train: 0.7642 - val: 0.8310\n",
      "[191/300] train: 0.7629 - val: 0.8271\n",
      "[192/300] train: 0.7635 - val: 0.8310\n",
      "[193/300] train: 0.7633 - val: 0.8279\n",
      "[194/300] train: 0.7634 - val: 0.8287\n",
      "[195/300] train: 0.7639 - val: 0.8279\n",
      "[196/300] train: 0.7623 - val: 0.8275\n",
      "[197/300] train: 0.7641 - val: 0.8283\n",
      "[198/300] train: 0.7626 - val: 0.8293\n",
      "[199/300] train: 0.7631 - val: 0.8298\n",
      "[200/300] train: 0.7634 - val: 0.8278\n",
      "[201/300] train: 0.7622 - val: 0.8287\n",
      "[202/300] train: 0.7629 - val: 0.8274\n",
      "[203/300] train: 0.7629 - val: 0.8271\n",
      "[204/300] train: 0.7634 - val: 0.8289\n",
      "[205/300] train: 0.7626 - val: 0.8285\n",
      "[206/300] train: 0.7614 - val: 0.8301\n",
      "[207/300] train: 0.7623 - val: 0.8278\n",
      "[208/300] train: 0.7615 - val: 0.8282\n",
      "[209/300] train: 0.7633 - val: 0.8265\n",
      "[210/300] train: 0.7629 - val: 0.8290\n",
      "[211/300] train: 0.7628 - val: 0.8280\n",
      "[212/300] train: 0.7625 - val: 0.8288\n",
      "[213/300] train: 0.7626 - val: 0.8294\n",
      "[214/300] train: 0.7630 - val: 0.8274\n",
      "[215/300] train: 0.7623 - val: 0.8290\n",
      "[216/300] train: 0.7617 - val: 0.8294\n",
      "[217/300] train: 0.7620 - val: 0.8275\n",
      "[218/300] train: 0.7622 - val: 0.8287\n",
      "[219/300] train: 0.7627 - val: 0.8299\n",
      "[220/300] train: 0.7625 - val: 0.8293\n",
      "[221/300] train: 0.7614 - val: 0.8272\n",
      "[222/300] train: 0.7623 - val: 0.8284\n",
      "[223/300] train: 0.7634 - val: 0.8277\n",
      "[224/300] train: 0.7629 - val: 0.8270\n",
      "[225/300] train: 0.7615 - val: 0.8285\n",
      "[226/300] train: 0.7614 - val: 0.8285\n",
      "[227/300] train: 0.7630 - val: 0.8310\n",
      "[228/300] train: 0.7623 - val: 0.8289\n",
      "[229/300] train: 0.7637 - val: 0.8295\n",
      "[230/300] train: 0.7609 - val: 0.8277\n",
      "[231/300] train: 0.7630 - val: 0.8297\n",
      "[232/300] train: 0.7621 - val: 0.8282\n",
      "[233/300] train: 0.7616 - val: 0.8311\n",
      "[234/300] train: 0.7622 - val: 0.8278\n",
      "[235/300] train: 0.7619 - val: 0.8294\n",
      "[236/300] train: 0.7625 - val: 0.8285\n",
      "[237/300] train: 0.7624 - val: 0.8295\n",
      "[238/300] train: 0.7623 - val: 0.8301\n",
      "[239/300] train: 0.7624 - val: 0.8282\n",
      "[240/300] train: 0.7621 - val: 0.8296\n",
      "[241/300] train: 0.7619 - val: 0.8316\n",
      "[242/300] train: 0.7614 - val: 0.8306\n",
      "[243/300] train: 0.7619 - val: 0.8286\n",
      "[244/300] train: 0.7614 - val: 0.8293\n",
      "[245/300] train: 0.7626 - val: 0.8289\n",
      "[246/300] train: 0.7629 - val: 0.8280\n",
      "[247/300] train: 0.7619 - val: 0.8277\n",
      "[248/300] train: 0.7625 - val: 0.8297\n",
      "[249/300] train: 0.7625 - val: 0.8291\n",
      "[250/300] train: 0.7608 - val: 0.8309\n",
      "[251/300] train: 0.7612 - val: 0.8325\n",
      "[252/300] train: 0.7605 - val: 0.8293\n",
      "[253/300] train: 0.7617 - val: 0.8281\n",
      "[254/300] train: 0.7614 - val: 0.8294\n",
      "[255/300] train: 0.7612 - val: 0.8305\n",
      "[256/300] train: 0.7603 - val: 0.8297\n",
      "[257/300] train: 0.7619 - val: 0.8298\n",
      "[258/300] train: 0.7609 - val: 0.8295\n",
      "[259/300] train: 0.7599 - val: 0.8294\n",
      "[260/300] train: 0.7612 - val: 0.8306\n",
      "[261/300] train: 0.7611 - val: 0.8298\n",
      "[262/300] train: 0.7612 - val: 0.8289\n",
      "[263/300] train: 0.7606 - val: 0.8301\n",
      "[264/300] train: 0.7623 - val: 0.8300\n",
      "[265/300] train: 0.7612 - val: 0.8309\n",
      "[266/300] train: 0.7609 - val: 0.8294\n",
      "[267/300] train: 0.7615 - val: 0.8289\n",
      "[268/300] train: 0.7602 - val: 0.8291\n",
      "[269/300] train: 0.7614 - val: 0.8298\n",
      "[270/300] train: 0.7611 - val: 0.8290\n",
      "[271/300] train: 0.7633 - val: 0.8286\n",
      "[272/300] train: 0.7616 - val: 0.8313\n",
      "[273/300] train: 0.7617 - val: 0.8300\n",
      "[274/300] train: 0.7604 - val: 0.8295\n",
      "[275/300] train: 0.7621 - val: 0.8298\n",
      "[276/300] train: 0.7593 - val: 0.8288\n",
      "[277/300] train: 0.7602 - val: 0.8276\n",
      "[278/300] train: 0.7613 - val: 0.8300\n",
      "[279/300] train: 0.7604 - val: 0.8285\n",
      "[280/300] train: 0.7616 - val: 0.8302\n",
      "[281/300] train: 0.7610 - val: 0.8298\n",
      "[282/300] train: 0.7603 - val: 0.8324\n",
      "[283/300] train: 0.7610 - val: 0.8286\n",
      "[284/300] train: 0.7607 - val: 0.8307\n",
      "[285/300] train: 0.7616 - val: 0.8303\n",
      "[286/300] train: 0.7606 - val: 0.8289\n",
      "[287/300] train: 0.7610 - val: 0.8322\n",
      "[288/300] train: 0.7604 - val: 0.8287\n",
      "[289/300] train: 0.7605 - val: 0.8316\n",
      "[290/300] train: 0.7615 - val: 0.8292\n",
      "[291/300] train: 0.7619 - val: 0.8294\n",
      "[292/300] train: 0.7600 - val: 0.8296\n",
      "[293/300] train: 0.7607 - val: 0.8286\n",
      "[294/300] train: 0.7597 - val: 0.8318\n",
      "[295/300] train: 0.7611 - val: 0.8273\n",
      "[296/300] train: 0.7611 - val: 0.8311\n",
      "[297/300] train: 0.7595 - val: 0.8305\n",
      "[298/300] train: 0.7612 - val: 0.8307\n",
      "[299/300] train: 0.7599 - val: 0.8306\n",
      "[300/300] train: 0.7598 - val: 0.8301\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    #scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #lr_history.extend(scheduler.get_lr())\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 16\n",
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')\n",
    "\n",
    "plt.savefig(figure_path + '/ml-1M-small-loss.png', dpi=300, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Plot decreasing learning rate\n",
    "\n",
    "#_ = plt.plot(lr_history[:2*iterations_per_epoch-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.9145\n"
     ]
    }
   ],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.9168\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "with open('best.weights.small', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 87.8068\n"
     ]
    }
   ],
   "source": [
    "print(f'duration: {round(time.time() - start, 4)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# medium net\n",
    "\n",
    "start = time.time()\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_movies=m,\n",
    "    n_factors=20, hidden=[10 ,10],\n",
    "    embedding_dropout=0.05, dropouts=[0.3, 0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "#iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "#scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 1.1910 - val: 0.9509\n",
      "loss improvement on epoch: 2\n",
      "[002/300] train: 0.9255 - val: 0.8859\n",
      "loss improvement on epoch: 3\n",
      "[003/300] train: 0.8761 - val: 0.8629\n",
      "loss improvement on epoch: 4\n",
      "[004/300] train: 0.8557 - val: 0.8562\n",
      "loss improvement on epoch: 5\n",
      "[005/300] train: 0.8459 - val: 0.8535\n",
      "loss improvement on epoch: 6\n",
      "[006/300] train: 0.8396 - val: 0.8493\n",
      "loss improvement on epoch: 7\n",
      "[007/300] train: 0.8350 - val: 0.8492\n",
      "loss improvement on epoch: 8\n",
      "[008/300] train: 0.8311 - val: 0.8488\n",
      "[009/300] train: 0.8265 - val: 0.8489\n",
      "loss improvement on epoch: 10\n",
      "[010/300] train: 0.8259 - val: 0.8446\n",
      "[011/300] train: 0.8229 - val: 0.8475\n",
      "[012/300] train: 0.8202 - val: 0.8485\n",
      "[013/300] train: 0.8188 - val: 0.8483\n",
      "[014/300] train: 0.8173 - val: 0.8476\n",
      "[015/300] train: 0.8146 - val: 0.8466\n",
      "[016/300] train: 0.8136 - val: 0.8466\n",
      "[017/300] train: 0.8124 - val: 0.8464\n",
      "[018/300] train: 0.8098 - val: 0.8449\n",
      "[019/300] train: 0.8091 - val: 0.8478\n",
      "[020/300] train: 0.8067 - val: 0.8475\n",
      "loss improvement on epoch: 21\n",
      "[021/300] train: 0.8054 - val: 0.8444\n",
      "[022/300] train: 0.8044 - val: 0.8466\n",
      "[023/300] train: 0.8031 - val: 0.8462\n",
      "[024/300] train: 0.8031 - val: 0.8466\n",
      "[025/300] train: 0.8009 - val: 0.8455\n",
      "[026/300] train: 0.8011 - val: 0.8453\n",
      "[027/300] train: 0.7994 - val: 0.8471\n",
      "[028/300] train: 0.7982 - val: 0.8455\n",
      "[029/300] train: 0.7974 - val: 0.8479\n",
      "[030/300] train: 0.7965 - val: 0.8477\n",
      "[031/300] train: 0.7951 - val: 0.8470\n",
      "[032/300] train: 0.7957 - val: 0.8483\n",
      "[033/300] train: 0.7949 - val: 0.8477\n",
      "[034/300] train: 0.7939 - val: 0.8464\n",
      "[035/300] train: 0.7938 - val: 0.8477\n",
      "[036/300] train: 0.7921 - val: 0.8505\n",
      "[037/300] train: 0.7913 - val: 0.8494\n",
      "[038/300] train: 0.7916 - val: 0.8484\n",
      "[039/300] train: 0.7910 - val: 0.8502\n",
      "[040/300] train: 0.7899 - val: 0.8488\n",
      "[041/300] train: 0.7904 - val: 0.8495\n",
      "[042/300] train: 0.7902 - val: 0.8498\n",
      "[043/300] train: 0.7882 - val: 0.8506\n",
      "[044/300] train: 0.7879 - val: 0.8497\n",
      "[045/300] train: 0.7873 - val: 0.8497\n",
      "[046/300] train: 0.7862 - val: 0.8496\n",
      "[047/300] train: 0.7875 - val: 0.8483\n",
      "[048/300] train: 0.7878 - val: 0.8502\n",
      "[049/300] train: 0.7855 - val: 0.8511\n",
      "[050/300] train: 0.7841 - val: 0.8509\n",
      "[051/300] train: 0.7855 - val: 0.8519\n",
      "[052/300] train: 0.7851 - val: 0.8489\n",
      "[053/300] train: 0.7836 - val: 0.8519\n",
      "[054/300] train: 0.7838 - val: 0.8509\n",
      "[055/300] train: 0.7841 - val: 0.8502\n",
      "[056/300] train: 0.7837 - val: 0.8511\n",
      "[057/300] train: 0.7829 - val: 0.8509\n",
      "[058/300] train: 0.7839 - val: 0.8501\n",
      "[059/300] train: 0.7825 - val: 0.8504\n",
      "[060/300] train: 0.7829 - val: 0.8520\n",
      "[061/300] train: 0.7822 - val: 0.8504\n",
      "[062/300] train: 0.7824 - val: 0.8495\n",
      "[063/300] train: 0.7812 - val: 0.8506\n",
      "[064/300] train: 0.7800 - val: 0.8527\n",
      "[065/300] train: 0.7812 - val: 0.8519\n",
      "[066/300] train: 0.7804 - val: 0.8490\n",
      "[067/300] train: 0.7802 - val: 0.8529\n",
      "[068/300] train: 0.7806 - val: 0.8509\n",
      "[069/300] train: 0.7800 - val: 0.8494\n",
      "[070/300] train: 0.7788 - val: 0.8519\n",
      "[071/300] train: 0.7798 - val: 0.8517\n",
      "[072/300] train: 0.7795 - val: 0.8499\n",
      "[073/300] train: 0.7798 - val: 0.8522\n",
      "[074/300] train: 0.7785 - val: 0.8508\n",
      "[075/300] train: 0.7782 - val: 0.8540\n",
      "[076/300] train: 0.7797 - val: 0.8522\n",
      "[077/300] train: 0.7797 - val: 0.8527\n",
      "[078/300] train: 0.7784 - val: 0.8521\n",
      "[079/300] train: 0.7789 - val: 0.8539\n",
      "[080/300] train: 0.7778 - val: 0.8525\n",
      "[081/300] train: 0.7790 - val: 0.8528\n",
      "[082/300] train: 0.7773 - val: 0.8519\n",
      "[083/300] train: 0.7780 - val: 0.8531\n",
      "[084/300] train: 0.7770 - val: 0.8529\n",
      "[085/300] train: 0.7764 - val: 0.8527\n",
      "[086/300] train: 0.7773 - val: 0.8540\n",
      "[087/300] train: 0.7770 - val: 0.8526\n",
      "[088/300] train: 0.7760 - val: 0.8500\n",
      "[089/300] train: 0.7763 - val: 0.8503\n",
      "[090/300] train: 0.7759 - val: 0.8525\n",
      "[091/300] train: 0.7747 - val: 0.8521\n",
      "[092/300] train: 0.7756 - val: 0.8538\n",
      "[093/300] train: 0.7751 - val: 0.8520\n",
      "[094/300] train: 0.7743 - val: 0.8504\n",
      "[095/300] train: 0.7743 - val: 0.8547\n",
      "[096/300] train: 0.7749 - val: 0.8523\n",
      "[097/300] train: 0.7748 - val: 0.8519\n",
      "[098/300] train: 0.7740 - val: 0.8558\n",
      "[099/300] train: 0.7751 - val: 0.8519\n",
      "[100/300] train: 0.7740 - val: 0.8516\n",
      "[101/300] train: 0.7742 - val: 0.8531\n",
      "[102/300] train: 0.7737 - val: 0.8527\n",
      "[103/300] train: 0.7740 - val: 0.8540\n",
      "[104/300] train: 0.7737 - val: 0.8539\n",
      "[105/300] train: 0.7725 - val: 0.8551\n",
      "[106/300] train: 0.7726 - val: 0.8533\n",
      "[107/300] train: 0.7729 - val: 0.8557\n",
      "[108/300] train: 0.7734 - val: 0.8527\n",
      "[109/300] train: 0.7731 - val: 0.8516\n",
      "[110/300] train: 0.7724 - val: 0.8523\n",
      "[111/300] train: 0.7731 - val: 0.8514\n",
      "[112/300] train: 0.7717 - val: 0.8519\n",
      "[113/300] train: 0.7733 - val: 0.8517\n",
      "[114/300] train: 0.7718 - val: 0.8522\n",
      "[115/300] train: 0.7726 - val: 0.8497\n",
      "[116/300] train: 0.7729 - val: 0.8531\n",
      "[117/300] train: 0.7720 - val: 0.8520\n",
      "[118/300] train: 0.7711 - val: 0.8528\n",
      "[119/300] train: 0.7727 - val: 0.8543\n",
      "[120/300] train: 0.7719 - val: 0.8534\n",
      "[121/300] train: 0.7727 - val: 0.8525\n",
      "[122/300] train: 0.7707 - val: 0.8521\n",
      "[123/300] train: 0.7717 - val: 0.8532\n",
      "[124/300] train: 0.7708 - val: 0.8541\n",
      "[125/300] train: 0.7713 - val: 0.8529\n",
      "[126/300] train: 0.7709 - val: 0.8535\n",
      "[127/300] train: 0.7710 - val: 0.8545\n",
      "[128/300] train: 0.7707 - val: 0.8536\n",
      "[129/300] train: 0.7702 - val: 0.8521\n",
      "[130/300] train: 0.7691 - val: 0.8542\n",
      "[131/300] train: 0.7696 - val: 0.8518\n",
      "[132/300] train: 0.7702 - val: 0.8564\n",
      "[133/300] train: 0.7702 - val: 0.8533\n",
      "[134/300] train: 0.7698 - val: 0.8525\n",
      "[135/300] train: 0.7705 - val: 0.8535\n",
      "[136/300] train: 0.7702 - val: 0.8532\n",
      "[137/300] train: 0.7707 - val: 0.8555\n",
      "[138/300] train: 0.7695 - val: 0.8535\n",
      "[139/300] train: 0.7700 - val: 0.8528\n",
      "[140/300] train: 0.7710 - val: 0.8510\n",
      "[141/300] train: 0.7697 - val: 0.8538\n",
      "[142/300] train: 0.7692 - val: 0.8526\n",
      "[143/300] train: 0.7698 - val: 0.8523\n",
      "[144/300] train: 0.7687 - val: 0.8563\n",
      "[145/300] train: 0.7694 - val: 0.8510\n",
      "[146/300] train: 0.7694 - val: 0.8544\n",
      "[147/300] train: 0.7688 - val: 0.8525\n",
      "[148/300] train: 0.7690 - val: 0.8518\n",
      "[149/300] train: 0.7692 - val: 0.8538\n",
      "[150/300] train: 0.7690 - val: 0.8528\n",
      "[151/300] train: 0.7689 - val: 0.8550\n",
      "[152/300] train: 0.7699 - val: 0.8552\n",
      "[153/300] train: 0.7690 - val: 0.8561\n",
      "[154/300] train: 0.7685 - val: 0.8539\n",
      "[155/300] train: 0.7687 - val: 0.8546\n",
      "[156/300] train: 0.7684 - val: 0.8544\n",
      "[157/300] train: 0.7683 - val: 0.8532\n",
      "[158/300] train: 0.7672 - val: 0.8527\n",
      "[159/300] train: 0.7686 - val: 0.8557\n",
      "[160/300] train: 0.7676 - val: 0.8520\n",
      "[161/300] train: 0.7685 - val: 0.8533\n",
      "[162/300] train: 0.7693 - val: 0.8536\n",
      "[163/300] train: 0.7669 - val: 0.8535\n",
      "[164/300] train: 0.7671 - val: 0.8553\n",
      "[165/300] train: 0.7691 - val: 0.8561\n",
      "[166/300] train: 0.7675 - val: 0.8553\n",
      "[167/300] train: 0.7669 - val: 0.8586\n",
      "[168/300] train: 0.7684 - val: 0.8538\n",
      "[169/300] train: 0.7671 - val: 0.8546\n",
      "[170/300] train: 0.7684 - val: 0.8548\n",
      "[171/300] train: 0.7671 - val: 0.8555\n",
      "[172/300] train: 0.7673 - val: 0.8556\n",
      "[173/300] train: 0.7669 - val: 0.8560\n",
      "[174/300] train: 0.7675 - val: 0.8535\n",
      "[175/300] train: 0.7674 - val: 0.8559\n",
      "[176/300] train: 0.7670 - val: 0.8556\n",
      "[177/300] train: 0.7676 - val: 0.8563\n",
      "[178/300] train: 0.7668 - val: 0.8542\n",
      "[179/300] train: 0.7667 - val: 0.8565\n",
      "[180/300] train: 0.7672 - val: 0.8536\n",
      "[181/300] train: 0.7679 - val: 0.8527\n",
      "[182/300] train: 0.7661 - val: 0.8553\n",
      "[183/300] train: 0.7664 - val: 0.8549\n",
      "[184/300] train: 0.7665 - val: 0.8554\n",
      "[185/300] train: 0.7661 - val: 0.8544\n",
      "[186/300] train: 0.7670 - val: 0.8548\n",
      "[187/300] train: 0.7669 - val: 0.8521\n",
      "[188/300] train: 0.7669 - val: 0.8551\n",
      "[189/300] train: 0.7663 - val: 0.8568\n",
      "[190/300] train: 0.7669 - val: 0.8549\n",
      "[191/300] train: 0.7666 - val: 0.8547\n",
      "[192/300] train: 0.7669 - val: 0.8558\n",
      "[193/300] train: 0.7656 - val: 0.8561\n",
      "[194/300] train: 0.7656 - val: 0.8591\n",
      "[195/300] train: 0.7660 - val: 0.8525\n",
      "[196/300] train: 0.7664 - val: 0.8549\n",
      "[197/300] train: 0.7657 - val: 0.8540\n",
      "[198/300] train: 0.7669 - val: 0.8549\n",
      "[199/300] train: 0.7662 - val: 0.8541\n",
      "[200/300] train: 0.7654 - val: 0.8555\n",
      "[201/300] train: 0.7660 - val: 0.8579\n",
      "[202/300] train: 0.7661 - val: 0.8552\n",
      "[203/300] train: 0.7651 - val: 0.8561\n",
      "[204/300] train: 0.7665 - val: 0.8547\n",
      "[205/300] train: 0.7668 - val: 0.8550\n",
      "[206/300] train: 0.7659 - val: 0.8564\n",
      "[207/300] train: 0.7654 - val: 0.8572\n",
      "[208/300] train: 0.7657 - val: 0.8569\n",
      "[209/300] train: 0.7652 - val: 0.8535\n",
      "[210/300] train: 0.7656 - val: 0.8566\n",
      "[211/300] train: 0.7657 - val: 0.8554\n",
      "[212/300] train: 0.7652 - val: 0.8571\n",
      "[213/300] train: 0.7654 - val: 0.8559\n",
      "[214/300] train: 0.7651 - val: 0.8550\n",
      "[215/300] train: 0.7665 - val: 0.8551\n",
      "[216/300] train: 0.7656 - val: 0.8561\n",
      "[217/300] train: 0.7652 - val: 0.8556\n",
      "[218/300] train: 0.7649 - val: 0.8546\n",
      "[219/300] train: 0.7662 - val: 0.8572\n",
      "[220/300] train: 0.7643 - val: 0.8572\n",
      "[221/300] train: 0.7640 - val: 0.8560\n",
      "[222/300] train: 0.7651 - val: 0.8561\n",
      "[223/300] train: 0.7656 - val: 0.8570\n",
      "[224/300] train: 0.7644 - val: 0.8562\n",
      "[225/300] train: 0.7635 - val: 0.8571\n",
      "[226/300] train: 0.7648 - val: 0.8538\n",
      "[227/300] train: 0.7639 - val: 0.8566\n",
      "[228/300] train: 0.7646 - val: 0.8548\n",
      "[229/300] train: 0.7631 - val: 0.8559\n",
      "[230/300] train: 0.7638 - val: 0.8546\n",
      "[231/300] train: 0.7643 - val: 0.8561\n",
      "[232/300] train: 0.7656 - val: 0.8560\n",
      "[233/300] train: 0.7648 - val: 0.8568\n",
      "[234/300] train: 0.7635 - val: 0.8553\n",
      "[235/300] train: 0.7647 - val: 0.8582\n",
      "[236/300] train: 0.7636 - val: 0.8577\n",
      "[237/300] train: 0.7641 - val: 0.8575\n",
      "[238/300] train: 0.7637 - val: 0.8569\n",
      "[239/300] train: 0.7642 - val: 0.8564\n",
      "[240/300] train: 0.7642 - val: 0.8571\n",
      "[241/300] train: 0.7645 - val: 0.8553\n",
      "[242/300] train: 0.7642 - val: 0.8554\n",
      "[243/300] train: 0.7637 - val: 0.8599\n",
      "[244/300] train: 0.7633 - val: 0.8582\n",
      "[245/300] train: 0.7632 - val: 0.8547\n",
      "[246/300] train: 0.7643 - val: 0.8569\n",
      "[247/300] train: 0.7639 - val: 0.8576\n",
      "[248/300] train: 0.7627 - val: 0.8594\n",
      "[249/300] train: 0.7629 - val: 0.8598\n",
      "[250/300] train: 0.7650 - val: 0.8561\n",
      "[251/300] train: 0.7631 - val: 0.8581\n",
      "[252/300] train: 0.7632 - val: 0.8570\n",
      "[253/300] train: 0.7632 - val: 0.8560\n",
      "[254/300] train: 0.7635 - val: 0.8576\n",
      "[255/300] train: 0.7636 - val: 0.8563\n",
      "[256/300] train: 0.7622 - val: 0.8578\n",
      "[257/300] train: 0.7635 - val: 0.8590\n",
      "[258/300] train: 0.7634 - val: 0.8556\n",
      "[259/300] train: 0.7628 - val: 0.8574\n",
      "[260/300] train: 0.7628 - val: 0.8560\n",
      "[261/300] train: 0.7632 - val: 0.8578\n",
      "[262/300] train: 0.7618 - val: 0.8582\n",
      "[263/300] train: 0.7636 - val: 0.8582\n",
      "[264/300] train: 0.7639 - val: 0.8586\n",
      "[265/300] train: 0.7616 - val: 0.8578\n",
      "[266/300] train: 0.7625 - val: 0.8598\n",
      "[267/300] train: 0.7638 - val: 0.8580\n",
      "[268/300] train: 0.7629 - val: 0.8563\n",
      "[269/300] train: 0.7620 - val: 0.8574\n",
      "[270/300] train: 0.7617 - val: 0.8571\n",
      "[271/300] train: 0.7620 - val: 0.8594\n",
      "[272/300] train: 0.7619 - val: 0.8579\n",
      "[273/300] train: 0.7634 - val: 0.8574\n",
      "[274/300] train: 0.7633 - val: 0.8580\n",
      "[275/300] train: 0.7625 - val: 0.8585\n",
      "[276/300] train: 0.7611 - val: 0.8557\n",
      "[277/300] train: 0.7634 - val: 0.8566\n",
      "[278/300] train: 0.7631 - val: 0.8576\n",
      "[279/300] train: 0.7628 - val: 0.8580\n",
      "[280/300] train: 0.7623 - val: 0.8564\n",
      "[281/300] train: 0.7637 - val: 0.8574\n",
      "[282/300] train: 0.7629 - val: 0.8576\n",
      "[283/300] train: 0.7619 - val: 0.8574\n",
      "[284/300] train: 0.7629 - val: 0.8560\n",
      "[285/300] train: 0.7621 - val: 0.8576\n",
      "[286/300] train: 0.7628 - val: 0.8581\n",
      "[287/300] train: 0.7639 - val: 0.8557\n",
      "[288/300] train: 0.7614 - val: 0.8579\n",
      "[289/300] train: 0.7621 - val: 0.8606\n",
      "[290/300] train: 0.7628 - val: 0.8572\n",
      "[291/300] train: 0.7613 - val: 0.8595\n",
      "[292/300] train: 0.7624 - val: 0.8603\n",
      "[293/300] train: 0.7616 - val: 0.8598\n",
      "[294/300] train: 0.7630 - val: 0.8572\n",
      "[295/300] train: 0.7618 - val: 0.8574\n",
      "[296/300] train: 0.7612 - val: 0.8569\n",
      "[297/300] train: 0.7618 - val: 0.8579\n",
      "[298/300] train: 0.7609 - val: 0.8565\n",
      "[299/300] train: 0.7615 - val: 0.8559\n",
      "[300/300] train: 0.7617 - val: 0.8605\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    #scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #lr_history.extend(scheduler.get_lr())\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 16\n",
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')\n",
    "\n",
    "plt.savefig(figure_path + '/ml-1M-medium-loss.png', dpi=300, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# Plot decreasing learning rate\n",
    "\n",
    "#_ = plt.plot(lr_history[:2*iterations_per_epoch-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.9281\n"
     ]
    }
   ],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.9312\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "with open('best.weights.medium', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 98.1039\n"
     ]
    }
   ],
   "source": [
    "print(f'duration: {round(time.time() - start, 4)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# big net\n",
    "\n",
    "start = time.time()\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_movies=m,\n",
    "    n_factors=50, hidden=[100, 100, 100],\n",
    "    embedding_dropout=0.05, dropouts=[0.3, 0.3, 0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "#iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "#scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 0.9961 - val: 0.8413\n",
      "loss improvement on epoch: 2\n",
      "[002/300] train: 0.8310 - val: 0.8156\n",
      "loss improvement on epoch: 3\n",
      "[003/300] train: 0.8079 - val: 0.8084\n",
      "loss improvement on epoch: 4\n",
      "[004/300] train: 0.7930 - val: 0.8048\n",
      "loss improvement on epoch: 5\n",
      "[005/300] train: 0.7785 - val: 0.7975\n",
      "loss improvement on epoch: 6\n",
      "[006/300] train: 0.7621 - val: 0.7943\n",
      "[007/300] train: 0.7451 - val: 0.7964\n",
      "[008/300] train: 0.7292 - val: 0.7986\n",
      "[009/300] train: 0.7148 - val: 0.7975\n",
      "[010/300] train: 0.7004 - val: 0.8052\n",
      "[011/300] train: 0.6878 - val: 0.8071\n",
      "[012/300] train: 0.6751 - val: 0.8159\n",
      "[013/300] train: 0.6650 - val: 0.8217\n",
      "[014/300] train: 0.6547 - val: 0.8263\n",
      "[015/300] train: 0.6454 - val: 0.8258\n",
      "[016/300] train: 0.6367 - val: 0.8326\n",
      "[017/300] train: 0.6300 - val: 0.8318\n",
      "[018/300] train: 0.6227 - val: 0.8328\n",
      "[019/300] train: 0.6159 - val: 0.8363\n",
      "[020/300] train: 0.6099 - val: 0.8436\n",
      "[021/300] train: 0.6035 - val: 0.8423\n",
      "[022/300] train: 0.5989 - val: 0.8463\n",
      "[023/300] train: 0.5944 - val: 0.8501\n",
      "[024/300] train: 0.5904 - val: 0.8467\n",
      "[025/300] train: 0.5860 - val: 0.8512\n",
      "[026/300] train: 0.5820 - val: 0.8573\n",
      "[027/300] train: 0.5780 - val: 0.8535\n",
      "[028/300] train: 0.5748 - val: 0.8595\n",
      "[029/300] train: 0.5719 - val: 0.8542\n",
      "[030/300] train: 0.5696 - val: 0.8494\n",
      "[031/300] train: 0.5663 - val: 0.8479\n",
      "[032/300] train: 0.5630 - val: 0.8620\n",
      "[033/300] train: 0.5604 - val: 0.8599\n",
      "[034/300] train: 0.5584 - val: 0.8677\n",
      "[035/300] train: 0.5555 - val: 0.8586\n",
      "[036/300] train: 0.5540 - val: 0.8578\n",
      "[037/300] train: 0.5510 - val: 0.8595\n",
      "[038/300] train: 0.5482 - val: 0.8641\n",
      "[039/300] train: 0.5467 - val: 0.8691\n",
      "[040/300] train: 0.5456 - val: 0.8613\n",
      "[041/300] train: 0.5439 - val: 0.8691\n",
      "[042/300] train: 0.5411 - val: 0.8630\n",
      "[043/300] train: 0.5388 - val: 0.8717\n",
      "[044/300] train: 0.5377 - val: 0.8661\n",
      "[045/300] train: 0.5367 - val: 0.8729\n",
      "[046/300] train: 0.5343 - val: 0.8689\n",
      "[047/300] train: 0.5346 - val: 0.8744\n",
      "[048/300] train: 0.5312 - val: 0.8702\n",
      "[049/300] train: 0.5302 - val: 0.8771\n",
      "[050/300] train: 0.5293 - val: 0.8746\n",
      "[051/300] train: 0.5283 - val: 0.8751\n",
      "[052/300] train: 0.5264 - val: 0.8702\n",
      "[053/300] train: 0.5267 - val: 0.8686\n",
      "[054/300] train: 0.5244 - val: 0.8695\n",
      "[055/300] train: 0.5248 - val: 0.8801\n",
      "[056/300] train: 0.5219 - val: 0.8700\n",
      "[057/300] train: 0.5211 - val: 0.8782\n",
      "[058/300] train: 0.5194 - val: 0.8755\n",
      "[059/300] train: 0.5189 - val: 0.8772\n",
      "[060/300] train: 0.5186 - val: 0.8740\n",
      "[061/300] train: 0.5176 - val: 0.8795\n",
      "[062/300] train: 0.5158 - val: 0.8753\n",
      "[063/300] train: 0.5137 - val: 0.8797\n",
      "[064/300] train: 0.5146 - val: 0.8824\n",
      "[065/300] train: 0.5125 - val: 0.8807\n",
      "[066/300] train: 0.5142 - val: 0.8872\n",
      "[067/300] train: 0.5112 - val: 0.8799\n",
      "[068/300] train: 0.5111 - val: 0.8846\n",
      "[069/300] train: 0.5104 - val: 0.8829\n",
      "[070/300] train: 0.5100 - val: 0.8771\n",
      "[071/300] train: 0.5083 - val: 0.8857\n",
      "[072/300] train: 0.5077 - val: 0.8848\n",
      "[073/300] train: 0.5057 - val: 0.8850\n",
      "[074/300] train: 0.5051 - val: 0.8887\n",
      "[075/300] train: 0.5058 - val: 0.8798\n",
      "[076/300] train: 0.5047 - val: 0.8873\n",
      "[077/300] train: 0.5056 - val: 0.8847\n",
      "[078/300] train: 0.5035 - val: 0.8863\n",
      "[079/300] train: 0.5028 - val: 0.8822\n",
      "[080/300] train: 0.5020 - val: 0.8975\n",
      "[081/300] train: 0.5012 - val: 0.8899\n",
      "[082/300] train: 0.5019 - val: 0.8855\n",
      "[083/300] train: 0.5006 - val: 0.8869\n",
      "[084/300] train: 0.4994 - val: 0.8855\n",
      "[085/300] train: 0.4996 - val: 0.8878\n",
      "[086/300] train: 0.4990 - val: 0.8856\n",
      "[087/300] train: 0.4983 - val: 0.8865\n",
      "[088/300] train: 0.4969 - val: 0.8806\n",
      "[089/300] train: 0.4954 - val: 0.8863\n",
      "[090/300] train: 0.4970 - val: 0.8886\n",
      "[091/300] train: 0.4957 - val: 0.8850\n",
      "[092/300] train: 0.4953 - val: 0.8963\n",
      "[093/300] train: 0.4941 - val: 0.8905\n",
      "[094/300] train: 0.4935 - val: 0.8940\n",
      "[095/300] train: 0.4940 - val: 0.8905\n",
      "[096/300] train: 0.4936 - val: 0.8920\n",
      "[097/300] train: 0.4923 - val: 0.8881\n",
      "[098/300] train: 0.4919 - val: 0.8916\n",
      "[099/300] train: 0.4924 - val: 0.8906\n",
      "[100/300] train: 0.4903 - val: 0.8881\n",
      "[101/300] train: 0.4905 - val: 0.8969\n",
      "[102/300] train: 0.4905 - val: 0.8913\n",
      "[103/300] train: 0.4901 - val: 0.8904\n",
      "[104/300] train: 0.4908 - val: 0.8914\n",
      "[105/300] train: 0.4892 - val: 0.8889\n",
      "[106/300] train: 0.4888 - val: 0.8950\n",
      "[107/300] train: 0.4877 - val: 0.8888\n",
      "[108/300] train: 0.4879 - val: 0.8905\n",
      "[109/300] train: 0.4875 - val: 0.8913\n",
      "[110/300] train: 0.4875 - val: 0.8926\n",
      "[111/300] train: 0.4877 - val: 0.8912\n",
      "[112/300] train: 0.4861 - val: 0.8928\n",
      "[113/300] train: 0.4854 - val: 0.8988\n",
      "[114/300] train: 0.4866 - val: 0.8889\n",
      "[115/300] train: 0.4845 - val: 0.9011\n",
      "[116/300] train: 0.4846 - val: 0.8950\n",
      "[117/300] train: 0.4849 - val: 0.8984\n",
      "[118/300] train: 0.4847 - val: 0.8906\n",
      "[119/300] train: 0.4846 - val: 0.8954\n",
      "[120/300] train: 0.4830 - val: 0.8974\n",
      "[121/300] train: 0.4831 - val: 0.8941\n",
      "[122/300] train: 0.4824 - val: 0.8950\n",
      "[123/300] train: 0.4821 - val: 0.8948\n",
      "[124/300] train: 0.4812 - val: 0.8996\n",
      "[125/300] train: 0.4812 - val: 0.8919\n",
      "[126/300] train: 0.4813 - val: 0.9019\n",
      "[127/300] train: 0.4806 - val: 0.9003\n",
      "[128/300] train: 0.4803 - val: 0.8947\n",
      "[129/300] train: 0.4806 - val: 0.8969\n",
      "[130/300] train: 0.4797 - val: 0.8953\n",
      "[131/300] train: 0.4798 - val: 0.9019\n",
      "[132/300] train: 0.4795 - val: 0.8964\n",
      "[133/300] train: 0.4793 - val: 0.8941\n",
      "[134/300] train: 0.4787 - val: 0.8921\n",
      "[135/300] train: 0.4781 - val: 0.8921\n",
      "[136/300] train: 0.4769 - val: 0.8986\n",
      "[137/300] train: 0.4786 - val: 0.8996\n",
      "[138/300] train: 0.4768 - val: 0.9009\n",
      "[139/300] train: 0.4767 - val: 0.8960\n",
      "[140/300] train: 0.4760 - val: 0.8918\n",
      "[141/300] train: 0.4758 - val: 0.9027\n",
      "[142/300] train: 0.4758 - val: 0.8992\n",
      "[143/300] train: 0.4760 - val: 0.8951\n",
      "[144/300] train: 0.4762 - val: 0.8958\n",
      "[145/300] train: 0.4754 - val: 0.8950\n",
      "[146/300] train: 0.4743 - val: 0.8904\n",
      "[147/300] train: 0.4752 - val: 0.8985\n",
      "[148/300] train: 0.4744 - val: 0.9029\n",
      "[149/300] train: 0.4744 - val: 0.8972\n",
      "[150/300] train: 0.4729 - val: 0.8984\n",
      "[151/300] train: 0.4737 - val: 0.8951\n",
      "[152/300] train: 0.4738 - val: 0.9018\n",
      "[153/300] train: 0.4732 - val: 0.8978\n",
      "[154/300] train: 0.4733 - val: 0.8928\n",
      "[155/300] train: 0.4720 - val: 0.8952\n",
      "[156/300] train: 0.4721 - val: 0.9048\n",
      "[157/300] train: 0.4716 - val: 0.9005\n",
      "[158/300] train: 0.4727 - val: 0.9023\n",
      "[159/300] train: 0.4708 - val: 0.9001\n",
      "[160/300] train: 0.4715 - val: 0.8965\n",
      "[161/300] train: 0.4715 - val: 0.9025\n",
      "[162/300] train: 0.4709 - val: 0.9020\n",
      "[163/300] train: 0.4695 - val: 0.9045\n",
      "[164/300] train: 0.4708 - val: 0.9008\n",
      "[165/300] train: 0.4689 - val: 0.9065\n",
      "[166/300] train: 0.4699 - val: 0.9078\n",
      "[167/300] train: 0.4704 - val: 0.9040\n",
      "[168/300] train: 0.4695 - val: 0.9002\n",
      "[169/300] train: 0.4683 - val: 0.9001\n",
      "[170/300] train: 0.4691 - val: 0.9035\n",
      "[171/300] train: 0.4683 - val: 0.8993\n",
      "[172/300] train: 0.4690 - val: 0.9007\n",
      "[173/300] train: 0.4682 - val: 0.9073\n",
      "[174/300] train: 0.4675 - val: 0.9015\n",
      "[175/300] train: 0.4694 - val: 0.8969\n",
      "[176/300] train: 0.4676 - val: 0.9048\n",
      "[177/300] train: 0.4674 - val: 0.9036\n",
      "[178/300] train: 0.4675 - val: 0.9023\n",
      "[179/300] train: 0.4669 - val: 0.9025\n",
      "[180/300] train: 0.4662 - val: 0.9034\n",
      "[181/300] train: 0.4650 - val: 0.9126\n",
      "[182/300] train: 0.4663 - val: 0.9097\n",
      "[183/300] train: 0.4664 - val: 0.8993\n",
      "[184/300] train: 0.4663 - val: 0.9005\n",
      "[185/300] train: 0.4654 - val: 0.9005\n",
      "[186/300] train: 0.4644 - val: 0.9039\n",
      "[187/300] train: 0.4652 - val: 0.9045\n",
      "[188/300] train: 0.4643 - val: 0.9011\n",
      "[189/300] train: 0.4646 - val: 0.8982\n",
      "[190/300] train: 0.4649 - val: 0.9064\n",
      "[191/300] train: 0.4638 - val: 0.9117\n",
      "[192/300] train: 0.4646 - val: 0.9043\n",
      "[193/300] train: 0.4648 - val: 0.9012\n",
      "[194/300] train: 0.4640 - val: 0.9021\n",
      "[195/300] train: 0.4640 - val: 0.9085\n",
      "[196/300] train: 0.4640 - val: 0.9014\n",
      "[197/300] train: 0.4629 - val: 0.9110\n",
      "[198/300] train: 0.4635 - val: 0.8959\n",
      "[199/300] train: 0.4624 - val: 0.9073\n",
      "[200/300] train: 0.4626 - val: 0.9121\n",
      "[201/300] train: 0.4632 - val: 0.9033\n",
      "[202/300] train: 0.4624 - val: 0.9056\n",
      "[203/300] train: 0.4623 - val: 0.9066\n",
      "[204/300] train: 0.4616 - val: 0.9031\n",
      "[205/300] train: 0.4619 - val: 0.9039\n",
      "[206/300] train: 0.4619 - val: 0.9049\n",
      "[207/300] train: 0.4616 - val: 0.9046\n",
      "[208/300] train: 0.4615 - val: 0.9014\n",
      "[209/300] train: 0.4614 - val: 0.9056\n",
      "[210/300] train: 0.4611 - val: 0.9103\n",
      "[211/300] train: 0.4607 - val: 0.9066\n",
      "[212/300] train: 0.4595 - val: 0.9072\n",
      "[213/300] train: 0.4599 - val: 0.9059\n",
      "[214/300] train: 0.4602 - val: 0.9078\n",
      "[215/300] train: 0.4607 - val: 0.9055\n",
      "[216/300] train: 0.4600 - val: 0.9025\n",
      "[217/300] train: 0.4597 - val: 0.9009\n",
      "[218/300] train: 0.4590 - val: 0.9084\n",
      "[219/300] train: 0.4588 - val: 0.9038\n",
      "[220/300] train: 0.4585 - val: 0.9120\n",
      "[221/300] train: 0.4581 - val: 0.9019\n",
      "[222/300] train: 0.4577 - val: 0.9043\n",
      "[223/300] train: 0.4596 - val: 0.9054\n",
      "[224/300] train: 0.4585 - val: 0.9226\n",
      "[225/300] train: 0.4586 - val: 0.9073\n",
      "[226/300] train: 0.4584 - val: 0.9053\n",
      "[227/300] train: 0.4582 - val: 0.9122\n",
      "[228/300] train: 0.4577 - val: 0.9062\n",
      "[229/300] train: 0.4582 - val: 0.9038\n",
      "[230/300] train: 0.4568 - val: 0.9032\n",
      "[231/300] train: 0.4569 - val: 0.9155\n",
      "[232/300] train: 0.4563 - val: 0.9177\n",
      "[233/300] train: 0.4583 - val: 0.9081\n",
      "[234/300] train: 0.4584 - val: 0.9070\n",
      "[235/300] train: 0.4562 - val: 0.9101\n",
      "[236/300] train: 0.4568 - val: 0.9064\n",
      "[237/300] train: 0.4560 - val: 0.9111\n",
      "[238/300] train: 0.4573 - val: 0.9047\n",
      "[239/300] train: 0.4562 - val: 0.9055\n",
      "[240/300] train: 0.4545 - val: 0.9088\n",
      "[241/300] train: 0.4569 - val: 0.9132\n",
      "[242/300] train: 0.4556 - val: 0.9091\n",
      "[243/300] train: 0.4564 - val: 0.9093\n",
      "[244/300] train: 0.4542 - val: 0.9037\n",
      "[245/300] train: 0.4560 - val: 0.9060\n",
      "[246/300] train: 0.4534 - val: 0.9068\n",
      "[247/300] train: 0.4553 - val: 0.9014\n",
      "[248/300] train: 0.4553 - val: 0.9069\n",
      "[249/300] train: 0.4549 - val: 0.9111\n",
      "[250/300] train: 0.4544 - val: 0.9087\n",
      "[251/300] train: 0.4540 - val: 0.9199\n",
      "[252/300] train: 0.4549 - val: 0.9106\n",
      "[253/300] train: 0.4541 - val: 0.9045\n",
      "[254/300] train: 0.4541 - val: 0.9079\n",
      "[255/300] train: 0.4545 - val: 0.9113\n",
      "[256/300] train: 0.4523 - val: 0.9170\n",
      "[257/300] train: 0.4536 - val: 0.9137\n",
      "[258/300] train: 0.4544 - val: 0.9098\n",
      "[259/300] train: 0.4532 - val: 0.9135\n",
      "[260/300] train: 0.4535 - val: 0.9143\n",
      "[261/300] train: 0.4536 - val: 0.9087\n",
      "[262/300] train: 0.4526 - val: 0.9107\n",
      "[263/300] train: 0.4540 - val: 0.9114\n",
      "[264/300] train: 0.4523 - val: 0.9139\n",
      "[265/300] train: 0.4520 - val: 0.9088\n",
      "[266/300] train: 0.4526 - val: 0.9118\n",
      "[267/300] train: 0.4508 - val: 0.9031\n",
      "[268/300] train: 0.4516 - val: 0.9083\n",
      "[269/300] train: 0.4514 - val: 0.9120\n",
      "[270/300] train: 0.4520 - val: 0.9152\n",
      "[271/300] train: 0.4509 - val: 0.9140\n",
      "[272/300] train: 0.4509 - val: 0.9120\n",
      "[273/300] train: 0.4501 - val: 0.9104\n",
      "[274/300] train: 0.4523 - val: 0.9120\n",
      "[275/300] train: 0.4517 - val: 0.9168\n",
      "[276/300] train: 0.4512 - val: 0.9098\n",
      "[277/300] train: 0.4499 - val: 0.9072\n",
      "[278/300] train: 0.4513 - val: 0.9153\n",
      "[279/300] train: 0.4502 - val: 0.9211\n",
      "[280/300] train: 0.4505 - val: 0.9112\n",
      "[281/300] train: 0.4503 - val: 0.9048\n",
      "[282/300] train: 0.4510 - val: 0.9173\n",
      "[283/300] train: 0.4504 - val: 0.9112\n",
      "[284/300] train: 0.4493 - val: 0.9168\n",
      "[285/300] train: 0.4497 - val: 0.9112\n",
      "[286/300] train: 0.4494 - val: 0.9107\n",
      "[287/300] train: 0.4501 - val: 0.9159\n",
      "[288/300] train: 0.4500 - val: 0.9111\n",
      "[289/300] train: 0.4490 - val: 0.9130\n",
      "[290/300] train: 0.4494 - val: 0.9105\n",
      "[291/300] train: 0.4491 - val: 0.9172\n",
      "[292/300] train: 0.4488 - val: 0.9111\n",
      "[293/300] train: 0.4497 - val: 0.9106\n",
      "[294/300] train: 0.4486 - val: 0.9173\n",
      "[295/300] train: 0.4492 - val: 0.9152\n",
      "[296/300] train: 0.4492 - val: 0.9152\n",
      "[297/300] train: 0.4485 - val: 0.9117\n",
      "[298/300] train: 0.4485 - val: 0.9104\n",
      "[299/300] train: 0.4488 - val: 0.9111\n",
      "[300/300] train: 0.4487 - val: 0.9162\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    #scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #lr_history.extend(scheduler.get_lr())\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 16\n",
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')\n",
    "\n",
    "plt.savefig(figure_path + '/ml-1M-large-loss.png', dpi=300, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Plot decreasing learning rate\n",
    "\n",
    "#_ = plt.plot(lr_history[:2*iterations_per_epoch-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.8991\n"
     ]
    }
   ],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.9022\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "with open('best.weights.big', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 113.5746\n"
     ]
    }
   ],
   "source": [
    "print(f'duration: {round(time.time() - start, 4)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f57757a1",
   "language": "python",
   "display_name": "PyCharm (rs-via-gnn)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}