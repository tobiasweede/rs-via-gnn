{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MovieLens MLN Recommendation via PyTorch\n",
    "\n",
    "adapted from https://github.com/fastai/fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "\n",
    "RANDOM_STATE = 2021\n",
    "set_random_seed(RANDOM_STATE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    files = {}\n",
    "    path = Path(path)\n",
    "    for filename in path.glob('*'):\n",
    "        if filename.suffix == '.dat':\n",
    "            if filename.stem == 'ratings':\n",
    "                columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "            else:\n",
    "                columns = ['movieId', 'title', 'genres']\n",
    "            data = pd.read_csv(filename, sep='::', names=columns, engine='python')\n",
    "            files[filename.stem] = data\n",
    "    return files['ratings'], files['movies']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# pick one of the available folders\n",
    "ratings, movies = read_data('/home/weiss/rs_data/ml-10m')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "   userId  movieId  rating  timestamp\n0       1      122     5.0  838985046\n1       1      185     5.0  838983525\n2       1      231     5.0  838983392\n3       1      292     5.0  838983421\n4       1      316     5.0  838983392",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>122</td>\n      <td>5.0</td>\n      <td>838985046</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>185</td>\n      <td>5.0</td>\n      <td>838983525</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>231</td>\n      <td>5.0</td>\n      <td>838983392</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>292</td>\n      <td>5.0</td>\n      <td>838983421</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>316</td>\n      <td>5.0</td>\n      <td>838983392</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "   movieId                               title  \\\n0        1                    Toy Story (1995)   \n1        2                      Jumanji (1995)   \n2        3             Grumpier Old Men (1995)   \n3        4            Waiting to Exhale (1995)   \n4        5  Father of the Bride Part II (1995)   \n\n                                        genres  \n0  Adventure|Animation|Children|Comedy|Fantasy  \n1                   Adventure|Children|Fantasy  \n2                               Comedy|Romance  \n3                         Comedy|Drama|Romance  \n4                                       Comedy  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movieId</th>\n      <th>title</th>\n      <th>genres</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Toy Story (1995)</td>\n      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Jumanji (1995)</td>\n      <td>Adventure|Children|Fantasy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Grumpier Old Men (1995)</td>\n      <td>Comedy|Romance</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Waiting to Exhale (1995)</td>\n      <td>Comedy|Drama|Romance</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Father of the Bride Part II (1995)</td>\n      <td>Comedy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 69878 users, 10677 movies\n",
      "Dataset shape: (10000054, 2)\n",
      "Target shape: (10000054,)\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(ratings, top=None):\n",
    "    unique_users = ratings.userId.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.userId.map(user_to_index)\n",
    "\n",
    "    unique_movies = ratings.movieId.unique()\n",
    "    movie_to_index = {old: new for new, old in enumerate(unique_movies)}\n",
    "    new_movies = ratings.movieId.map(movie_to_index)\n",
    "\n",
    "    n_users = unique_users.shape[0]\n",
    "    n_movies = unique_movies.shape[0]\n",
    "\n",
    "    X = pd.DataFrame({'user_id': new_users, 'movie_id': new_movies})\n",
    "    y = ratings['rating'].astype(np.float32)\n",
    "    return (n_users, n_movies), (X, y), (user_to_index, movie_to_index)\n",
    "\n",
    "(n, m), (X, y), _ = create_dataset(ratings)\n",
    "print(f'Embeddings: {n} users, {m} movies')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=RANDOM_STATE)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, random_state=RANDOM_STATE)\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid), 'test': (X_test, y_test)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_valid), 'test': len(X_test)}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class RatingsIterator:\n",
    "\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in RatingsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "class RecommenderNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates dense MLN with embedding layers.\n",
    "\n",
    "    Args:\n",
    "        n_users:\n",
    "            Number of unique users in the dataset.\n",
    "\n",
    "        n_movies:\n",
    "            Number of unique movies in the dataset.\n",
    "\n",
    "        n_factors:\n",
    "            Number of columns in the embeddings matrix.\n",
    "\n",
    "        embedding_dropout:\n",
    "            Dropout rate to apply right after embeddings layer.\n",
    "\n",
    "        hidden:\n",
    "            A single integer or a list of integers defining the number of\n",
    "            units in hidden layer(s).\n",
    "\n",
    "        dropouts:\n",
    "            A single integer or a list of integers defining the dropout\n",
    "            layers rates applied right after each of hidden layers.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_movies,\n",
    "                 n_factors=50, embedding_dropout=0.02,\n",
    "                 hidden=10, dropouts=0.2):\n",
    "\n",
    "        super().__init__()\n",
    "        hidden = get_list(hidden)\n",
    "        dropouts = get_list(dropouts)\n",
    "        n_last = hidden[-1]\n",
    "\n",
    "        def gen_layers(n_in):\n",
    "            \"\"\"\n",
    "            A generator that yields a sequence of hidden layers and\n",
    "            their activations/dropouts.\n",
    "\n",
    "            Note that the function captures `hidden` and `dropouts`\n",
    "            values from the outer scope.\n",
    "            \"\"\"\n",
    "            nonlocal hidden, dropouts\n",
    "            assert len(dropouts) <= len(hidden)\n",
    "\n",
    "            for n_out, rate in zip_longest(hidden, dropouts):\n",
    "                yield nn.Linear(n_in, n_out)\n",
    "                yield nn.ReLU()\n",
    "                if rate is not None and rate > 0.:\n",
    "                    yield nn.Dropout(rate)\n",
    "                n_in = n_out\n",
    "\n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        self.m = nn.Embedding(n_movies, n_factors)\n",
    "        self.drop = nn.Dropout(embedding_dropout)\n",
    "        self.hidden = nn.Sequential(*list(gen_layers(n_factors * 2)))\n",
    "        self.fc = nn.Linear(n_last, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, users, movies, minmax=None):\n",
    "        features = torch.cat([self.u(users), self.m(movies)], dim=1)\n",
    "        x = self.drop(features)\n",
    "        x = self.hidden(x)\n",
    "        out = torch.sigmoid(self.fc(x))\n",
    "        #out = self.relu(self.fc(x))  # relu delivers worse rsme\n",
    "        if minmax is not None:\n",
    "            min_rating, max_rating = minmax\n",
    "            out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n",
    "        return out\n",
    "\n",
    "    def _init(self):\n",
    "        \"\"\"\n",
    "        Setup embeddings and hidden layers with reasonable initial values.\n",
    "        \"\"\"\n",
    "\n",
    "        def init(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        self.u.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.m.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.hidden.apply(init)\n",
    "        init(self.fc)\n",
    "\n",
    "\n",
    "def get_list(n):\n",
    "    if isinstance(n, (int, float)):\n",
    "        return [n]\n",
    "    elif hasattr(n, '__iter__'):\n",
    "        return list(n)\n",
    "    raise TypeError('layers configuraiton should be a single number or a list of numbers')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "\n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + math.cos(math.pi*t/t_max))/2\n",
    "    return scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def plot_lr(schedule):\n",
    "    ts = list(range(1000))\n",
    "    y = [schedule(t, 0.001) for t in ts]\n",
    "    plt.plot(ts, y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.5, 5.0)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax = float(ratings.rating.min()), float(ratings.rating.max())\n",
    "minmax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# small net\n",
    "\n",
    "start = time.time()\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_movies=m,\n",
    "    n_factors=10, hidden=[10],\n",
    "    embedding_dropout=0.05, dropouts=[0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "#iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "#scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 0.8456 - val: 0.7826\n",
      "loss improvement on epoch: 2\n",
      "[002/300] train: 0.7686 - val: 0.7732\n",
      "loss improvement on epoch: 3\n",
      "[003/300] train: 0.7586 - val: 0.7696\n",
      "loss improvement on epoch: 4\n",
      "[004/300] train: 0.7536 - val: 0.7689\n",
      "loss improvement on epoch: 5\n",
      "[005/300] train: 0.7501 - val: 0.7671\n",
      "loss improvement on epoch: 6\n",
      "[006/300] train: 0.7472 - val: 0.7662\n",
      "loss improvement on epoch: 7\n",
      "[007/300] train: 0.7441 - val: 0.7655\n",
      "loss improvement on epoch: 8\n",
      "[008/300] train: 0.7421 - val: 0.7641\n",
      "loss improvement on epoch: 9\n",
      "[009/300] train: 0.7404 - val: 0.7640\n",
      "loss improvement on epoch: 10\n",
      "[010/300] train: 0.7389 - val: 0.7632\n",
      "loss improvement on epoch: 11\n",
      "[011/300] train: 0.7373 - val: 0.7630\n",
      "loss improvement on epoch: 12\n",
      "[012/300] train: 0.7358 - val: 0.7626\n",
      "loss improvement on epoch: 13\n",
      "[013/300] train: 0.7345 - val: 0.7620\n",
      "loss improvement on epoch: 14\n",
      "[014/300] train: 0.7330 - val: 0.7617\n",
      "loss improvement on epoch: 15\n",
      "[015/300] train: 0.7318 - val: 0.7613\n",
      "[016/300] train: 0.7304 - val: 0.7613\n",
      "loss improvement on epoch: 17\n",
      "[017/300] train: 0.7297 - val: 0.7608\n",
      "loss improvement on epoch: 18\n",
      "[018/300] train: 0.7285 - val: 0.7607\n",
      "loss improvement on epoch: 19\n",
      "[019/300] train: 0.7274 - val: 0.7607\n",
      "loss improvement on epoch: 20\n",
      "[020/300] train: 0.7269 - val: 0.7604\n",
      "loss improvement on epoch: 21\n",
      "[021/300] train: 0.7262 - val: 0.7602\n",
      "[022/300] train: 0.7250 - val: 0.7605\n",
      "[023/300] train: 0.7243 - val: 0.7609\n",
      "loss improvement on epoch: 24\n",
      "[024/300] train: 0.7232 - val: 0.7599\n",
      "[025/300] train: 0.7227 - val: 0.7608\n",
      "[026/300] train: 0.7224 - val: 0.7607\n",
      "[027/300] train: 0.7216 - val: 0.7605\n",
      "[028/300] train: 0.7215 - val: 0.7605\n",
      "[029/300] train: 0.7212 - val: 0.7606\n",
      "[030/300] train: 0.7207 - val: 0.7608\n",
      "[031/300] train: 0.7201 - val: 0.7609\n",
      "[032/300] train: 0.7200 - val: 0.7612\n",
      "[033/300] train: 0.7195 - val: 0.7611\n",
      "[034/300] train: 0.7192 - val: 0.7611\n",
      "[035/300] train: 0.7193 - val: 0.7607\n",
      "[036/300] train: 0.7191 - val: 0.7600\n",
      "[037/300] train: 0.7185 - val: 0.7610\n",
      "[038/300] train: 0.7183 - val: 0.7606\n",
      "[039/300] train: 0.7183 - val: 0.7609\n",
      "[040/300] train: 0.7181 - val: 0.7605\n",
      "[041/300] train: 0.7177 - val: 0.7604\n",
      "[042/300] train: 0.7177 - val: 0.7611\n",
      "[043/300] train: 0.7174 - val: 0.7608\n",
      "[044/300] train: 0.7173 - val: 0.7605\n",
      "[045/300] train: 0.7170 - val: 0.7609\n",
      "[046/300] train: 0.7168 - val: 0.7606\n",
      "[047/300] train: 0.7168 - val: 0.7607\n",
      "[048/300] train: 0.7167 - val: 0.7607\n",
      "[049/300] train: 0.7166 - val: 0.7608\n",
      "[050/300] train: 0.7162 - val: 0.7608\n",
      "[051/300] train: 0.7160 - val: 0.7606\n",
      "[052/300] train: 0.7159 - val: 0.7607\n",
      "[053/300] train: 0.7157 - val: 0.7608\n",
      "[054/300] train: 0.7158 - val: 0.7605\n",
      "[055/300] train: 0.7154 - val: 0.7608\n",
      "[056/300] train: 0.7153 - val: 0.7611\n",
      "[057/300] train: 0.7150 - val: 0.7608\n",
      "[058/300] train: 0.7148 - val: 0.7609\n",
      "[059/300] train: 0.7150 - val: 0.7616\n",
      "[060/300] train: 0.7145 - val: 0.7612\n",
      "[061/300] train: 0.7147 - val: 0.7614\n",
      "[062/300] train: 0.7149 - val: 0.7616\n",
      "[063/300] train: 0.7148 - val: 0.7611\n",
      "[064/300] train: 0.7145 - val: 0.7614\n",
      "[065/300] train: 0.7143 - val: 0.7614\n",
      "[066/300] train: 0.7144 - val: 0.7610\n",
      "[067/300] train: 0.7142 - val: 0.7608\n",
      "[068/300] train: 0.7139 - val: 0.7619\n",
      "[069/300] train: 0.7141 - val: 0.7611\n",
      "[070/300] train: 0.7141 - val: 0.7616\n",
      "[071/300] train: 0.7140 - val: 0.7615\n",
      "[072/300] train: 0.7137 - val: 0.7614\n",
      "[073/300] train: 0.7137 - val: 0.7617\n",
      "[074/300] train: 0.7133 - val: 0.7616\n",
      "[075/300] train: 0.7137 - val: 0.7617\n",
      "[076/300] train: 0.7136 - val: 0.7618\n",
      "[077/300] train: 0.7132 - val: 0.7618\n",
      "[078/300] train: 0.7132 - val: 0.7606\n",
      "[079/300] train: 0.7130 - val: 0.7615\n",
      "[080/300] train: 0.7135 - val: 0.7618\n",
      "[081/300] train: 0.7129 - val: 0.7615\n",
      "[082/300] train: 0.7130 - val: 0.7609\n",
      "[083/300] train: 0.7130 - val: 0.7617\n",
      "[084/300] train: 0.7131 - val: 0.7609\n",
      "[085/300] train: 0.7130 - val: 0.7610\n",
      "[086/300] train: 0.7127 - val: 0.7619\n",
      "[087/300] train: 0.7124 - val: 0.7617\n",
      "[088/300] train: 0.7129 - val: 0.7610\n",
      "[089/300] train: 0.7125 - val: 0.7620\n",
      "[090/300] train: 0.7124 - val: 0.7619\n",
      "[091/300] train: 0.7127 - val: 0.7615\n",
      "[092/300] train: 0.7128 - val: 0.7615\n",
      "[093/300] train: 0.7125 - val: 0.7616\n",
      "[094/300] train: 0.7121 - val: 0.7619\n",
      "[095/300] train: 0.7127 - val: 0.7610\n",
      "[096/300] train: 0.7124 - val: 0.7613\n",
      "[097/300] train: 0.7119 - val: 0.7614\n",
      "[098/300] train: 0.7122 - val: 0.7617\n",
      "[099/300] train: 0.7117 - val: 0.7619\n",
      "[100/300] train: 0.7121 - val: 0.7612\n",
      "[101/300] train: 0.7119 - val: 0.7613\n",
      "[102/300] train: 0.7117 - val: 0.7624\n",
      "[103/300] train: 0.7119 - val: 0.7614\n",
      "[104/300] train: 0.7118 - val: 0.7617\n",
      "[105/300] train: 0.7116 - val: 0.7614\n",
      "[106/300] train: 0.7118 - val: 0.7620\n",
      "[107/300] train: 0.7114 - val: 0.7616\n",
      "[108/300] train: 0.7115 - val: 0.7621\n",
      "[109/300] train: 0.7115 - val: 0.7622\n",
      "[110/300] train: 0.7117 - val: 0.7622\n",
      "[111/300] train: 0.7114 - val: 0.7625\n",
      "[112/300] train: 0.7113 - val: 0.7616\n",
      "[113/300] train: 0.7109 - val: 0.7614\n",
      "[114/300] train: 0.7111 - val: 0.7617\n",
      "[115/300] train: 0.7115 - val: 0.7624\n",
      "[116/300] train: 0.7110 - val: 0.7622\n",
      "[117/300] train: 0.7112 - val: 0.7619\n",
      "[118/300] train: 0.7111 - val: 0.7623\n",
      "[119/300] train: 0.7110 - val: 0.7619\n",
      "[120/300] train: 0.7111 - val: 0.7623\n",
      "[121/300] train: 0.7109 - val: 0.7620\n",
      "[122/300] train: 0.7110 - val: 0.7621\n",
      "[123/300] train: 0.7111 - val: 0.7618\n",
      "[124/300] train: 0.7107 - val: 0.7617\n",
      "[125/300] train: 0.7108 - val: 0.7619\n",
      "[126/300] train: 0.7110 - val: 0.7623\n",
      "[127/300] train: 0.7109 - val: 0.7613\n",
      "[128/300] train: 0.7106 - val: 0.7615\n",
      "[129/300] train: 0.7108 - val: 0.7616\n",
      "[130/300] train: 0.7110 - val: 0.7618\n",
      "[131/300] train: 0.7105 - val: 0.7619\n",
      "[132/300] train: 0.7109 - val: 0.7620\n",
      "[133/300] train: 0.7107 - val: 0.7617\n",
      "[134/300] train: 0.7105 - val: 0.7624\n",
      "[135/300] train: 0.7107 - val: 0.7621\n",
      "[136/300] train: 0.7106 - val: 0.7622\n",
      "[137/300] train: 0.7105 - val: 0.7625\n",
      "[138/300] train: 0.7106 - val: 0.7619\n",
      "[139/300] train: 0.7105 - val: 0.7615\n",
      "[140/300] train: 0.7103 - val: 0.7615\n",
      "[141/300] train: 0.7102 - val: 0.7620\n",
      "[142/300] train: 0.7103 - val: 0.7618\n",
      "[143/300] train: 0.7104 - val: 0.7626\n",
      "[144/300] train: 0.7103 - val: 0.7623\n",
      "[145/300] train: 0.7101 - val: 0.7624\n",
      "[146/300] train: 0.7104 - val: 0.7618\n",
      "[147/300] train: 0.7101 - val: 0.7620\n",
      "[148/300] train: 0.7101 - val: 0.7620\n",
      "[149/300] train: 0.7099 - val: 0.7624\n",
      "[150/300] train: 0.7099 - val: 0.7618\n",
      "[151/300] train: 0.7098 - val: 0.7622\n",
      "[152/300] train: 0.7100 - val: 0.7623\n",
      "[153/300] train: 0.7102 - val: 0.7620\n",
      "[154/300] train: 0.7098 - val: 0.7615\n",
      "[155/300] train: 0.7095 - val: 0.7621\n",
      "[156/300] train: 0.7096 - val: 0.7622\n",
      "[157/300] train: 0.7099 - val: 0.7630\n",
      "[158/300] train: 0.7098 - val: 0.7625\n",
      "[159/300] train: 0.7098 - val: 0.7614\n",
      "[160/300] train: 0.7099 - val: 0.7616\n",
      "[161/300] train: 0.7097 - val: 0.7619\n",
      "[162/300] train: 0.7096 - val: 0.7617\n",
      "[163/300] train: 0.7102 - val: 0.7621\n",
      "[164/300] train: 0.7099 - val: 0.7623\n",
      "[165/300] train: 0.7096 - val: 0.7618\n",
      "[166/300] train: 0.7093 - val: 0.7623\n",
      "[167/300] train: 0.7095 - val: 0.7620\n",
      "[168/300] train: 0.7097 - val: 0.7617\n",
      "[169/300] train: 0.7099 - val: 0.7619\n",
      "[170/300] train: 0.7095 - val: 0.7623\n",
      "[171/300] train: 0.7096 - val: 0.7619\n",
      "[172/300] train: 0.7097 - val: 0.7625\n",
      "[173/300] train: 0.7094 - val: 0.7629\n",
      "[174/300] train: 0.7094 - val: 0.7618\n",
      "[175/300] train: 0.7092 - val: 0.7618\n",
      "[176/300] train: 0.7095 - val: 0.7617\n",
      "[177/300] train: 0.7093 - val: 0.7621\n",
      "[178/300] train: 0.7095 - val: 0.7621\n",
      "[179/300] train: 0.7094 - val: 0.7624\n",
      "[180/300] train: 0.7092 - val: 0.7618\n",
      "[181/300] train: 0.7092 - val: 0.7623\n",
      "[182/300] train: 0.7091 - val: 0.7627\n",
      "[183/300] train: 0.7092 - val: 0.7624\n",
      "[184/300] train: 0.7093 - val: 0.7627\n",
      "[185/300] train: 0.7092 - val: 0.7619\n",
      "[186/300] train: 0.7092 - val: 0.7620\n",
      "[187/300] train: 0.7093 - val: 0.7620\n",
      "[188/300] train: 0.7088 - val: 0.7625\n",
      "[189/300] train: 0.7090 - val: 0.7620\n",
      "[190/300] train: 0.7091 - val: 0.7625\n",
      "[191/300] train: 0.7094 - val: 0.7621\n",
      "[192/300] train: 0.7091 - val: 0.7626\n",
      "[193/300] train: 0.7092 - val: 0.7626\n",
      "[194/300] train: 0.7092 - val: 0.7623\n",
      "[195/300] train: 0.7089 - val: 0.7623\n",
      "[196/300] train: 0.7092 - val: 0.7621\n",
      "[197/300] train: 0.7087 - val: 0.7630\n",
      "[198/300] train: 0.7087 - val: 0.7633\n",
      "[199/300] train: 0.7089 - val: 0.7626\n",
      "[200/300] train: 0.7088 - val: 0.7621\n",
      "[201/300] train: 0.7088 - val: 0.7625\n",
      "[202/300] train: 0.7088 - val: 0.7627\n",
      "[203/300] train: 0.7086 - val: 0.7619\n",
      "[204/300] train: 0.7086 - val: 0.7624\n",
      "[205/300] train: 0.7086 - val: 0.7622\n",
      "[206/300] train: 0.7088 - val: 0.7633\n",
      "[207/300] train: 0.7090 - val: 0.7625\n",
      "[208/300] train: 0.7086 - val: 0.7625\n",
      "[209/300] train: 0.7089 - val: 0.7629\n",
      "[210/300] train: 0.7087 - val: 0.7620\n",
      "[211/300] train: 0.7088 - val: 0.7618\n",
      "[212/300] train: 0.7085 - val: 0.7617\n",
      "[213/300] train: 0.7087 - val: 0.7621\n",
      "[214/300] train: 0.7084 - val: 0.7621\n",
      "[215/300] train: 0.7089 - val: 0.7620\n",
      "[216/300] train: 0.7085 - val: 0.7627\n",
      "[217/300] train: 0.7087 - val: 0.7624\n",
      "[218/300] train: 0.7085 - val: 0.7621\n",
      "[219/300] train: 0.7088 - val: 0.7623\n",
      "[220/300] train: 0.7085 - val: 0.7624\n",
      "[221/300] train: 0.7086 - val: 0.7630\n",
      "[222/300] train: 0.7087 - val: 0.7624\n",
      "[223/300] train: 0.7084 - val: 0.7633\n",
      "[224/300] train: 0.7086 - val: 0.7621\n",
      "[225/300] train: 0.7087 - val: 0.7621\n",
      "[226/300] train: 0.7085 - val: 0.7623\n",
      "[227/300] train: 0.7084 - val: 0.7624\n",
      "[228/300] train: 0.7084 - val: 0.7626\n",
      "[229/300] train: 0.7085 - val: 0.7631\n",
      "[230/300] train: 0.7085 - val: 0.7633\n",
      "[231/300] train: 0.7086 - val: 0.7633\n",
      "[232/300] train: 0.7084 - val: 0.7624\n",
      "[233/300] train: 0.7085 - val: 0.7629\n",
      "[234/300] train: 0.7084 - val: 0.7633\n",
      "[235/300] train: 0.7085 - val: 0.7624\n",
      "[236/300] train: 0.7083 - val: 0.7624\n",
      "[237/300] train: 0.7084 - val: 0.7633\n",
      "[238/300] train: 0.7084 - val: 0.7622\n",
      "[239/300] train: 0.7085 - val: 0.7626\n",
      "[240/300] train: 0.7084 - val: 0.7623\n",
      "[241/300] train: 0.7081 - val: 0.7624\n",
      "[242/300] train: 0.7084 - val: 0.7629\n",
      "[243/300] train: 0.7082 - val: 0.7621\n",
      "[244/300] train: 0.7081 - val: 0.7619\n",
      "[245/300] train: 0.7082 - val: 0.7632\n",
      "[246/300] train: 0.7082 - val: 0.7628\n",
      "[247/300] train: 0.7081 - val: 0.7623\n",
      "[248/300] train: 0.7081 - val: 0.7627\n",
      "[249/300] train: 0.7082 - val: 0.7629\n",
      "[250/300] train: 0.7078 - val: 0.7627\n",
      "[251/300] train: 0.7081 - val: 0.7620\n",
      "[252/300] train: 0.7080 - val: 0.7622\n",
      "[253/300] train: 0.7078 - val: 0.7624\n",
      "[254/300] train: 0.7082 - val: 0.7621\n",
      "[255/300] train: 0.7079 - val: 0.7631\n",
      "[256/300] train: 0.7081 - val: 0.7625\n",
      "[257/300] train: 0.7080 - val: 0.7624\n",
      "[258/300] train: 0.7078 - val: 0.7621\n",
      "[259/300] train: 0.7078 - val: 0.7626\n",
      "[260/300] train: 0.7081 - val: 0.7628\n",
      "[261/300] train: 0.7080 - val: 0.7625\n",
      "[262/300] train: 0.7079 - val: 0.7629\n",
      "[263/300] train: 0.7080 - val: 0.7627\n",
      "[264/300] train: 0.7080 - val: 0.7634\n",
      "[265/300] train: 0.7079 - val: 0.7622\n",
      "[266/300] train: 0.7077 - val: 0.7623\n",
      "[267/300] train: 0.7081 - val: 0.7631\n",
      "[268/300] train: 0.7077 - val: 0.7630\n",
      "[269/300] train: 0.7078 - val: 0.7627\n",
      "[270/300] train: 0.7077 - val: 0.7616\n",
      "[271/300] train: 0.7079 - val: 0.7627\n",
      "[272/300] train: 0.7080 - val: 0.7627\n",
      "[273/300] train: 0.7077 - val: 0.7627\n",
      "[274/300] train: 0.7079 - val: 0.7623\n",
      "[275/300] train: 0.7077 - val: 0.7631\n",
      "[276/300] train: 0.7078 - val: 0.7624\n",
      "[277/300] train: 0.7081 - val: 0.7624\n",
      "[278/300] train: 0.7080 - val: 0.7624\n",
      "[279/300] train: 0.7076 - val: 0.7635\n",
      "[280/300] train: 0.7080 - val: 0.7624\n",
      "[281/300] train: 0.7075 - val: 0.7629\n",
      "[282/300] train: 0.7073 - val: 0.7628\n",
      "[283/300] train: 0.7074 - val: 0.7616\n",
      "[284/300] train: 0.7077 - val: 0.7624\n",
      "[285/300] train: 0.7079 - val: 0.7638\n",
      "[286/300] train: 0.7075 - val: 0.7634\n",
      "[287/300] train: 0.7078 - val: 0.7615\n",
      "[288/300] train: 0.7076 - val: 0.7617\n",
      "[289/300] train: 0.7077 - val: 0.7637\n",
      "[290/300] train: 0.7079 - val: 0.7626\n",
      "[291/300] train: 0.7072 - val: 0.7618\n",
      "[292/300] train: 0.7079 - val: 0.7626\n",
      "[293/300] train: 0.7079 - val: 0.7640\n",
      "[294/300] train: 0.7076 - val: 0.7626\n",
      "[295/300] train: 0.7076 - val: 0.7629\n",
      "[296/300] train: 0.7074 - val: 0.7626\n",
      "[297/300] train: 0.7074 - val: 0.7627\n",
      "[298/300] train: 0.7074 - val: 0.7619\n",
      "[299/300] train: 0.7075 - val: 0.7632\n",
      "[300/300] train: 0.7073 - val: 0.7622\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    #scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #lr_history.extend(scheduler.get_lr())\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# Plot decreasing learning rate\n",
    "\n",
    "#_ = plt.plot(lr_history[:2*iterations_per_epoch-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.8726\n"
     ]
    }
   ],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.8721\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "with open('best.weights.small', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 3600.7334\n"
     ]
    }
   ],
   "source": [
    "print(f'duration: {round(time.time() - start, 4)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# medium net\n",
    "\n",
    "start = time.time()\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_movies=m,\n",
    "    n_factors=20, hidden=[10 ,10],\n",
    "    embedding_dropout=0.05, dropouts=[0.3, 0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "#iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "#scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 0.8640 - val: 0.7961\n",
      "loss improvement on epoch: 2\n",
      "[002/300] train: 0.7792 - val: 0.7845\n",
      "loss improvement on epoch: 3\n",
      "[003/300] train: 0.7679 - val: 0.7799\n",
      "loss improvement on epoch: 4\n",
      "[004/300] train: 0.7610 - val: 0.7764\n",
      "loss improvement on epoch: 5\n",
      "[005/300] train: 0.7555 - val: 0.7750\n",
      "loss improvement on epoch: 6\n",
      "[006/300] train: 0.7519 - val: 0.7748\n",
      "loss improvement on epoch: 7\n",
      "[007/300] train: 0.7485 - val: 0.7735\n",
      "loss improvement on epoch: 8\n",
      "[008/300] train: 0.7458 - val: 0.7728\n",
      "[009/300] train: 0.7436 - val: 0.7731\n",
      "[010/300] train: 0.7419 - val: 0.7730\n",
      "loss improvement on epoch: 11\n",
      "[011/300] train: 0.7400 - val: 0.7721\n",
      "loss improvement on epoch: 12\n",
      "[012/300] train: 0.7390 - val: 0.7720\n",
      "[013/300] train: 0.7372 - val: 0.7722\n",
      "[014/300] train: 0.7363 - val: 0.7721\n",
      "[015/300] train: 0.7353 - val: 0.7721\n",
      "loss improvement on epoch: 16\n",
      "[016/300] train: 0.7344 - val: 0.7714\n",
      "[017/300] train: 0.7336 - val: 0.7721\n",
      "loss improvement on epoch: 18\n",
      "[018/300] train: 0.7329 - val: 0.7708\n",
      "[019/300] train: 0.7319 - val: 0.7712\n",
      "[020/300] train: 0.7311 - val: 0.7716\n",
      "[021/300] train: 0.7306 - val: 0.7711\n",
      "[022/300] train: 0.7299 - val: 0.7710\n",
      "loss improvement on epoch: 23\n",
      "[023/300] train: 0.7294 - val: 0.7708\n",
      "[024/300] train: 0.7287 - val: 0.7721\n",
      "loss improvement on epoch: 25\n",
      "[025/300] train: 0.7281 - val: 0.7708\n",
      "[026/300] train: 0.7279 - val: 0.7713\n",
      "[027/300] train: 0.7273 - val: 0.7715\n",
      "[028/300] train: 0.7267 - val: 0.7719\n",
      "[029/300] train: 0.7267 - val: 0.7713\n",
      "[030/300] train: 0.7263 - val: 0.7716\n",
      "[031/300] train: 0.7256 - val: 0.7713\n",
      "[032/300] train: 0.7252 - val: 0.7714\n",
      "[033/300] train: 0.7245 - val: 0.7720\n",
      "[034/300] train: 0.7246 - val: 0.7718\n",
      "[035/300] train: 0.7239 - val: 0.7711\n",
      "[036/300] train: 0.7237 - val: 0.7725\n",
      "[037/300] train: 0.7239 - val: 0.7724\n",
      "[038/300] train: 0.7232 - val: 0.7724\n",
      "[039/300] train: 0.7228 - val: 0.7720\n",
      "[040/300] train: 0.7227 - val: 0.7716\n",
      "[041/300] train: 0.7225 - val: 0.7720\n",
      "[042/300] train: 0.7221 - val: 0.7724\n",
      "[043/300] train: 0.7219 - val: 0.7722\n",
      "[044/300] train: 0.7213 - val: 0.7728\n",
      "[045/300] train: 0.7216 - val: 0.7719\n",
      "[046/300] train: 0.7210 - val: 0.7725\n",
      "[047/300] train: 0.7210 - val: 0.7725\n",
      "[048/300] train: 0.7205 - val: 0.7724\n",
      "[049/300] train: 0.7201 - val: 0.7725\n",
      "[050/300] train: 0.7206 - val: 0.7729\n",
      "[051/300] train: 0.7201 - val: 0.7720\n",
      "[052/300] train: 0.7196 - val: 0.7724\n",
      "[053/300] train: 0.7193 - val: 0.7732\n",
      "[054/300] train: 0.7194 - val: 0.7731\n",
      "[055/300] train: 0.7194 - val: 0.7730\n",
      "[056/300] train: 0.7190 - val: 0.7731\n",
      "[057/300] train: 0.7186 - val: 0.7735\n",
      "[058/300] train: 0.7188 - val: 0.7733\n",
      "[059/300] train: 0.7186 - val: 0.7737\n",
      "[060/300] train: 0.7182 - val: 0.7741\n",
      "[061/300] train: 0.7181 - val: 0.7732\n",
      "[062/300] train: 0.7181 - val: 0.7728\n",
      "[063/300] train: 0.7178 - val: 0.7724\n",
      "[064/300] train: 0.7179 - val: 0.7744\n",
      "[065/300] train: 0.7176 - val: 0.7736\n",
      "[066/300] train: 0.7176 - val: 0.7737\n",
      "[067/300] train: 0.7173 - val: 0.7734\n",
      "[068/300] train: 0.7171 - val: 0.7743\n",
      "[069/300] train: 0.7171 - val: 0.7735\n",
      "[070/300] train: 0.7171 - val: 0.7730\n",
      "[071/300] train: 0.7171 - val: 0.7735\n",
      "[072/300] train: 0.7168 - val: 0.7743\n",
      "[073/300] train: 0.7167 - val: 0.7741\n",
      "[074/300] train: 0.7163 - val: 0.7746\n",
      "[075/300] train: 0.7168 - val: 0.7731\n",
      "[076/300] train: 0.7163 - val: 0.7740\n",
      "[077/300] train: 0.7159 - val: 0.7734\n",
      "[078/300] train: 0.7162 - val: 0.7746\n",
      "[079/300] train: 0.7150 - val: 0.7733\n",
      "[080/300] train: 0.7137 - val: 0.7732\n",
      "[081/300] train: 0.7134 - val: 0.7738\n",
      "[082/300] train: 0.7134 - val: 0.7730\n",
      "[083/300] train: 0.7129 - val: 0.7730\n",
      "[084/300] train: 0.7131 - val: 0.7724\n",
      "[085/300] train: 0.7129 - val: 0.7729\n",
      "[086/300] train: 0.7127 - val: 0.7733\n",
      "[087/300] train: 0.7123 - val: 0.7725\n",
      "[088/300] train: 0.7125 - val: 0.7742\n",
      "[089/300] train: 0.7124 - val: 0.7743\n",
      "[090/300] train: 0.7123 - val: 0.7736\n",
      "[091/300] train: 0.7122 - val: 0.7738\n",
      "[092/300] train: 0.7123 - val: 0.7729\n",
      "[093/300] train: 0.7121 - val: 0.7742\n",
      "[094/300] train: 0.7119 - val: 0.7728\n",
      "[095/300] train: 0.7119 - val: 0.7730\n",
      "[096/300] train: 0.7119 - val: 0.7745\n",
      "[097/300] train: 0.7119 - val: 0.7732\n",
      "[098/300] train: 0.7118 - val: 0.7735\n",
      "[099/300] train: 0.7115 - val: 0.7741\n",
      "[100/300] train: 0.7112 - val: 0.7737\n",
      "[101/300] train: 0.7115 - val: 0.7734\n",
      "[102/300] train: 0.7115 - val: 0.7746\n",
      "[103/300] train: 0.7113 - val: 0.7741\n",
      "[104/300] train: 0.7112 - val: 0.7734\n",
      "[105/300] train: 0.7112 - val: 0.7732\n",
      "[106/300] train: 0.7111 - val: 0.7736\n",
      "[107/300] train: 0.7112 - val: 0.7737\n",
      "[108/300] train: 0.7108 - val: 0.7746\n",
      "[109/300] train: 0.7108 - val: 0.7759\n",
      "[110/300] train: 0.7108 - val: 0.7729\n",
      "[111/300] train: 0.7108 - val: 0.7742\n",
      "[112/300] train: 0.7104 - val: 0.7738\n",
      "[113/300] train: 0.7108 - val: 0.7748\n",
      "[114/300] train: 0.7106 - val: 0.7741\n",
      "[115/300] train: 0.7105 - val: 0.7747\n",
      "[116/300] train: 0.7105 - val: 0.7744\n",
      "[117/300] train: 0.7103 - val: 0.7739\n",
      "[118/300] train: 0.7102 - val: 0.7743\n",
      "[119/300] train: 0.7106 - val: 0.7747\n",
      "[120/300] train: 0.7103 - val: 0.7738\n",
      "[121/300] train: 0.7102 - val: 0.7743\n",
      "[122/300] train: 0.7099 - val: 0.7750\n",
      "[123/300] train: 0.7101 - val: 0.7745\n",
      "[124/300] train: 0.7102 - val: 0.7735\n",
      "[125/300] train: 0.7100 - val: 0.7741\n",
      "[126/300] train: 0.7097 - val: 0.7747\n",
      "[127/300] train: 0.7097 - val: 0.7751\n",
      "[128/300] train: 0.7096 - val: 0.7743\n",
      "[129/300] train: 0.7098 - val: 0.7733\n",
      "[130/300] train: 0.7100 - val: 0.7738\n",
      "[131/300] train: 0.7096 - val: 0.7743\n",
      "[132/300] train: 0.7091 - val: 0.7750\n",
      "[133/300] train: 0.7092 - val: 0.7742\n",
      "[134/300] train: 0.7096 - val: 0.7746\n",
      "[135/300] train: 0.7095 - val: 0.7751\n",
      "[136/300] train: 0.7097 - val: 0.7744\n",
      "[137/300] train: 0.7091 - val: 0.7741\n",
      "[138/300] train: 0.7092 - val: 0.7750\n",
      "[139/300] train: 0.7094 - val: 0.7747\n",
      "[140/300] train: 0.7093 - val: 0.7745\n",
      "[141/300] train: 0.7092 - val: 0.7748\n",
      "[142/300] train: 0.7092 - val: 0.7742\n",
      "[143/300] train: 0.7092 - val: 0.7742\n",
      "[144/300] train: 0.7088 - val: 0.7737\n",
      "[145/300] train: 0.7089 - val: 0.7741\n",
      "[146/300] train: 0.7091 - val: 0.7748\n",
      "[147/300] train: 0.7090 - val: 0.7745\n",
      "[148/300] train: 0.7090 - val: 0.7749\n",
      "[149/300] train: 0.7088 - val: 0.7754\n",
      "[150/300] train: 0.7089 - val: 0.7751\n",
      "[151/300] train: 0.7091 - val: 0.7742\n",
      "[152/300] train: 0.7086 - val: 0.7750\n",
      "[153/300] train: 0.7087 - val: 0.7757\n",
      "[154/300] train: 0.7086 - val: 0.7753\n",
      "[155/300] train: 0.7088 - val: 0.7751\n",
      "[156/300] train: 0.7086 - val: 0.7754\n",
      "[157/300] train: 0.7085 - val: 0.7754\n",
      "[158/300] train: 0.7086 - val: 0.7748\n",
      "[159/300] train: 0.7083 - val: 0.7746\n",
      "[160/300] train: 0.7086 - val: 0.7744\n",
      "[161/300] train: 0.7083 - val: 0.7762\n",
      "[162/300] train: 0.7085 - val: 0.7746\n",
      "[163/300] train: 0.7079 - val: 0.7747\n",
      "[164/300] train: 0.7083 - val: 0.7747\n",
      "[165/300] train: 0.7084 - val: 0.7746\n",
      "[166/300] train: 0.7086 - val: 0.7742\n",
      "[167/300] train: 0.7084 - val: 0.7749\n",
      "[168/300] train: 0.7081 - val: 0.7758\n",
      "[169/300] train: 0.7082 - val: 0.7757\n",
      "[170/300] train: 0.7082 - val: 0.7750\n",
      "[171/300] train: 0.7080 - val: 0.7761\n",
      "[172/300] train: 0.7078 - val: 0.7770\n",
      "[173/300] train: 0.7081 - val: 0.7746\n",
      "[174/300] train: 0.7079 - val: 0.7753\n",
      "[175/300] train: 0.7079 - val: 0.7766\n",
      "[176/300] train: 0.7076 - val: 0.7765\n",
      "[177/300] train: 0.7080 - val: 0.7757\n",
      "[178/300] train: 0.7076 - val: 0.7754\n",
      "[179/300] train: 0.7081 - val: 0.7748\n",
      "[180/300] train: 0.7081 - val: 0.7757\n",
      "[181/300] train: 0.7076 - val: 0.7750\n",
      "[182/300] train: 0.7077 - val: 0.7751\n",
      "[183/300] train: 0.7077 - val: 0.7752\n",
      "[184/300] train: 0.7077 - val: 0.7750\n",
      "[185/300] train: 0.7078 - val: 0.7748\n",
      "[186/300] train: 0.7078 - val: 0.7748\n",
      "[187/300] train: 0.7079 - val: 0.7761\n",
      "[188/300] train: 0.7073 - val: 0.7749\n",
      "[189/300] train: 0.7075 - val: 0.7754\n",
      "[190/300] train: 0.7075 - val: 0.7755\n",
      "[191/300] train: 0.7072 - val: 0.7748\n",
      "[192/300] train: 0.7075 - val: 0.7753\n",
      "[193/300] train: 0.7073 - val: 0.7762\n",
      "[194/300] train: 0.7073 - val: 0.7747\n",
      "[195/300] train: 0.7076 - val: 0.7754\n",
      "[196/300] train: 0.7077 - val: 0.7754\n",
      "[197/300] train: 0.7072 - val: 0.7758\n",
      "[198/300] train: 0.7076 - val: 0.7755\n",
      "[199/300] train: 0.7075 - val: 0.7765\n",
      "[200/300] train: 0.7072 - val: 0.7769\n",
      "[201/300] train: 0.7071 - val: 0.7758\n",
      "[202/300] train: 0.7072 - val: 0.7751\n",
      "[203/300] train: 0.7071 - val: 0.7766\n",
      "[204/300] train: 0.7073 - val: 0.7746\n",
      "[205/300] train: 0.7072 - val: 0.7757\n",
      "[206/300] train: 0.7072 - val: 0.7755\n",
      "[207/300] train: 0.7073 - val: 0.7750\n",
      "[208/300] train: 0.7072 - val: 0.7764\n",
      "[209/300] train: 0.7070 - val: 0.7749\n",
      "[210/300] train: 0.7076 - val: 0.7761\n",
      "[211/300] train: 0.7072 - val: 0.7761\n",
      "[212/300] train: 0.7067 - val: 0.7759\n",
      "[213/300] train: 0.7070 - val: 0.7765\n",
      "[214/300] train: 0.7068 - val: 0.7762\n",
      "[215/300] train: 0.7070 - val: 0.7753\n",
      "[216/300] train: 0.7072 - val: 0.7742\n",
      "[217/300] train: 0.7071 - val: 0.7755\n",
      "[218/300] train: 0.7072 - val: 0.7755\n",
      "[219/300] train: 0.7070 - val: 0.7762\n",
      "[220/300] train: 0.7069 - val: 0.7747\n",
      "[221/300] train: 0.7072 - val: 0.7771\n",
      "[222/300] train: 0.7070 - val: 0.7751\n",
      "[223/300] train: 0.7069 - val: 0.7758\n",
      "[224/300] train: 0.7066 - val: 0.7757\n",
      "[225/300] train: 0.7070 - val: 0.7757\n",
      "[226/300] train: 0.7065 - val: 0.7761\n",
      "[227/300] train: 0.7066 - val: 0.7756\n",
      "[228/300] train: 0.7068 - val: 0.7756\n",
      "[229/300] train: 0.7072 - val: 0.7756\n",
      "[230/300] train: 0.7065 - val: 0.7760\n",
      "[231/300] train: 0.7068 - val: 0.7765\n",
      "[232/300] train: 0.7066 - val: 0.7761\n",
      "[233/300] train: 0.7064 - val: 0.7756\n",
      "[234/300] train: 0.7069 - val: 0.7753\n",
      "[235/300] train: 0.7069 - val: 0.7768\n",
      "[236/300] train: 0.7070 - val: 0.7762\n",
      "[237/300] train: 0.7069 - val: 0.7755\n",
      "[238/300] train: 0.7060 - val: 0.7766\n",
      "[239/300] train: 0.7066 - val: 0.7771\n",
      "[240/300] train: 0.7065 - val: 0.7752\n",
      "[241/300] train: 0.7066 - val: 0.7764\n",
      "[242/300] train: 0.7063 - val: 0.7757\n",
      "[243/300] train: 0.7065 - val: 0.7755\n",
      "[244/300] train: 0.7063 - val: 0.7755\n",
      "[245/300] train: 0.7069 - val: 0.7779\n",
      "[246/300] train: 0.7068 - val: 0.7765\n",
      "[247/300] train: 0.7064 - val: 0.7756\n",
      "[248/300] train: 0.7067 - val: 0.7766\n",
      "[249/300] train: 0.7066 - val: 0.7760\n",
      "[250/300] train: 0.7066 - val: 0.7771\n",
      "[251/300] train: 0.7063 - val: 0.7759\n",
      "[252/300] train: 0.7067 - val: 0.7773\n",
      "[253/300] train: 0.7063 - val: 0.7773\n",
      "[254/300] train: 0.7063 - val: 0.7747\n",
      "[255/300] train: 0.7063 - val: 0.7760\n",
      "[256/300] train: 0.7060 - val: 0.7763\n",
      "[257/300] train: 0.7062 - val: 0.7765\n",
      "[258/300] train: 0.7062 - val: 0.7754\n",
      "[259/300] train: 0.7062 - val: 0.7765\n",
      "[260/300] train: 0.7067 - val: 0.7777\n",
      "[261/300] train: 0.7061 - val: 0.7759\n",
      "[262/300] train: 0.7062 - val: 0.7754\n",
      "[263/300] train: 0.7063 - val: 0.7749\n",
      "[264/300] train: 0.7062 - val: 0.7769\n",
      "[265/300] train: 0.7063 - val: 0.7766\n",
      "[266/300] train: 0.7064 - val: 0.7756\n",
      "[267/300] train: 0.7060 - val: 0.7750\n",
      "[268/300] train: 0.7059 - val: 0.7772\n",
      "[269/300] train: 0.7061 - val: 0.7760\n",
      "[270/300] train: 0.7060 - val: 0.7759\n",
      "[271/300] train: 0.7063 - val: 0.7757\n",
      "[272/300] train: 0.7061 - val: 0.7760\n",
      "[273/300] train: 0.7059 - val: 0.7771\n",
      "[274/300] train: 0.7059 - val: 0.7760\n",
      "[275/300] train: 0.7065 - val: 0.7757\n",
      "[276/300] train: 0.7058 - val: 0.7763\n",
      "[277/300] train: 0.7057 - val: 0.7765\n",
      "[278/300] train: 0.7057 - val: 0.7765\n",
      "[279/300] train: 0.7059 - val: 0.7761\n",
      "[280/300] train: 0.7057 - val: 0.7763\n",
      "[281/300] train: 0.7061 - val: 0.7755\n",
      "[282/300] train: 0.7060 - val: 0.7756\n",
      "[283/300] train: 0.7059 - val: 0.7781\n",
      "[284/300] train: 0.7060 - val: 0.7762\n",
      "[285/300] train: 0.7062 - val: 0.7772\n",
      "[286/300] train: 0.7059 - val: 0.7767\n",
      "[287/300] train: 0.7060 - val: 0.7757\n",
      "[288/300] train: 0.7058 - val: 0.7763\n",
      "[289/300] train: 0.7058 - val: 0.7759\n",
      "[290/300] train: 0.7063 - val: 0.7773\n",
      "[291/300] train: 0.7059 - val: 0.7756\n",
      "[292/300] train: 0.7060 - val: 0.7782\n",
      "[293/300] train: 0.7058 - val: 0.7766\n",
      "[294/300] train: 0.7058 - val: 0.7765\n",
      "[295/300] train: 0.7060 - val: 0.7773\n",
      "[296/300] train: 0.7057 - val: 0.7758\n",
      "[297/300] train: 0.7059 - val: 0.7763\n",
      "[298/300] train: 0.7058 - val: 0.7762\n",
      "[299/300] train: 0.7060 - val: 0.7768\n",
      "[300/300] train: 0.7058 - val: 0.7755\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    #scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #lr_history.extend(scheduler.get_lr())\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# Plot decreasing learning rate\n",
    "\n",
    "#_ = plt.plot(lr_history[:2*iterations_per_epoch-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.8780\n"
     ]
    }
   ],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.8785\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "with open('best.weights.medium', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 5392.5075\n"
     ]
    }
   ],
   "source": [
    "print(f'duration: {round(time.time() - start, 4)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# big net\n",
    "\n",
    "start = time.time()\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_movies=m,\n",
    "    n_factors=50, hidden=[100, 100, 100],\n",
    "    embedding_dropout=0.05, dropouts=[0.3, 0.3, 0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "#iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "#scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 0.7889 - val: 0.7380\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-75-3677654c9e49>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     19\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m                     \u001B[0;31m#scheduler.step()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m                     \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m                     \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m                     \u001B[0;31m#lr_history.extend(scheduler.get_lr())\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/GNN/lib/python3.7/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    243\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    244\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 245\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    246\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    247\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/GNN/lib/python3.7/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    145\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[1;32m    146\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 147\u001B[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001B[0m\u001B[1;32m    148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    #scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #lr_history.extend(scheduler.get_lr())\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot decreasing learning rate\n",
    "\n",
    "#_ = plt.plot(lr_history[:2*iterations_per_epoch-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('best.weights.big', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'duration: {round(time.time() - start, 4)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f57757a1",
   "language": "python",
   "display_name": "PyCharm (rs-via-gnn)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}