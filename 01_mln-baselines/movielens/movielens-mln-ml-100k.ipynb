{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MovieLens MLN Recommendation via PyTorch\n",
    "\n",
    "adapted from https://github.com/fastai/fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [],
   "source": [
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "\n",
    "RANDOM_STATE = 2021\n",
    "set_random_seed(RANDOM_STATE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    files = {}\n",
    "    path = Path(path)\n",
    "    for filename in path.glob('*'):\n",
    "        if filename.suffix == '.csv':\n",
    "            files[filename.stem] = pd.read_csv(filename)\n",
    "        elif filename.suffix == '.dat':\n",
    "            if filename.stem == 'ratings':\n",
    "                columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "            else:\n",
    "                columns = ['movieId', 'title', 'genres']\n",
    "            data = pd.read_csv(filename, sep='::', names=columns, engine='python')\n",
    "            files[filename.stem] = data\n",
    "        elif filename.suffix == '.data':  # ml-100k\n",
    "            if filename.stem == 'u':\n",
    "                columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "                data = pd.read_csv(filename, sep='\\t', names=columns, engine='python')\n",
    "                files['ratings'] = data\n",
    "        elif filename.suffix == '.item':  # ml-100k\n",
    "            if filename.stem == 'u':\n",
    "                columns = ['movieId', 'title', 'genres']\n",
    "                data = pd.read_csv(filename, sep='|', names=columns, engine='python')\n",
    "                files['movies'] = data\n",
    "    return files['ratings'], files['movies']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "# pick one of the available folders\n",
    "ratings, movies = read_data('/home/weiss/rs_data/ml-100k')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [
    {
     "data": {
      "text/plain": "   userId  movieId  rating  timestamp\n0     196      242       3  881250949\n1     186      302       3  891717742\n2      22      377       1  878887116\n3     244       51       2  880606923\n4     166      346       1  886397596",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>242</td>\n      <td>3</td>\n      <td>881250949</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>186</td>\n      <td>302</td>\n      <td>3</td>\n      <td>891717742</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22</td>\n      <td>377</td>\n      <td>1</td>\n      <td>878887116</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>244</td>\n      <td>51</td>\n      <td>2</td>\n      <td>880606923</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>166</td>\n      <td>346</td>\n      <td>1</td>\n      <td>886397596</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                                                                        movieId  \\\n1 Toy Story (1995)  01-Jan-1995 NaN http://us.imdb.com/M/title-exact?Toy%20Story%20... 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0        0   \n2 GoldenEye (1995)  01-Jan-1995 NaN http://us.imdb.com/M/title-exact?GoldenEye%20(1... 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0        1   \n3 Four Rooms (1995) 01-Jan-1995 NaN http://us.imdb.com/M/title-exact?Four%20Rooms%2... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0        1   \n4 Get Shorty (1995) 01-Jan-1995 NaN http://us.imdb.com/M/title-exact?Get%20Shorty%2... 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0        0   \n5 Copycat (1995)    01-Jan-1995 NaN http://us.imdb.com/M/title-exact?Copycat%20(1995)  0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0        1   \n\n                                                                                                                        title  \\\n1 Toy Story (1995)  01-Jan-1995 NaN http://us.imdb.com/M/title-exact?Toy%20Story%20... 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0      0   \n2 GoldenEye (1995)  01-Jan-1995 NaN http://us.imdb.com/M/title-exact?GoldenEye%20(1... 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0      0   \n3 Four Rooms (1995) 01-Jan-1995 NaN http://us.imdb.com/M/title-exact?Four%20Rooms%2... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0      0   \n4 Get Shorty (1995) 01-Jan-1995 NaN http://us.imdb.com/M/title-exact?Get%20Shorty%2... 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0      0   \n5 Copycat (1995)    01-Jan-1995 NaN http://us.imdb.com/M/title-exact?Copycat%20(1995)  0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0      0   \n\n                                                                                                                        genres  \n1 Toy Story (1995)  01-Jan-1995 NaN http://us.imdb.com/M/title-exact?Toy%20Story%20... 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0       0  \n2 GoldenEye (1995)  01-Jan-1995 NaN http://us.imdb.com/M/title-exact?GoldenEye%20(1... 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0       0  \n3 Four Rooms (1995) 01-Jan-1995 NaN http://us.imdb.com/M/title-exact?Four%20Rooms%2... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0       0  \n4 Get Shorty (1995) 01-Jan-1995 NaN http://us.imdb.com/M/title-exact?Get%20Shorty%2... 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0       0  \n5 Copycat (1995)    01-Jan-1995 NaN http://us.imdb.com/M/title-exact?Copycat%20(1995)  0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>movieId</th>\n      <th>title</th>\n      <th>genres</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <th>Toy Story (1995)</th>\n      <th>01-Jan-1995</th>\n      <th>NaN</th>\n      <th>http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <th>GoldenEye (1995)</th>\n      <th>01-Jan-1995</th>\n      <th>NaN</th>\n      <th>http://us.imdb.com/M/title-exact?GoldenEye%20(1995)</th>\n      <th>0</th>\n      <th>1</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <th>Four Rooms (1995)</th>\n      <th>01-Jan-1995</th>\n      <th>NaN</th>\n      <th>http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <th>Get Shorty (1995)</th>\n      <th>01-Jan-1995</th>\n      <th>NaN</th>\n      <th>http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)</th>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <th>Copycat (1995)</th>\n      <th>01-Jan-1995</th>\n      <th>NaN</th>\n      <th>http://us.imdb.com/M/title-exact?Copycat%20(1995)</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 943 users, 1682 movies\n",
      "Dataset shape: (100000, 2)\n",
      "Target shape: (100000,)\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(ratings, top=None):\n",
    "    if top is not None:\n",
    "        ratings.groupby('userId')['rating'].count()\n",
    "\n",
    "    unique_users = ratings.userId.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.userId.map(user_to_index)\n",
    "\n",
    "    unique_movies = ratings.movieId.unique()\n",
    "    movie_to_index = {old: new for new, old in enumerate(unique_movies)}\n",
    "    new_movies = ratings.movieId.map(movie_to_index)\n",
    "\n",
    "    n_users = unique_users.shape[0]\n",
    "    n_movies = unique_movies.shape[0]\n",
    "\n",
    "    X = pd.DataFrame({'user_id': new_users, 'movie_id': new_movies})\n",
    "    y = ratings['rating'].astype(np.float32)\n",
    "    return (n_users, n_movies), (X, y), (user_to_index, movie_to_index)\n",
    "\n",
    "(n, m), (X, y), _ = create_dataset(ratings)\n",
    "print(f'Embeddings: {n} users, {m} movies')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=RANDOM_STATE)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, random_state=RANDOM_STATE)\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid), 'test': (X_test, y_test)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_valid), 'test': len(X_test)}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [],
   "source": [
    "class RatingsIterator:\n",
    "\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [],
   "source": [
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in RatingsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [],
   "source": [
    "class RecommenderNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates dense MLN with embedding layers.\n",
    "\n",
    "    Args:\n",
    "        n_users:\n",
    "            Number of unique users in the dataset.\n",
    "\n",
    "        n_movies:\n",
    "            Number of unique movies in the dataset.\n",
    "\n",
    "        n_factors:\n",
    "            Number of columns in the embeddings matrix.\n",
    "\n",
    "        embedding_dropout:\n",
    "            Dropout rate to apply right after embeddings layer.\n",
    "\n",
    "        hidden:\n",
    "            A single integer or a list of integers defining the number of\n",
    "            units in hidden layer(s).\n",
    "\n",
    "        dropouts:\n",
    "            A single integer or a list of integers defining the dropout\n",
    "            layers rates applied right after each of hidden layers.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_movies,\n",
    "                 n_factors=50, embedding_dropout=0.02,\n",
    "                 hidden=10, dropouts=0.2):\n",
    "\n",
    "        super().__init__()\n",
    "        hidden = get_list(hidden)\n",
    "        dropouts = get_list(dropouts)\n",
    "        n_last = hidden[-1]\n",
    "\n",
    "        def gen_layers(n_in):\n",
    "            \"\"\"\n",
    "            A generator that yields a sequence of hidden layers and\n",
    "            their activations/dropouts.\n",
    "\n",
    "            Note that the function captures `hidden` and `dropouts`\n",
    "            values from the outer scope.\n",
    "            \"\"\"\n",
    "            nonlocal hidden, dropouts\n",
    "            assert len(dropouts) <= len(hidden)\n",
    "\n",
    "            for n_out, rate in zip_longest(hidden, dropouts):\n",
    "                yield nn.Linear(n_in, n_out)\n",
    "                yield nn.ReLU()\n",
    "                if rate is not None and rate > 0.:\n",
    "                    yield nn.Dropout(rate)\n",
    "                n_in = n_out\n",
    "\n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        self.m = nn.Embedding(n_movies, n_factors)\n",
    "        self.drop = nn.Dropout(embedding_dropout)\n",
    "        self.hidden = nn.Sequential(*list(gen_layers(n_factors * 2)))\n",
    "        self.fc = nn.Linear(n_last, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, users, movies, minmax=None):\n",
    "        features = torch.cat([self.u(users), self.m(movies)], dim=1)\n",
    "        x = self.drop(features)\n",
    "        x = self.hidden(x)\n",
    "        out = torch.sigmoid(self.fc(x))\n",
    "        #out = self.relu(self.fc(x))  # relu delivers worse rsme\n",
    "        if minmax is not None:\n",
    "            min_rating, max_rating = minmax\n",
    "            out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n",
    "        return out\n",
    "\n",
    "    def _init(self):\n",
    "        \"\"\"\n",
    "        Setup embeddings and hidden layers with reasonable initial values.\n",
    "        \"\"\"\n",
    "\n",
    "        def init(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        self.u.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.m.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.hidden.apply(init)\n",
    "        init(self.fc)\n",
    "\n",
    "\n",
    "def get_list(n):\n",
    "    if isinstance(n, (int, float)):\n",
    "        return [n]\n",
    "    elif hasattr(n, '__iter__'):\n",
    "        return list(n)\n",
    "    raise TypeError('layers configuraiton should be a single number or a list of numbers')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "\n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + math.cos(math.pi*t/t_max))/2\n",
    "    return scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [],
   "source": [
    "def plot_lr(schedule):\n",
    "    ts = list(range(1000))\n",
    "    y = [schedule(t, 0.001) for t in ts]\n",
    "    plt.plot(ts, y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [
    {
     "data": {
      "text/plain": "(1.0, 5.0)"
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax = float(ratings.rating.min()), float(ratings.rating.max())\n",
    "minmax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [],
   "source": [
    "# small net\n",
    "\n",
    "start = time.time()\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_movies=m,\n",
    "    n_factors=10, hidden=[10],\n",
    "    embedding_dropout=0.05, dropouts=[0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "#iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "#scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 1.4755 - val: 1.2107\n",
      "loss improvement on epoch: 2\n",
      "[002/300] train: 1.3701 - val: 1.1408\n",
      "loss improvement on epoch: 3\n",
      "[003/300] train: 1.2979 - val: 1.0829\n",
      "loss improvement on epoch: 4\n",
      "[004/300] train: 1.2267 - val: 1.0260\n",
      "loss improvement on epoch: 5\n",
      "[005/300] train: 1.1607 - val: 0.9740\n",
      "loss improvement on epoch: 6\n",
      "[006/300] train: 1.1016 - val: 0.9320\n",
      "loss improvement on epoch: 7\n",
      "[007/300] train: 1.0467 - val: 0.8983\n",
      "loss improvement on epoch: 8\n",
      "[008/300] train: 1.0028 - val: 0.8624\n",
      "loss improvement on epoch: 9\n",
      "[009/300] train: 0.9636 - val: 0.8409\n",
      "loss improvement on epoch: 10\n",
      "[010/300] train: 0.9354 - val: 0.8223\n",
      "loss improvement on epoch: 11\n",
      "[011/300] train: 0.9161 - val: 0.8177\n",
      "loss improvement on epoch: 12\n",
      "[012/300] train: 0.8987 - val: 0.8077\n",
      "loss improvement on epoch: 13\n",
      "[013/300] train: 0.8905 - val: 0.8053\n",
      "loss improvement on epoch: 14\n",
      "[014/300] train: 0.8821 - val: 0.7990\n",
      "loss improvement on epoch: 15\n",
      "[015/300] train: 0.8788 - val: 0.7949\n",
      "loss improvement on epoch: 16\n",
      "[016/300] train: 0.8737 - val: 0.7850\n",
      "[017/300] train: 0.8627 - val: 0.7880\n",
      "[018/300] train: 0.8614 - val: 0.7886\n",
      "[019/300] train: 0.8576 - val: 0.7880\n",
      "[020/300] train: 0.8513 - val: 0.7915\n",
      "loss improvement on epoch: 21\n",
      "[021/300] train: 0.8539 - val: 0.7822\n",
      "[022/300] train: 0.8461 - val: 0.7852\n",
      "[023/300] train: 0.8391 - val: 0.7832\n",
      "loss improvement on epoch: 24\n",
      "[024/300] train: 0.8428 - val: 0.7817\n",
      "loss improvement on epoch: 25\n",
      "[025/300] train: 0.8406 - val: 0.7788\n",
      "[026/300] train: 0.8332 - val: 0.7798\n",
      "loss improvement on epoch: 27\n",
      "[027/300] train: 0.8346 - val: 0.7739\n",
      "[028/300] train: 0.8267 - val: 0.7772\n",
      "[029/300] train: 0.8303 - val: 0.7814\n",
      "[030/300] train: 0.8250 - val: 0.7748\n",
      "[031/300] train: 0.8231 - val: 0.7791\n",
      "loss improvement on epoch: 32\n",
      "[032/300] train: 0.8189 - val: 0.7684\n",
      "[033/300] train: 0.8235 - val: 0.7753\n",
      "[034/300] train: 0.8177 - val: 0.7699\n",
      "[035/300] train: 0.8193 - val: 0.7740\n",
      "[036/300] train: 0.8146 - val: 0.7750\n",
      "[037/300] train: 0.8119 - val: 0.7718\n",
      "[038/300] train: 0.8135 - val: 0.7756\n",
      "[039/300] train: 0.8074 - val: 0.7717\n",
      "[040/300] train: 0.8075 - val: 0.7777\n",
      "[041/300] train: 0.8056 - val: 0.7687\n",
      "[042/300] train: 0.8051 - val: 0.7731\n",
      "[043/300] train: 0.8008 - val: 0.7727\n",
      "[044/300] train: 0.8057 - val: 0.7717\n",
      "[045/300] train: 0.8005 - val: 0.7708\n",
      "[046/300] train: 0.8009 - val: 0.7730\n",
      "[047/300] train: 0.7988 - val: 0.7722\n",
      "loss improvement on epoch: 48\n",
      "[048/300] train: 0.7970 - val: 0.7642\n",
      "[049/300] train: 0.7948 - val: 0.7672\n",
      "[050/300] train: 0.7943 - val: 0.7723\n",
      "[051/300] train: 0.7967 - val: 0.7697\n",
      "loss improvement on epoch: 52\n",
      "[052/300] train: 0.7900 - val: 0.7593\n",
      "[053/300] train: 0.7883 - val: 0.7695\n",
      "[054/300] train: 0.7912 - val: 0.7734\n",
      "[055/300] train: 0.7902 - val: 0.7704\n",
      "[056/300] train: 0.7870 - val: 0.7709\n",
      "[057/300] train: 0.7860 - val: 0.7668\n",
      "[058/300] train: 0.7817 - val: 0.7681\n",
      "[059/300] train: 0.7813 - val: 0.7750\n",
      "[060/300] train: 0.7837 - val: 0.7700\n",
      "[061/300] train: 0.7799 - val: 0.7674\n",
      "[062/300] train: 0.7788 - val: 0.7732\n",
      "[063/300] train: 0.7807 - val: 0.7633\n",
      "[064/300] train: 0.7817 - val: 0.7659\n",
      "[065/300] train: 0.7785 - val: 0.7686\n",
      "[066/300] train: 0.7776 - val: 0.7733\n",
      "[067/300] train: 0.7762 - val: 0.7691\n",
      "[068/300] train: 0.7751 - val: 0.7721\n",
      "[069/300] train: 0.7729 - val: 0.7671\n",
      "[070/300] train: 0.7707 - val: 0.7704\n",
      "[071/300] train: 0.7737 - val: 0.7661\n",
      "[072/300] train: 0.7713 - val: 0.7697\n",
      "[073/300] train: 0.7689 - val: 0.7720\n",
      "[074/300] train: 0.7678 - val: 0.7703\n",
      "[075/300] train: 0.7686 - val: 0.7706\n",
      "[076/300] train: 0.7672 - val: 0.7712\n",
      "[077/300] train: 0.7695 - val: 0.7711\n",
      "[078/300] train: 0.7654 - val: 0.7706\n",
      "[079/300] train: 0.7675 - val: 0.7709\n",
      "[080/300] train: 0.7657 - val: 0.7773\n",
      "[081/300] train: 0.7644 - val: 0.7711\n",
      "[082/300] train: 0.7655 - val: 0.7720\n",
      "[083/300] train: 0.7659 - val: 0.7710\n",
      "[084/300] train: 0.7634 - val: 0.7717\n",
      "[085/300] train: 0.7607 - val: 0.7683\n",
      "[086/300] train: 0.7579 - val: 0.7773\n",
      "[087/300] train: 0.7614 - val: 0.7700\n",
      "[088/300] train: 0.7548 - val: 0.7697\n",
      "[089/300] train: 0.7564 - val: 0.7727\n",
      "[090/300] train: 0.7603 - val: 0.7728\n",
      "[091/300] train: 0.7563 - val: 0.7705\n",
      "[092/300] train: 0.7569 - val: 0.7765\n",
      "[093/300] train: 0.7564 - val: 0.7730\n",
      "[094/300] train: 0.7550 - val: 0.7679\n",
      "[095/300] train: 0.7534 - val: 0.7742\n",
      "[096/300] train: 0.7536 - val: 0.7769\n",
      "[097/300] train: 0.7526 - val: 0.7677\n",
      "[098/300] train: 0.7540 - val: 0.7730\n",
      "[099/300] train: 0.7530 - val: 0.7764\n",
      "[100/300] train: 0.7552 - val: 0.7716\n",
      "[101/300] train: 0.7521 - val: 0.7711\n",
      "[102/300] train: 0.7530 - val: 0.7749\n",
      "[103/300] train: 0.7519 - val: 0.7698\n",
      "[104/300] train: 0.7470 - val: 0.7752\n",
      "[105/300] train: 0.7490 - val: 0.7753\n",
      "[106/300] train: 0.7501 - val: 0.7760\n",
      "[107/300] train: 0.7478 - val: 0.7751\n",
      "[108/300] train: 0.7501 - val: 0.7779\n",
      "[109/300] train: 0.7472 - val: 0.7748\n",
      "[110/300] train: 0.7463 - val: 0.7754\n",
      "[111/300] train: 0.7476 - val: 0.7710\n",
      "[112/300] train: 0.7460 - val: 0.7695\n",
      "[113/300] train: 0.7493 - val: 0.7796\n",
      "[114/300] train: 0.7485 - val: 0.7727\n",
      "[115/300] train: 0.7477 - val: 0.7797\n",
      "[116/300] train: 0.7434 - val: 0.7790\n",
      "[117/300] train: 0.7454 - val: 0.7773\n",
      "[118/300] train: 0.7464 - val: 0.7789\n",
      "[119/300] train: 0.7433 - val: 0.7735\n",
      "[120/300] train: 0.7420 - val: 0.7712\n",
      "[121/300] train: 0.7433 - val: 0.7734\n",
      "[122/300] train: 0.7429 - val: 0.7774\n",
      "[123/300] train: 0.7426 - val: 0.7758\n",
      "[124/300] train: 0.7428 - val: 0.7754\n",
      "[125/300] train: 0.7425 - val: 0.7773\n",
      "[126/300] train: 0.7446 - val: 0.7827\n",
      "[127/300] train: 0.7420 - val: 0.7829\n",
      "[128/300] train: 0.7412 - val: 0.7747\n",
      "[129/300] train: 0.7364 - val: 0.7708\n",
      "[130/300] train: 0.7373 - val: 0.7777\n",
      "[131/300] train: 0.7398 - val: 0.7824\n",
      "[132/300] train: 0.7401 - val: 0.7808\n",
      "[133/300] train: 0.7403 - val: 0.7804\n",
      "[134/300] train: 0.7412 - val: 0.7759\n",
      "[135/300] train: 0.7393 - val: 0.7780\n",
      "[136/300] train: 0.7409 - val: 0.7788\n",
      "[137/300] train: 0.7368 - val: 0.7785\n",
      "[138/300] train: 0.7374 - val: 0.7763\n",
      "[139/300] train: 0.7355 - val: 0.7866\n",
      "[140/300] train: 0.7366 - val: 0.7812\n",
      "[141/300] train: 0.7346 - val: 0.7797\n",
      "[142/300] train: 0.7368 - val: 0.7754\n",
      "[143/300] train: 0.7399 - val: 0.7782\n",
      "[144/300] train: 0.7323 - val: 0.7802\n",
      "[145/300] train: 0.7356 - val: 0.7767\n",
      "[146/300] train: 0.7323 - val: 0.7874\n",
      "[147/300] train: 0.7329 - val: 0.7732\n",
      "[148/300] train: 0.7290 - val: 0.7783\n",
      "[149/300] train: 0.7341 - val: 0.7731\n",
      "[150/300] train: 0.7339 - val: 0.7769\n",
      "[151/300] train: 0.7329 - val: 0.7740\n",
      "[152/300] train: 0.7334 - val: 0.7761\n",
      "[153/300] train: 0.7342 - val: 0.7837\n",
      "[154/300] train: 0.7292 - val: 0.7805\n",
      "[155/300] train: 0.7325 - val: 0.7782\n",
      "[156/300] train: 0.7326 - val: 0.7792\n",
      "[157/300] train: 0.7331 - val: 0.7796\n",
      "[158/300] train: 0.7328 - val: 0.7834\n",
      "[159/300] train: 0.7346 - val: 0.7828\n",
      "[160/300] train: 0.7343 - val: 0.7850\n",
      "[161/300] train: 0.7350 - val: 0.7850\n",
      "[162/300] train: 0.7301 - val: 0.7750\n",
      "[163/300] train: 0.7327 - val: 0.7807\n",
      "[164/300] train: 0.7320 - val: 0.7847\n",
      "[165/300] train: 0.7314 - val: 0.7791\n",
      "[166/300] train: 0.7304 - val: 0.7829\n",
      "[167/300] train: 0.7280 - val: 0.7766\n",
      "[168/300] train: 0.7282 - val: 0.7780\n",
      "[169/300] train: 0.7268 - val: 0.7749\n",
      "[170/300] train: 0.7296 - val: 0.7771\n",
      "[171/300] train: 0.7272 - val: 0.7789\n",
      "[172/300] train: 0.7270 - val: 0.7793\n",
      "[173/300] train: 0.7273 - val: 0.7805\n",
      "[174/300] train: 0.7274 - val: 0.7756\n",
      "[175/300] train: 0.7285 - val: 0.7828\n",
      "[176/300] train: 0.7239 - val: 0.7798\n",
      "[177/300] train: 0.7268 - val: 0.7739\n",
      "[178/300] train: 0.7300 - val: 0.7810\n",
      "[179/300] train: 0.7277 - val: 0.7779\n",
      "[180/300] train: 0.7267 - val: 0.7757\n",
      "[181/300] train: 0.7247 - val: 0.7780\n",
      "[182/300] train: 0.7279 - val: 0.7746\n",
      "[183/300] train: 0.7260 - val: 0.7800\n",
      "[184/300] train: 0.7274 - val: 0.7807\n",
      "[185/300] train: 0.7253 - val: 0.7747\n",
      "[186/300] train: 0.7258 - val: 0.7793\n",
      "[187/300] train: 0.7247 - val: 0.7820\n",
      "[188/300] train: 0.7253 - val: 0.7785\n",
      "[189/300] train: 0.7255 - val: 0.7844\n",
      "[190/300] train: 0.7252 - val: 0.7848\n",
      "[191/300] train: 0.7247 - val: 0.7821\n",
      "[192/300] train: 0.7247 - val: 0.7837\n",
      "[193/300] train: 0.7276 - val: 0.7809\n",
      "[194/300] train: 0.7216 - val: 0.7782\n",
      "[195/300] train: 0.7245 - val: 0.7809\n",
      "[196/300] train: 0.7235 - val: 0.7781\n",
      "[197/300] train: 0.7216 - val: 0.7761\n",
      "[198/300] train: 0.7249 - val: 0.7757\n",
      "[199/300] train: 0.7236 - val: 0.7845\n",
      "[200/300] train: 0.7264 - val: 0.7766\n",
      "[201/300] train: 0.7209 - val: 0.7800\n",
      "[202/300] train: 0.7246 - val: 0.7815\n",
      "[203/300] train: 0.7242 - val: 0.7814\n",
      "[204/300] train: 0.7225 - val: 0.7821\n",
      "[205/300] train: 0.7216 - val: 0.7803\n",
      "[206/300] train: 0.7183 - val: 0.7790\n",
      "[207/300] train: 0.7208 - val: 0.7862\n",
      "[208/300] train: 0.7222 - val: 0.7854\n",
      "[209/300] train: 0.7254 - val: 0.7836\n",
      "[210/300] train: 0.7236 - val: 0.7809\n",
      "[211/300] train: 0.7195 - val: 0.7848\n",
      "[212/300] train: 0.7216 - val: 0.7850\n",
      "[213/300] train: 0.7185 - val: 0.7801\n",
      "[214/300] train: 0.7206 - val: 0.7831\n",
      "[215/300] train: 0.7214 - val: 0.7766\n",
      "[216/300] train: 0.7215 - val: 0.7873\n",
      "[217/300] train: 0.7243 - val: 0.7838\n",
      "[218/300] train: 0.7230 - val: 0.7840\n",
      "[219/300] train: 0.7203 - val: 0.7836\n",
      "[220/300] train: 0.7208 - val: 0.7830\n",
      "[221/300] train: 0.7233 - val: 0.7901\n",
      "[222/300] train: 0.7190 - val: 0.7783\n",
      "[223/300] train: 0.7166 - val: 0.7811\n",
      "[224/300] train: 0.7181 - val: 0.7807\n",
      "[225/300] train: 0.7189 - val: 0.7871\n",
      "[226/300] train: 0.7193 - val: 0.7903\n",
      "[227/300] train: 0.7221 - val: 0.7857\n",
      "[228/300] train: 0.7193 - val: 0.7816\n",
      "[229/300] train: 0.7204 - val: 0.7819\n",
      "[230/300] train: 0.7175 - val: 0.7840\n",
      "[231/300] train: 0.7211 - val: 0.7828\n",
      "[232/300] train: 0.7215 - val: 0.7845\n",
      "[233/300] train: 0.7178 - val: 0.7840\n",
      "[234/300] train: 0.7208 - val: 0.7810\n",
      "[235/300] train: 0.7179 - val: 0.7888\n",
      "[236/300] train: 0.7163 - val: 0.7784\n",
      "[237/300] train: 0.7208 - val: 0.7780\n",
      "[238/300] train: 0.7166 - val: 0.7819\n",
      "[239/300] train: 0.7176 - val: 0.7883\n",
      "[240/300] train: 0.7184 - val: 0.7835\n",
      "[241/300] train: 0.7181 - val: 0.7857\n",
      "[242/300] train: 0.7189 - val: 0.7844\n",
      "[243/300] train: 0.7153 - val: 0.7871\n",
      "[244/300] train: 0.7209 - val: 0.7796\n",
      "[245/300] train: 0.7161 - val: 0.7811\n",
      "[246/300] train: 0.7145 - val: 0.7820\n",
      "[247/300] train: 0.7184 - val: 0.7860\n",
      "[248/300] train: 0.7134 - val: 0.7826\n",
      "[249/300] train: 0.7175 - val: 0.7852\n",
      "[250/300] train: 0.7150 - val: 0.7838\n",
      "[251/300] train: 0.7174 - val: 0.7846\n",
      "[252/300] train: 0.7149 - val: 0.7915\n",
      "[253/300] train: 0.7157 - val: 0.7851\n",
      "[254/300] train: 0.7176 - val: 0.7835\n",
      "[255/300] train: 0.7175 - val: 0.7840\n",
      "[256/300] train: 0.7182 - val: 0.7919\n",
      "[257/300] train: 0.7180 - val: 0.7914\n",
      "[258/300] train: 0.7146 - val: 0.7884\n",
      "[259/300] train: 0.7162 - val: 0.7847\n",
      "[260/300] train: 0.7155 - val: 0.7893\n",
      "[261/300] train: 0.7138 - val: 0.7872\n",
      "[262/300] train: 0.7137 - val: 0.7795\n",
      "[263/300] train: 0.7139 - val: 0.7867\n",
      "[264/300] train: 0.7176 - val: 0.7967\n",
      "[265/300] train: 0.7170 - val: 0.7827\n",
      "[266/300] train: 0.7152 - val: 0.7843\n",
      "[267/300] train: 0.7176 - val: 0.7857\n",
      "[268/300] train: 0.7176 - val: 0.7883\n",
      "[269/300] train: 0.7130 - val: 0.7894\n",
      "[270/300] train: 0.7112 - val: 0.7955\n",
      "[271/300] train: 0.7131 - val: 0.7894\n",
      "[272/300] train: 0.7152 - val: 0.7876\n",
      "[273/300] train: 0.7133 - val: 0.7863\n",
      "[274/300] train: 0.7143 - val: 0.7896\n",
      "[275/300] train: 0.7159 - val: 0.7889\n",
      "[276/300] train: 0.7156 - val: 0.7832\n",
      "[277/300] train: 0.7095 - val: 0.7907\n",
      "[278/300] train: 0.7113 - val: 0.7889\n",
      "[279/300] train: 0.7142 - val: 0.7891\n",
      "[280/300] train: 0.7147 - val: 0.7906\n",
      "[281/300] train: 0.7132 - val: 0.7879\n",
      "[282/300] train: 0.7130 - val: 0.7875\n",
      "[283/300] train: 0.7111 - val: 0.7924\n",
      "[284/300] train: 0.7096 - val: 0.7787\n",
      "[285/300] train: 0.7140 - val: 0.8043\n",
      "[286/300] train: 0.7108 - val: 0.7870\n",
      "[287/300] train: 0.7091 - val: 0.7866\n",
      "[288/300] train: 0.7103 - val: 0.7911\n",
      "[289/300] train: 0.7159 - val: 0.7841\n",
      "[290/300] train: 0.7159 - val: 0.7801\n",
      "[291/300] train: 0.7159 - val: 0.7863\n",
      "[292/300] train: 0.7129 - val: 0.7891\n",
      "[293/300] train: 0.7157 - val: 0.7907\n",
      "[294/300] train: 0.7116 - val: 0.7871\n",
      "[295/300] train: 0.7154 - val: 0.7783\n",
      "[296/300] train: 0.7119 - val: 0.7874\n",
      "[297/300] train: 0.7114 - val: 0.7863\n",
      "[298/300] train: 0.7139 - val: 0.7860\n",
      "[299/300] train: 0.7107 - val: 0.7857\n",
      "[300/300] train: 0.7077 - val: 0.7900\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    #scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #lr_history.extend(scheduler.get_lr())\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [],
   "source": [
    "# Plot decreasing learning rate\n",
    "\n",
    "#_ = plt.plot(lr_history[:2*iterations_per_epoch-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.9684\n"
     ]
    }
   ],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.9777\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "outputs": [],
   "source": [
    "with open('best.weights.small', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 24.5885\n"
     ]
    }
   ],
   "source": [
    "print(f'duration: {round(time.time() - start, 4)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "outputs": [],
   "source": [
    "# medium net\n",
    "\n",
    "start = time.time()\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_movies=m,\n",
    "    n_factors=20, hidden=[10 ,10],\n",
    "    embedding_dropout=0.05, dropouts=[0.3, 0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "#iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "#scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 1.4455 - val: 1.1931\n",
      "loss improvement on epoch: 2\n",
      "[002/300] train: 1.3553 - val: 1.1317\n",
      "loss improvement on epoch: 3\n",
      "[003/300] train: 1.2826 - val: 1.0726\n",
      "loss improvement on epoch: 4\n",
      "[004/300] train: 1.2135 - val: 1.0143\n",
      "loss improvement on epoch: 5\n",
      "[005/300] train: 1.1445 - val: 0.9702\n",
      "loss improvement on epoch: 6\n",
      "[006/300] train: 1.0798 - val: 0.9146\n",
      "loss improvement on epoch: 7\n",
      "[007/300] train: 1.0215 - val: 0.8867\n",
      "loss improvement on epoch: 8\n",
      "[008/300] train: 0.9853 - val: 0.8592\n",
      "loss improvement on epoch: 9\n",
      "[009/300] train: 0.9592 - val: 0.8509\n",
      "loss improvement on epoch: 10\n",
      "[010/300] train: 0.9384 - val: 0.8359\n",
      "loss improvement on epoch: 11\n",
      "[011/300] train: 0.9206 - val: 0.8331\n",
      "loss improvement on epoch: 12\n",
      "[012/300] train: 0.9142 - val: 0.8207\n",
      "loss improvement on epoch: 13\n",
      "[013/300] train: 0.8993 - val: 0.8149\n",
      "loss improvement on epoch: 14\n",
      "[014/300] train: 0.8870 - val: 0.8119\n",
      "loss improvement on epoch: 15\n",
      "[015/300] train: 0.8807 - val: 0.8046\n",
      "loss improvement on epoch: 16\n",
      "[016/300] train: 0.8750 - val: 0.8013\n",
      "loss improvement on epoch: 17\n",
      "[017/300] train: 0.8665 - val: 0.7912\n",
      "[018/300] train: 0.8595 - val: 0.8004\n",
      "[019/300] train: 0.8540 - val: 0.7930\n",
      "loss improvement on epoch: 20\n",
      "[020/300] train: 0.8504 - val: 0.7904\n",
      "loss improvement on epoch: 21\n",
      "[021/300] train: 0.8446 - val: 0.7892\n",
      "[022/300] train: 0.8430 - val: 0.7915\n",
      "[023/300] train: 0.8366 - val: 0.7918\n",
      "loss improvement on epoch: 24\n",
      "[024/300] train: 0.8327 - val: 0.7846\n",
      "[025/300] train: 0.8276 - val: 0.7853\n",
      "[026/300] train: 0.8268 - val: 0.7875\n",
      "loss improvement on epoch: 27\n",
      "[027/300] train: 0.8201 - val: 0.7807\n",
      "[028/300] train: 0.8211 - val: 0.7856\n",
      "[029/300] train: 0.8176 - val: 0.7839\n",
      "[030/300] train: 0.8171 - val: 0.7826\n",
      "[031/300] train: 0.8165 - val: 0.7854\n",
      "[032/300] train: 0.8097 - val: 0.7891\n",
      "[033/300] train: 0.8095 - val: 0.7810\n",
      "loss improvement on epoch: 34\n",
      "[034/300] train: 0.8111 - val: 0.7767\n",
      "[035/300] train: 0.8049 - val: 0.7889\n",
      "[036/300] train: 0.8047 - val: 0.7871\n",
      "[037/300] train: 0.8022 - val: 0.7836\n",
      "[038/300] train: 0.8034 - val: 0.7888\n",
      "[039/300] train: 0.8011 - val: 0.7788\n",
      "[040/300] train: 0.7997 - val: 0.7846\n",
      "[041/300] train: 0.7953 - val: 0.7883\n",
      "[042/300] train: 0.7948 - val: 0.7842\n",
      "[043/300] train: 0.7925 - val: 0.7802\n",
      "[044/300] train: 0.7941 - val: 0.7849\n",
      "[045/300] train: 0.7911 - val: 0.7812\n",
      "[046/300] train: 0.7890 - val: 0.7810\n",
      "[047/300] train: 0.7889 - val: 0.7799\n",
      "[048/300] train: 0.7838 - val: 0.7805\n",
      "[049/300] train: 0.7853 - val: 0.7832\n",
      "[050/300] train: 0.7874 - val: 0.7847\n",
      "[051/300] train: 0.7825 - val: 0.7836\n",
      "[052/300] train: 0.7790 - val: 0.7821\n",
      "[053/300] train: 0.7809 - val: 0.7829\n",
      "[054/300] train: 0.7800 - val: 0.7864\n",
      "[055/300] train: 0.7782 - val: 0.7882\n",
      "[056/300] train: 0.7759 - val: 0.7880\n",
      "[057/300] train: 0.7776 - val: 0.7797\n",
      "[058/300] train: 0.7713 - val: 0.7884\n",
      "[059/300] train: 0.7722 - val: 0.7875\n",
      "[060/300] train: 0.7710 - val: 0.7828\n",
      "[061/300] train: 0.7713 - val: 0.7860\n",
      "[062/300] train: 0.7708 - val: 0.7876\n",
      "[063/300] train: 0.7666 - val: 0.7864\n",
      "[064/300] train: 0.7658 - val: 0.7871\n",
      "[065/300] train: 0.7694 - val: 0.7914\n",
      "[066/300] train: 0.7658 - val: 0.7915\n",
      "[067/300] train: 0.7653 - val: 0.7885\n",
      "[068/300] train: 0.7665 - val: 0.7892\n",
      "[069/300] train: 0.7650 - val: 0.7886\n",
      "[070/300] train: 0.7635 - val: 0.7874\n",
      "[071/300] train: 0.7651 - val: 0.7818\n",
      "[072/300] train: 0.7636 - val: 0.7893\n",
      "[073/300] train: 0.7619 - val: 0.7910\n",
      "[074/300] train: 0.7620 - val: 0.7884\n",
      "[075/300] train: 0.7594 - val: 0.7843\n",
      "[076/300] train: 0.7563 - val: 0.7907\n",
      "[077/300] train: 0.7564 - val: 0.7795\n",
      "[078/300] train: 0.7602 - val: 0.7905\n",
      "[079/300] train: 0.7568 - val: 0.7918\n",
      "[080/300] train: 0.7541 - val: 0.7886\n",
      "[081/300] train: 0.7574 - val: 0.7886\n",
      "[082/300] train: 0.7591 - val: 0.7881\n",
      "[083/300] train: 0.7552 - val: 0.7944\n",
      "[084/300] train: 0.7525 - val: 0.7909\n",
      "[085/300] train: 0.7523 - val: 0.7884\n",
      "[086/300] train: 0.7548 - val: 0.7799\n",
      "[087/300] train: 0.7529 - val: 0.7927\n",
      "[088/300] train: 0.7520 - val: 0.7886\n",
      "[089/300] train: 0.7492 - val: 0.7870\n",
      "[090/300] train: 0.7531 - val: 0.7898\n",
      "[091/300] train: 0.7488 - val: 0.7911\n",
      "[092/300] train: 0.7498 - val: 0.7928\n",
      "[093/300] train: 0.7474 - val: 0.7861\n",
      "[094/300] train: 0.7488 - val: 0.7879\n",
      "[095/300] train: 0.7496 - val: 0.7914\n",
      "[096/300] train: 0.7454 - val: 0.7910\n",
      "[097/300] train: 0.7491 - val: 0.7913\n",
      "[098/300] train: 0.7503 - val: 0.7925\n",
      "[099/300] train: 0.7477 - val: 0.7966\n",
      "[100/300] train: 0.7440 - val: 0.7869\n",
      "[101/300] train: 0.7464 - val: 0.7958\n",
      "[102/300] train: 0.7500 - val: 0.7942\n",
      "[103/300] train: 0.7450 - val: 0.7916\n",
      "[104/300] train: 0.7435 - val: 0.7932\n",
      "[105/300] train: 0.7393 - val: 0.7923\n",
      "[106/300] train: 0.7402 - val: 0.7963\n",
      "[107/300] train: 0.7407 - val: 0.7862\n",
      "[108/300] train: 0.7432 - val: 0.7904\n",
      "[109/300] train: 0.7356 - val: 0.7888\n",
      "[110/300] train: 0.7370 - val: 0.8034\n",
      "[111/300] train: 0.7358 - val: 0.7977\n",
      "[112/300] train: 0.7374 - val: 0.8021\n",
      "[113/300] train: 0.7384 - val: 0.7866\n",
      "[114/300] train: 0.7350 - val: 0.7917\n",
      "[115/300] train: 0.7393 - val: 0.7980\n",
      "[116/300] train: 0.7339 - val: 0.7996\n",
      "[117/300] train: 0.7376 - val: 0.7992\n",
      "[118/300] train: 0.7359 - val: 0.7939\n",
      "[119/300] train: 0.7320 - val: 0.7984\n",
      "[120/300] train: 0.7370 - val: 0.7944\n",
      "[121/300] train: 0.7348 - val: 0.7989\n",
      "[122/300] train: 0.7370 - val: 0.8001\n",
      "[123/300] train: 0.7335 - val: 0.8018\n",
      "[124/300] train: 0.7284 - val: 0.7972\n",
      "[125/300] train: 0.7305 - val: 0.8001\n",
      "[126/300] train: 0.7336 - val: 0.7967\n",
      "[127/300] train: 0.7353 - val: 0.7983\n",
      "[128/300] train: 0.7371 - val: 0.7989\n",
      "[129/300] train: 0.7377 - val: 0.7963\n",
      "[130/300] train: 0.7304 - val: 0.7962\n",
      "[131/300] train: 0.7322 - val: 0.7977\n",
      "[132/300] train: 0.7365 - val: 0.7982\n",
      "[133/300] train: 0.7276 - val: 0.7944\n",
      "[134/300] train: 0.7317 - val: 0.8031\n",
      "[135/300] train: 0.7324 - val: 0.7977\n",
      "[136/300] train: 0.7296 - val: 0.8018\n",
      "[137/300] train: 0.7348 - val: 0.8015\n",
      "[138/300] train: 0.7268 - val: 0.7918\n",
      "[139/300] train: 0.7309 - val: 0.8037\n",
      "[140/300] train: 0.7334 - val: 0.8010\n",
      "[141/300] train: 0.7304 - val: 0.8022\n",
      "[142/300] train: 0.7322 - val: 0.7974\n",
      "[143/300] train: 0.7315 - val: 0.8047\n",
      "[144/300] train: 0.7284 - val: 0.8005\n",
      "[145/300] train: 0.7293 - val: 0.8012\n",
      "[146/300] train: 0.7226 - val: 0.8025\n",
      "[147/300] train: 0.7282 - val: 0.8021\n",
      "[148/300] train: 0.7304 - val: 0.7989\n",
      "[149/300] train: 0.7285 - val: 0.8051\n",
      "[150/300] train: 0.7275 - val: 0.7999\n",
      "[151/300] train: 0.7241 - val: 0.8063\n",
      "[152/300] train: 0.7266 - val: 0.8049\n",
      "[153/300] train: 0.7222 - val: 0.8002\n",
      "[154/300] train: 0.7244 - val: 0.8015\n",
      "[155/300] train: 0.7283 - val: 0.8025\n",
      "[156/300] train: 0.7268 - val: 0.8035\n",
      "[157/300] train: 0.7207 - val: 0.8008\n",
      "[158/300] train: 0.7292 - val: 0.8023\n",
      "[159/300] train: 0.7245 - val: 0.8066\n",
      "[160/300] train: 0.7226 - val: 0.8039\n",
      "[161/300] train: 0.7272 - val: 0.8106\n",
      "[162/300] train: 0.7221 - val: 0.8018\n",
      "[163/300] train: 0.7235 - val: 0.8018\n",
      "[164/300] train: 0.7273 - val: 0.8014\n",
      "[165/300] train: 0.7250 - val: 0.7995\n",
      "[166/300] train: 0.7264 - val: 0.8022\n",
      "[167/300] train: 0.7265 - val: 0.7981\n",
      "[168/300] train: 0.7230 - val: 0.8064\n",
      "[169/300] train: 0.7251 - val: 0.8031\n",
      "[170/300] train: 0.7273 - val: 0.8015\n",
      "[171/300] train: 0.7205 - val: 0.7992\n",
      "[172/300] train: 0.7249 - val: 0.8052\n",
      "[173/300] train: 0.7213 - val: 0.8108\n",
      "[174/300] train: 0.7241 - val: 0.8085\n",
      "[175/300] train: 0.7219 - val: 0.8021\n",
      "[176/300] train: 0.7237 - val: 0.8004\n",
      "[177/300] train: 0.7194 - val: 0.8032\n",
      "[178/300] train: 0.7243 - val: 0.8062\n",
      "[179/300] train: 0.7197 - val: 0.8141\n",
      "[180/300] train: 0.7205 - val: 0.8049\n",
      "[181/300] train: 0.7242 - val: 0.8046\n",
      "[182/300] train: 0.7213 - val: 0.8050\n",
      "[183/300] train: 0.7224 - val: 0.8017\n",
      "[184/300] train: 0.7210 - val: 0.8045\n",
      "[185/300] train: 0.7181 - val: 0.8113\n",
      "[186/300] train: 0.7222 - val: 0.8050\n",
      "[187/300] train: 0.7215 - val: 0.8029\n",
      "[188/300] train: 0.7212 - val: 0.8064\n",
      "[189/300] train: 0.7178 - val: 0.8034\n",
      "[190/300] train: 0.7209 - val: 0.8021\n",
      "[191/300] train: 0.7220 - val: 0.8123\n",
      "[192/300] train: 0.7190 - val: 0.8092\n",
      "[193/300] train: 0.7208 - val: 0.8085\n",
      "[194/300] train: 0.7184 - val: 0.8051\n",
      "[195/300] train: 0.7236 - val: 0.8049\n",
      "[196/300] train: 0.7185 - val: 0.8065\n",
      "[197/300] train: 0.7204 - val: 0.8015\n",
      "[198/300] train: 0.7213 - val: 0.8070\n",
      "[199/300] train: 0.7196 - val: 0.8039\n",
      "[200/300] train: 0.7198 - val: 0.8133\n",
      "[201/300] train: 0.7193 - val: 0.8113\n",
      "[202/300] train: 0.7195 - val: 0.8109\n",
      "[203/300] train: 0.7245 - val: 0.8108\n",
      "[204/300] train: 0.7123 - val: 0.8122\n",
      "[205/300] train: 0.7180 - val: 0.8100\n",
      "[206/300] train: 0.7232 - val: 0.8076\n",
      "[207/300] train: 0.7183 - val: 0.8106\n",
      "[208/300] train: 0.7174 - val: 0.8080\n",
      "[209/300] train: 0.7239 - val: 0.8029\n",
      "[210/300] train: 0.7183 - val: 0.8080\n",
      "[211/300] train: 0.7195 - val: 0.8150\n",
      "[212/300] train: 0.7189 - val: 0.8114\n",
      "[213/300] train: 0.7222 - val: 0.8084\n",
      "[214/300] train: 0.7177 - val: 0.8075\n",
      "[215/300] train: 0.7191 - val: 0.8157\n",
      "[216/300] train: 0.7198 - val: 0.8106\n",
      "[217/300] train: 0.7158 - val: 0.8186\n",
      "[218/300] train: 0.7173 - val: 0.8093\n",
      "[219/300] train: 0.7150 - val: 0.8239\n",
      "[220/300] train: 0.7179 - val: 0.8063\n",
      "[221/300] train: 0.7165 - val: 0.8138\n",
      "[222/300] train: 0.7147 - val: 0.8098\n",
      "[223/300] train: 0.7187 - val: 0.8142\n",
      "[224/300] train: 0.7124 - val: 0.8176\n",
      "[225/300] train: 0.7165 - val: 0.8052\n",
      "[226/300] train: 0.7170 - val: 0.8096\n",
      "[227/300] train: 0.7131 - val: 0.8069\n",
      "[228/300] train: 0.7138 - val: 0.8084\n",
      "[229/300] train: 0.7157 - val: 0.8145\n",
      "[230/300] train: 0.7145 - val: 0.8100\n",
      "[231/300] train: 0.7155 - val: 0.8097\n",
      "[232/300] train: 0.7138 - val: 0.8042\n",
      "[233/300] train: 0.7157 - val: 0.8114\n",
      "[234/300] train: 0.7178 - val: 0.8121\n",
      "[235/300] train: 0.7147 - val: 0.8104\n",
      "[236/300] train: 0.7138 - val: 0.8099\n",
      "[237/300] train: 0.7110 - val: 0.8152\n",
      "[238/300] train: 0.7142 - val: 0.8153\n",
      "[239/300] train: 0.7172 - val: 0.8145\n",
      "[240/300] train: 0.7169 - val: 0.8104\n",
      "[241/300] train: 0.7126 - val: 0.8131\n",
      "[242/300] train: 0.7139 - val: 0.8221\n",
      "[243/300] train: 0.7110 - val: 0.8134\n",
      "[244/300] train: 0.7175 - val: 0.8182\n",
      "[245/300] train: 0.7148 - val: 0.8176\n",
      "[246/300] train: 0.7137 - val: 0.8228\n",
      "[247/300] train: 0.7118 - val: 0.8160\n",
      "[248/300] train: 0.7098 - val: 0.8146\n",
      "[249/300] train: 0.7071 - val: 0.8166\n",
      "[250/300] train: 0.7123 - val: 0.8122\n",
      "[251/300] train: 0.7124 - val: 0.8152\n",
      "[252/300] train: 0.7130 - val: 0.8191\n",
      "[253/300] train: 0.7124 - val: 0.8120\n",
      "[254/300] train: 0.7161 - val: 0.8178\n",
      "[255/300] train: 0.7112 - val: 0.8153\n",
      "[256/300] train: 0.7107 - val: 0.8173\n",
      "[257/300] train: 0.7102 - val: 0.8066\n",
      "[258/300] train: 0.7135 - val: 0.8120\n",
      "[259/300] train: 0.7127 - val: 0.8224\n",
      "[260/300] train: 0.7138 - val: 0.8191\n",
      "[261/300] train: 0.7091 - val: 0.8189\n",
      "[262/300] train: 0.7111 - val: 0.8183\n",
      "[263/300] train: 0.7140 - val: 0.8158\n",
      "[264/300] train: 0.7110 - val: 0.8124\n",
      "[265/300] train: 0.7086 - val: 0.8129\n",
      "[266/300] train: 0.7136 - val: 0.8120\n",
      "[267/300] train: 0.7125 - val: 0.8124\n",
      "[268/300] train: 0.7162 - val: 0.8102\n",
      "[269/300] train: 0.7125 - val: 0.8154\n",
      "[270/300] train: 0.7097 - val: 0.8177\n",
      "[271/300] train: 0.7107 - val: 0.8166\n",
      "[272/300] train: 0.7147 - val: 0.8150\n",
      "[273/300] train: 0.7107 - val: 0.8182\n",
      "[274/300] train: 0.7143 - val: 0.8131\n",
      "[275/300] train: 0.7138 - val: 0.8135\n",
      "[276/300] train: 0.7113 - val: 0.8246\n",
      "[277/300] train: 0.7113 - val: 0.8167\n",
      "[278/300] train: 0.7109 - val: 0.8186\n",
      "[279/300] train: 0.7130 - val: 0.8206\n",
      "[280/300] train: 0.7111 - val: 0.8217\n",
      "[281/300] train: 0.7072 - val: 0.8245\n",
      "[282/300] train: 0.7117 - val: 0.8198\n",
      "[283/300] train: 0.7067 - val: 0.8275\n",
      "[284/300] train: 0.7116 - val: 0.8144\n",
      "[285/300] train: 0.7124 - val: 0.8163\n",
      "[286/300] train: 0.7137 - val: 0.8144\n",
      "[287/300] train: 0.7118 - val: 0.8146\n",
      "[288/300] train: 0.7109 - val: 0.8103\n",
      "[289/300] train: 0.7077 - val: 0.8093\n",
      "[290/300] train: 0.7093 - val: 0.8161\n",
      "[291/300] train: 0.7048 - val: 0.8235\n",
      "[292/300] train: 0.7085 - val: 0.8224\n",
      "[293/300] train: 0.7092 - val: 0.8201\n",
      "[294/300] train: 0.7084 - val: 0.8262\n",
      "[295/300] train: 0.7089 - val: 0.8189\n",
      "[296/300] train: 0.7084 - val: 0.8214\n",
      "[297/300] train: 0.7068 - val: 0.8238\n",
      "[298/300] train: 0.7089 - val: 0.8204\n",
      "[299/300] train: 0.7057 - val: 0.8272\n",
      "[300/300] train: 0.7118 - val: 0.8192\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    #scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #lr_history.extend(scheduler.get_lr())\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [],
   "source": [
    "# Plot decreasing learning rate\n",
    "\n",
    "#_ = plt.plot(lr_history[:2*iterations_per_epoch-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.9784\n"
     ]
    }
   ],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.9821\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "outputs": [],
   "source": [
    "with open('best.weights.medium', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 26.3646\n"
     ]
    }
   ],
   "source": [
    "print(f'duration: {round(time.time() - start, 4)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "outputs": [],
   "source": [
    "# big net\n",
    "\n",
    "start = time.time()\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_movies=m,\n",
    "    n_factors=50, hidden=[100, 100, 100],\n",
    "    embedding_dropout=0.05, dropouts=[0.3, 0.3, 0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "#iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "#scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 1.2858 - val: 1.0368\n",
      "loss improvement on epoch: 2\n",
      "[002/300] train: 1.1515 - val: 0.9362\n",
      "loss improvement on epoch: 3\n",
      "[003/300] train: 1.0110 - val: 0.8395\n",
      "loss improvement on epoch: 4\n",
      "[004/300] train: 0.9241 - val: 0.8053\n",
      "loss improvement on epoch: 5\n",
      "[005/300] train: 0.8823 - val: 0.7814\n",
      "loss improvement on epoch: 6\n",
      "[006/300] train: 0.8457 - val: 0.7613\n",
      "loss improvement on epoch: 7\n",
      "[007/300] train: 0.8241 - val: 0.7492\n",
      "loss improvement on epoch: 8\n",
      "[008/300] train: 0.8115 - val: 0.7457\n",
      "loss improvement on epoch: 9\n",
      "[009/300] train: 0.7978 - val: 0.7425\n",
      "loss improvement on epoch: 10\n",
      "[010/300] train: 0.7839 - val: 0.7396\n",
      "loss improvement on epoch: 11\n",
      "[011/300] train: 0.7790 - val: 0.7378\n",
      "[012/300] train: 0.7692 - val: 0.7382\n",
      "loss improvement on epoch: 13\n",
      "[013/300] train: 0.7621 - val: 0.7362\n",
      "[014/300] train: 0.7504 - val: 0.7437\n",
      "[015/300] train: 0.7452 - val: 0.7452\n",
      "[016/300] train: 0.7352 - val: 0.7473\n",
      "[017/300] train: 0.7231 - val: 0.7491\n",
      "[018/300] train: 0.7117 - val: 0.7516\n",
      "[019/300] train: 0.6967 - val: 0.7579\n",
      "[020/300] train: 0.6838 - val: 0.7769\n",
      "[021/300] train: 0.6691 - val: 0.7771\n",
      "[022/300] train: 0.6499 - val: 0.7873\n",
      "[023/300] train: 0.6367 - val: 0.7929\n",
      "[024/300] train: 0.6204 - val: 0.8065\n",
      "[025/300] train: 0.6044 - val: 0.8166\n",
      "[026/300] train: 0.5903 - val: 0.8186\n",
      "[027/300] train: 0.5786 - val: 0.8252\n",
      "[028/300] train: 0.5702 - val: 0.8345\n",
      "[029/300] train: 0.5591 - val: 0.8406\n",
      "[030/300] train: 0.5506 - val: 0.8312\n",
      "[031/300] train: 0.5379 - val: 0.8498\n",
      "[032/300] train: 0.5288 - val: 0.8423\n",
      "[033/300] train: 0.5160 - val: 0.8509\n",
      "[034/300] train: 0.5126 - val: 0.8545\n",
      "[035/300] train: 0.5037 - val: 0.8513\n",
      "[036/300] train: 0.4968 - val: 0.8652\n",
      "[037/300] train: 0.4911 - val: 0.8564\n",
      "[038/300] train: 0.4858 - val: 0.8593\n",
      "[039/300] train: 0.4789 - val: 0.8604\n",
      "[040/300] train: 0.4727 - val: 0.8653\n",
      "[041/300] train: 0.4651 - val: 0.8797\n",
      "[042/300] train: 0.4610 - val: 0.8706\n",
      "[043/300] train: 0.4564 - val: 0.8775\n",
      "[044/300] train: 0.4550 - val: 0.8796\n",
      "[045/300] train: 0.4516 - val: 0.8763\n",
      "[046/300] train: 0.4444 - val: 0.8846\n",
      "[047/300] train: 0.4402 - val: 0.8799\n",
      "[048/300] train: 0.4408 - val: 0.8701\n",
      "[049/300] train: 0.4392 - val: 0.8843\n",
      "[050/300] train: 0.4297 - val: 0.8862\n",
      "[051/300] train: 0.4251 - val: 0.8887\n",
      "[052/300] train: 0.4231 - val: 0.8966\n",
      "[053/300] train: 0.4229 - val: 0.8761\n",
      "[054/300] train: 0.4181 - val: 0.8875\n",
      "[055/300] train: 0.4144 - val: 0.8772\n",
      "[056/300] train: 0.4127 - val: 0.8935\n",
      "[057/300] train: 0.4095 - val: 0.8914\n",
      "[058/300] train: 0.4037 - val: 0.8973\n",
      "[059/300] train: 0.4037 - val: 0.8985\n",
      "[060/300] train: 0.4014 - val: 0.8950\n",
      "[061/300] train: 0.4008 - val: 0.9032\n",
      "[062/300] train: 0.3994 - val: 0.8948\n",
      "[063/300] train: 0.3932 - val: 0.8971\n",
      "[064/300] train: 0.3935 - val: 0.9128\n",
      "[065/300] train: 0.3923 - val: 0.8937\n",
      "[066/300] train: 0.3898 - val: 0.9088\n",
      "[067/300] train: 0.3861 - val: 0.9033\n",
      "[068/300] train: 0.3844 - val: 0.8978\n",
      "[069/300] train: 0.3819 - val: 0.8974\n",
      "[070/300] train: 0.3827 - val: 0.8952\n",
      "[071/300] train: 0.3815 - val: 0.9047\n",
      "[072/300] train: 0.3800 - val: 0.9017\n",
      "[073/300] train: 0.3763 - val: 0.8991\n",
      "[074/300] train: 0.3757 - val: 0.9079\n",
      "[075/300] train: 0.3754 - val: 0.9060\n",
      "[076/300] train: 0.3738 - val: 0.9134\n",
      "[077/300] train: 0.3718 - val: 0.9038\n",
      "[078/300] train: 0.3699 - val: 0.9129\n",
      "[079/300] train: 0.3692 - val: 0.9007\n",
      "[080/300] train: 0.3699 - val: 0.9155\n",
      "[081/300] train: 0.3656 - val: 0.8996\n",
      "[082/300] train: 0.3628 - val: 0.9078\n",
      "[083/300] train: 0.3643 - val: 0.9141\n",
      "[084/300] train: 0.3600 - val: 0.9053\n",
      "[085/300] train: 0.3623 - val: 0.9107\n",
      "[086/300] train: 0.3597 - val: 0.9181\n",
      "[087/300] train: 0.3607 - val: 0.9028\n",
      "[088/300] train: 0.3601 - val: 0.9085\n",
      "[089/300] train: 0.3559 - val: 0.9076\n",
      "[090/300] train: 0.3556 - val: 0.9165\n",
      "[091/300] train: 0.3531 - val: 0.9082\n",
      "[092/300] train: 0.3526 - val: 0.9229\n",
      "[093/300] train: 0.3534 - val: 0.8999\n",
      "[094/300] train: 0.3517 - val: 0.9195\n",
      "[095/300] train: 0.3528 - val: 0.9034\n",
      "[096/300] train: 0.3479 - val: 0.9187\n",
      "[097/300] train: 0.3480 - val: 0.9112\n",
      "[098/300] train: 0.3463 - val: 0.9145\n",
      "[099/300] train: 0.3434 - val: 0.9214\n",
      "[100/300] train: 0.3422 - val: 0.9226\n",
      "[101/300] train: 0.3449 - val: 0.9208\n",
      "[102/300] train: 0.3464 - val: 0.9044\n",
      "[103/300] train: 0.3409 - val: 0.9053\n",
      "[104/300] train: 0.3389 - val: 0.9178\n",
      "[105/300] train: 0.3421 - val: 0.9119\n",
      "[106/300] train: 0.3409 - val: 0.9176\n",
      "[107/300] train: 0.3381 - val: 0.9139\n",
      "[108/300] train: 0.3378 - val: 0.9091\n",
      "[109/300] train: 0.3415 - val: 0.9146\n",
      "[110/300] train: 0.3382 - val: 0.9069\n",
      "[111/300] train: 0.3364 - val: 0.9125\n",
      "[112/300] train: 0.3373 - val: 0.9160\n",
      "[113/300] train: 0.3375 - val: 0.9086\n",
      "[114/300] train: 0.3346 - val: 0.9144\n",
      "[115/300] train: 0.3330 - val: 0.9175\n",
      "[116/300] train: 0.3284 - val: 0.8972\n",
      "[117/300] train: 0.3312 - val: 0.9200\n",
      "[118/300] train: 0.3300 - val: 0.9097\n",
      "[119/300] train: 0.3334 - val: 0.9095\n",
      "[120/300] train: 0.3319 - val: 0.9104\n",
      "[121/300] train: 0.3300 - val: 0.9137\n",
      "[122/300] train: 0.3285 - val: 0.9251\n",
      "[123/300] train: 0.3275 - val: 0.9019\n",
      "[124/300] train: 0.3244 - val: 0.9044\n",
      "[125/300] train: 0.3276 - val: 0.9095\n",
      "[126/300] train: 0.3268 - val: 0.9099\n",
      "[127/300] train: 0.3236 - val: 0.9198\n",
      "[128/300] train: 0.3237 - val: 0.9231\n",
      "[129/300] train: 0.3236 - val: 0.9136\n",
      "[130/300] train: 0.3228 - val: 0.9160\n",
      "[131/300] train: 0.3260 - val: 0.9116\n",
      "[132/300] train: 0.3225 - val: 0.9241\n",
      "[133/300] train: 0.3229 - val: 0.9057\n",
      "[134/300] train: 0.3221 - val: 0.9193\n",
      "[135/300] train: 0.3197 - val: 0.9206\n",
      "[136/300] train: 0.3206 - val: 0.9118\n",
      "[137/300] train: 0.3223 - val: 0.9142\n",
      "[138/300] train: 0.3216 - val: 0.9123\n",
      "[139/300] train: 0.3202 - val: 0.9155\n",
      "[140/300] train: 0.3208 - val: 0.9095\n",
      "[141/300] train: 0.3202 - val: 0.9231\n",
      "[142/300] train: 0.3162 - val: 0.9140\n",
      "[143/300] train: 0.3177 - val: 0.9279\n",
      "[144/300] train: 0.3187 - val: 0.9130\n",
      "[145/300] train: 0.3166 - val: 0.9165\n",
      "[146/300] train: 0.3177 - val: 0.9219\n",
      "[147/300] train: 0.3156 - val: 0.9211\n",
      "[148/300] train: 0.3143 - val: 0.9228\n",
      "[149/300] train: 0.3131 - val: 0.9221\n",
      "[150/300] train: 0.3143 - val: 0.9143\n",
      "[151/300] train: 0.3157 - val: 0.9076\n",
      "[152/300] train: 0.3141 - val: 0.9281\n",
      "[153/300] train: 0.3121 - val: 0.9227\n",
      "[154/300] train: 0.3135 - val: 0.9162\n",
      "[155/300] train: 0.3127 - val: 0.9161\n",
      "[156/300] train: 0.3131 - val: 0.9096\n",
      "[157/300] train: 0.3109 - val: 0.9100\n",
      "[158/300] train: 0.3086 - val: 0.9092\n",
      "[159/300] train: 0.3093 - val: 0.9089\n",
      "[160/300] train: 0.3127 - val: 0.9100\n",
      "[161/300] train: 0.3103 - val: 0.9125\n",
      "[162/300] train: 0.3116 - val: 0.9237\n",
      "[163/300] train: 0.3112 - val: 0.9051\n",
      "[164/300] train: 0.3088 - val: 0.9224\n",
      "[165/300] train: 0.3123 - val: 0.9172\n",
      "[166/300] train: 0.3083 - val: 0.9167\n",
      "[167/300] train: 0.3071 - val: 0.9196\n",
      "[168/300] train: 0.3072 - val: 0.9213\n",
      "[169/300] train: 0.3080 - val: 0.9183\n",
      "[170/300] train: 0.3058 - val: 0.9131\n",
      "[171/300] train: 0.3053 - val: 0.9008\n",
      "[172/300] train: 0.3074 - val: 0.9278\n",
      "[173/300] train: 0.3049 - val: 0.9005\n",
      "[174/300] train: 0.3032 - val: 0.9130\n",
      "[175/300] train: 0.3047 - val: 0.9062\n",
      "[176/300] train: 0.3066 - val: 0.9134\n",
      "[177/300] train: 0.3065 - val: 0.9033\n",
      "[178/300] train: 0.3039 - val: 0.9173\n",
      "[179/300] train: 0.3011 - val: 0.9143\n",
      "[180/300] train: 0.3052 - val: 0.9107\n",
      "[181/300] train: 0.3056 - val: 0.9188\n",
      "[182/300] train: 0.3030 - val: 0.9118\n",
      "[183/300] train: 0.3008 - val: 0.9154\n",
      "[184/300] train: 0.3030 - val: 0.9239\n",
      "[185/300] train: 0.3003 - val: 0.9134\n",
      "[186/300] train: 0.3003 - val: 0.9195\n",
      "[187/300] train: 0.2961 - val: 0.9257\n",
      "[188/300] train: 0.3023 - val: 0.9144\n",
      "[189/300] train: 0.3028 - val: 0.9159\n",
      "[190/300] train: 0.2974 - val: 0.9164\n",
      "[191/300] train: 0.2992 - val: 0.9090\n",
      "[192/300] train: 0.2993 - val: 0.9045\n",
      "[193/300] train: 0.3008 - val: 0.9074\n",
      "[194/300] train: 0.2997 - val: 0.9184\n",
      "[195/300] train: 0.2997 - val: 0.9203\n",
      "[196/300] train: 0.2974 - val: 0.9039\n",
      "[197/300] train: 0.2976 - val: 0.9205\n",
      "[198/300] train: 0.3008 - val: 0.9191\n",
      "[199/300] train: 0.3009 - val: 0.9152\n",
      "[200/300] train: 0.3010 - val: 0.9102\n",
      "[201/300] train: 0.2973 - val: 0.9093\n",
      "[202/300] train: 0.2951 - val: 0.9057\n",
      "[203/300] train: 0.2984 - val: 0.9055\n",
      "[204/300] train: 0.2993 - val: 0.9147\n",
      "[205/300] train: 0.2962 - val: 0.9118\n",
      "[206/300] train: 0.2944 - val: 0.9298\n",
      "[207/300] train: 0.2974 - val: 0.9210\n",
      "[208/300] train: 0.2956 - val: 0.9144\n",
      "[209/300] train: 0.2961 - val: 0.9192\n",
      "[210/300] train: 0.2963 - val: 0.9117\n",
      "[211/300] train: 0.2982 - val: 0.9327\n",
      "[212/300] train: 0.2929 - val: 0.9241\n",
      "[213/300] train: 0.2918 - val: 0.9130\n",
      "[214/300] train: 0.2906 - val: 0.9173\n",
      "[215/300] train: 0.2937 - val: 0.9251\n",
      "[216/300] train: 0.2912 - val: 0.9272\n",
      "[217/300] train: 0.2943 - val: 0.9163\n",
      "[218/300] train: 0.2945 - val: 0.9201\n",
      "[219/300] train: 0.2957 - val: 0.9096\n",
      "[220/300] train: 0.2957 - val: 0.9084\n",
      "[221/300] train: 0.2944 - val: 0.9138\n",
      "[222/300] train: 0.2924 - val: 0.9136\n",
      "[223/300] train: 0.2938 - val: 0.9151\n",
      "[224/300] train: 0.2908 - val: 0.9071\n",
      "[225/300] train: 0.2917 - val: 0.9201\n",
      "[226/300] train: 0.2914 - val: 0.9179\n",
      "[227/300] train: 0.2890 - val: 0.9258\n",
      "[228/300] train: 0.2904 - val: 0.9146\n",
      "[229/300] train: 0.2901 - val: 0.9183\n",
      "[230/300] train: 0.2895 - val: 0.9196\n",
      "[231/300] train: 0.2905 - val: 0.9161\n",
      "[232/300] train: 0.2904 - val: 0.9143\n",
      "[233/300] train: 0.2933 - val: 0.9259\n",
      "[234/300] train: 0.2867 - val: 0.9235\n",
      "[235/300] train: 0.2923 - val: 0.9256\n",
      "[236/300] train: 0.2893 - val: 0.9138\n",
      "[237/300] train: 0.2928 - val: 0.9205\n",
      "[238/300] train: 0.2879 - val: 0.9188\n",
      "[239/300] train: 0.2903 - val: 0.9279\n",
      "[240/300] train: 0.2886 - val: 0.9167\n",
      "[241/300] train: 0.2884 - val: 0.9208\n",
      "[242/300] train: 0.2896 - val: 0.9142\n",
      "[243/300] train: 0.2904 - val: 0.9115\n",
      "[244/300] train: 0.2876 - val: 0.9069\n",
      "[245/300] train: 0.2908 - val: 0.9066\n",
      "[246/300] train: 0.2887 - val: 0.9153\n",
      "[247/300] train: 0.2840 - val: 0.9138\n",
      "[248/300] train: 0.2885 - val: 0.9216\n",
      "[249/300] train: 0.2824 - val: 0.9164\n",
      "[250/300] train: 0.2845 - val: 0.9049\n",
      "[251/300] train: 0.2851 - val: 0.9225\n",
      "[252/300] train: 0.2836 - val: 0.9150\n",
      "[253/300] train: 0.2855 - val: 0.9142\n",
      "[254/300] train: 0.2859 - val: 0.9341\n",
      "[255/300] train: 0.2852 - val: 0.9271\n",
      "[256/300] train: 0.2840 - val: 0.9250\n",
      "[257/300] train: 0.2861 - val: 0.9213\n",
      "[258/300] train: 0.2823 - val: 0.9200\n",
      "[259/300] train: 0.2858 - val: 0.9142\n",
      "[260/300] train: 0.2839 - val: 0.9266\n",
      "[261/300] train: 0.2835 - val: 0.9140\n",
      "[262/300] train: 0.2832 - val: 0.9103\n",
      "[263/300] train: 0.2845 - val: 0.9111\n",
      "[264/300] train: 0.2826 - val: 0.9132\n",
      "[265/300] train: 0.2822 - val: 0.9149\n",
      "[266/300] train: 0.2838 - val: 0.9208\n",
      "[267/300] train: 0.2825 - val: 0.9221\n",
      "[268/300] train: 0.2839 - val: 0.9203\n",
      "[269/300] train: 0.2791 - val: 0.9220\n",
      "[270/300] train: 0.2809 - val: 0.9182\n",
      "[271/300] train: 0.2843 - val: 0.9167\n",
      "[272/300] train: 0.2805 - val: 0.9203\n",
      "[273/300] train: 0.2817 - val: 0.9197\n",
      "[274/300] train: 0.2832 - val: 0.9252\n",
      "[275/300] train: 0.2829 - val: 0.9163\n",
      "[276/300] train: 0.2814 - val: 0.9212\n",
      "[277/300] train: 0.2843 - val: 0.9161\n",
      "[278/300] train: 0.2808 - val: 0.9313\n",
      "[279/300] train: 0.2816 - val: 0.9191\n",
      "[280/300] train: 0.2806 - val: 0.9142\n",
      "[281/300] train: 0.2778 - val: 0.9147\n",
      "[282/300] train: 0.2796 - val: 0.9175\n",
      "[283/300] train: 0.2823 - val: 0.9192\n",
      "[284/300] train: 0.2807 - val: 0.9191\n",
      "[285/300] train: 0.2811 - val: 0.9118\n",
      "[286/300] train: 0.2800 - val: 0.9181\n",
      "[287/300] train: 0.2790 - val: 0.9051\n",
      "[288/300] train: 0.2775 - val: 0.9240\n",
      "[289/300] train: 0.2784 - val: 0.9158\n",
      "[290/300] train: 0.2798 - val: 0.9117\n",
      "[291/300] train: 0.2809 - val: 0.9024\n",
      "[292/300] train: 0.2795 - val: 0.9264\n",
      "[293/300] train: 0.2776 - val: 0.9198\n",
      "[294/300] train: 0.2791 - val: 0.9131\n",
      "[295/300] train: 0.2787 - val: 0.9120\n",
      "[296/300] train: 0.2797 - val: 0.9118\n",
      "[297/300] train: 0.2788 - val: 0.9253\n",
      "[298/300] train: 0.2768 - val: 0.9194\n",
      "[299/300] train: 0.2798 - val: 0.9119\n",
      "[300/300] train: 0.2774 - val: 0.9020\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    #scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #lr_history.extend(scheduler.get_lr())\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "outputs": [],
   "source": [
    "# Plot decreasing learning rate\n",
    "\n",
    "#_ = plt.plot(lr_history[:2*iterations_per_epoch-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.9487\n"
     ]
    }
   ],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.9523\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "outputs": [],
   "source": [
    "with open('best.weights.big', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 54.6365\n"
     ]
    }
   ],
   "source": [
    "print(f'duration: {round(time.time() - start, 4)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f57757a1",
   "language": "python",
   "display_name": "PyCharm (rs-via-gnn)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}