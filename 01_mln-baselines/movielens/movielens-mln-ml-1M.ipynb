{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MovieLens MLN Recommendation via PyTorch\n",
    "\n",
    "adapted from https://github.com/fastai/fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "#from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "\n",
    "RANDOM_STATE = 2021\n",
    "set_random_seed(RANDOM_STATE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    files = {}\n",
    "    path = Path(path)\n",
    "    for filename in path.glob('*'):\n",
    "        if filename.suffix == '.dat':\n",
    "            if filename.stem == 'ratings':\n",
    "                columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "            elif filename.stem == 'users':\n",
    "                columns = ['userId', 'gender', 'Occupation', 'zip-code']\n",
    "            else:\n",
    "                columns = ['movieId', 'title', 'genres']\n",
    "            data = pd.read_csv(filename, sep='::', names=columns, engine='python')\n",
    "            files[filename.stem] = data\n",
    "    return files['ratings'], files['movies'], files['users']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# pick one of the available folders\n",
    "ratings, movies, users = read_data('/home/weiss/rs_data/ml-1m')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "   userId  movieId  rating  timestamp\n0       1     1193       5  978300760\n1       1      661       3  978302109\n2       1      914       3  978301968\n3       1     3408       4  978300275\n4       1     2355       5  978824291",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1193</td>\n      <td>5</td>\n      <td>978300760</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>661</td>\n      <td>3</td>\n      <td>978302109</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>914</td>\n      <td>3</td>\n      <td>978301968</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3408</td>\n      <td>4</td>\n      <td>978300275</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2355</td>\n      <td>5</td>\n      <td>978824291</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "   movieId                               title                        genres\n0        1                    Toy Story (1995)   Animation|Children's|Comedy\n1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n2        3             Grumpier Old Men (1995)                Comedy|Romance\n3        4            Waiting to Exhale (1995)                  Comedy|Drama\n4        5  Father of the Bride Part II (1995)                        Comedy",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movieId</th>\n      <th>title</th>\n      <th>genres</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Toy Story (1995)</td>\n      <td>Animation|Children's|Comedy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Jumanji (1995)</td>\n      <td>Adventure|Children's|Fantasy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Grumpier Old Men (1995)</td>\n      <td>Comedy|Romance</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Waiting to Exhale (1995)</td>\n      <td>Comedy|Drama</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Father of the Bride Part II (1995)</td>\n      <td>Comedy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 6040 users, 3706 movies\n",
      "Dataset shape: (1000209, 2)\n",
      "Target shape: (1000209,)\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(ratings, top=None):\n",
    "    if top is not None:\n",
    "        ratings.groupby('userId')['rating'].count()\n",
    "\n",
    "    unique_users = ratings.userId.unique()\n",
    "    user_to_index = {old: new for new, old in enumerate(unique_users)}\n",
    "    new_users = ratings.userId.map(user_to_index)\n",
    "\n",
    "    unique_movies = ratings.movieId.unique()\n",
    "    movie_to_index = {old: new for new, old in enumerate(unique_movies)}\n",
    "    new_movies = ratings.movieId.map(movie_to_index)\n",
    "\n",
    "    n_users = unique_users.shape[0]\n",
    "    n_movies = unique_movies.shape[0]\n",
    "\n",
    "    X = pd.DataFrame({'user_id': new_users, 'movie_id': new_movies})\n",
    "    y = ratings['rating'].astype(np.float32)\n",
    "    return (n_users, n_movies), (X, y), (user_to_index, movie_to_index)\n",
    "\n",
    "(n, m), (X, y), _ = create_dataset(ratings)\n",
    "print(f'Embeddings: {n} users, {m} movies')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=RANDOM_STATE)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, random_state=RANDOM_STATE)\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid), 'test': (X_test, y_test)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_valid), 'test': len(X_test)}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "class RatingsIterator:\n",
    "\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        if shuffle:\n",
    "            index = np.random.permutation(X.shape[0])\n",
    "            X, y = X[index], y[index]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n",
    "        self._current = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        if self._current >= self.n_batches:\n",
    "            raise StopIteration()\n",
    "        k = self._current\n",
    "        self._current += 1\n",
    "        bs = self.batch_size\n",
    "        return self.X[k*bs:(k + 1)*bs], self.y[k*bs:(k + 1)*bs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def batches(X, y, bs=32, shuffle=True):\n",
    "    for xb, yb in RatingsIterator(X, y, bs, shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        yield xb, yb.view(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "class RecommenderNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates dense MLN with embedding layers.\n",
    "\n",
    "    Args:\n",
    "        n_users:\n",
    "            Number of unique users in the dataset.\n",
    "\n",
    "        n_movies:\n",
    "            Number of unique movies in the dataset.\n",
    "\n",
    "        n_factors:\n",
    "            Number of columns in the embeddings matrix.\n",
    "\n",
    "        embedding_dropout:\n",
    "            Dropout rate to apply right after embeddings layer.\n",
    "\n",
    "        hidden:\n",
    "            A single integer or a list of integers defining the number of\n",
    "            units in hidden layer(s).\n",
    "\n",
    "        dropouts:\n",
    "            A single integer or a list of integers defining the dropout\n",
    "            layers rates applied right after each of hidden layers.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_users, n_movies,\n",
    "                 n_factors=50, embedding_dropout=0.02,\n",
    "                 hidden=10, dropouts=0.2):\n",
    "\n",
    "        super().__init__()\n",
    "        hidden = get_list(hidden)\n",
    "        dropouts = get_list(dropouts)\n",
    "        n_last = hidden[-1]\n",
    "\n",
    "        def gen_layers(n_in):\n",
    "            \"\"\"\n",
    "            A generator that yields a sequence of hidden layers and\n",
    "            their activations/dropouts.\n",
    "\n",
    "            Note that the function captures `hidden` and `dropouts`\n",
    "            values from the outer scope.\n",
    "            \"\"\"\n",
    "            nonlocal hidden, dropouts\n",
    "            assert len(dropouts) <= len(hidden)\n",
    "\n",
    "            for n_out, rate in zip_longest(hidden, dropouts):\n",
    "                yield nn.Linear(n_in, n_out)\n",
    "                yield nn.ReLU()\n",
    "                if rate is not None and rate > 0.:\n",
    "                    yield nn.Dropout(rate)\n",
    "                n_in = n_out\n",
    "\n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        self.m = nn.Embedding(n_movies, n_factors)\n",
    "        self.drop = nn.Dropout(embedding_dropout)\n",
    "        self.hidden = nn.Sequential(*list(gen_layers(n_factors * 2)))\n",
    "        self.fc = nn.Linear(n_last, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self._init()\n",
    "\n",
    "    def forward(self, users, movies, minmax=None):\n",
    "        features = torch.cat([self.u(users), self.m(movies)], dim=1)\n",
    "        x = self.drop(features)\n",
    "        x = self.hidden(x)\n",
    "        out = torch.sigmoid(self.fc(x))\n",
    "        #out = self.relu(self.fc(x))  # relu delivers worse rsme\n",
    "        if minmax is not None:\n",
    "            min_rating, max_rating = minmax\n",
    "            out = out*(max_rating - min_rating + 1) + min_rating - 0.5\n",
    "        return out\n",
    "\n",
    "    def _init(self):\n",
    "        \"\"\"\n",
    "        Setup embeddings and hidden layers with reasonable initial values.\n",
    "        \"\"\"\n",
    "\n",
    "        def init(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        self.u.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.m.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.hidden.apply(init)\n",
    "        init(self.fc)\n",
    "\n",
    "\n",
    "def get_list(n):\n",
    "    if isinstance(n, (int, float)):\n",
    "        return [n]\n",
    "    elif hasattr(n, '__iter__'):\n",
    "        return list(n)\n",
    "    raise TypeError('layers configuraiton should be a single number or a list of numbers')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "\n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + math.cos(math.pi*t/t_max))/2\n",
    "    return scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def plot_lr(schedule):\n",
    "    ts = list(range(1000))\n",
    "    y = [schedule(t, 0.001) for t in ts]\n",
    "    plt.plot(ts, y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "(1.0, 5.0)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax = float(ratings.rating.min()), float(ratings.rating.max())\n",
    "minmax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# small net\n",
    "\n",
    "start = time.time()\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_movies=m,\n",
    "    n_factors=10, hidden=[10],\n",
    "    embedding_dropout=0.05, dropouts=[0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "#iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "#scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 1.1985 - val: 0.9439\n",
      "loss improvement on epoch: 2\n",
      "[002/300] train: 0.9015 - val: 0.8675\n",
      "loss improvement on epoch: 3\n",
      "[003/300] train: 0.8620 - val: 0.8532\n",
      "loss improvement on epoch: 4\n",
      "[004/300] train: 0.8468 - val: 0.8419\n",
      "loss improvement on epoch: 5\n",
      "[005/300] train: 0.8362 - val: 0.8386\n",
      "loss improvement on epoch: 6\n",
      "[006/300] train: 0.8287 - val: 0.8343\n",
      "loss improvement on epoch: 7\n",
      "[007/300] train: 0.8236 - val: 0.8337\n",
      "loss improvement on epoch: 8\n",
      "[008/300] train: 0.8198 - val: 0.8315\n",
      "loss improvement on epoch: 9\n",
      "[009/300] train: 0.8164 - val: 0.8287\n",
      "loss improvement on epoch: 10\n",
      "[010/300] train: 0.8138 - val: 0.8280\n",
      "loss improvement on epoch: 11\n",
      "[011/300] train: 0.8120 - val: 0.8268\n",
      "loss improvement on epoch: 12\n",
      "[012/300] train: 0.8097 - val: 0.8267\n",
      "loss improvement on epoch: 13\n",
      "[013/300] train: 0.8089 - val: 0.8257\n",
      "[014/300] train: 0.8078 - val: 0.8259\n",
      "loss improvement on epoch: 15\n",
      "[015/300] train: 0.8072 - val: 0.8244\n",
      "[016/300] train: 0.8058 - val: 0.8251\n",
      "[017/300] train: 0.8046 - val: 0.8259\n",
      "loss improvement on epoch: 18\n",
      "[018/300] train: 0.8038 - val: 0.8240\n",
      "loss improvement on epoch: 19\n",
      "[019/300] train: 0.8020 - val: 0.8235\n",
      "[020/300] train: 0.8012 - val: 0.8236\n",
      "[021/300] train: 0.7997 - val: 0.8254\n",
      "[022/300] train: 0.7987 - val: 0.8253\n",
      "[023/300] train: 0.7990 - val: 0.8240\n",
      "loss improvement on epoch: 24\n",
      "[024/300] train: 0.7970 - val: 0.8224\n",
      "[025/300] train: 0.7966 - val: 0.8231\n",
      "[026/300] train: 0.7952 - val: 0.8239\n",
      "[027/300] train: 0.7936 - val: 0.8226\n",
      "[028/300] train: 0.7939 - val: 0.8235\n",
      "loss improvement on epoch: 29\n",
      "[029/300] train: 0.7917 - val: 0.8207\n",
      "[030/300] train: 0.7912 - val: 0.8235\n",
      "[031/300] train: 0.7913 - val: 0.8231\n",
      "[032/300] train: 0.7892 - val: 0.8245\n",
      "[033/300] train: 0.7892 - val: 0.8231\n",
      "[034/300] train: 0.7875 - val: 0.8228\n",
      "[035/300] train: 0.7877 - val: 0.8214\n",
      "[036/300] train: 0.7866 - val: 0.8224\n",
      "[037/300] train: 0.7854 - val: 0.8217\n",
      "[038/300] train: 0.7852 - val: 0.8219\n",
      "[039/300] train: 0.7847 - val: 0.8236\n",
      "[040/300] train: 0.7850 - val: 0.8213\n",
      "[041/300] train: 0.7840 - val: 0.8215\n",
      "[042/300] train: 0.7838 - val: 0.8219\n",
      "[043/300] train: 0.7832 - val: 0.8209\n",
      "[044/300] train: 0.7829 - val: 0.8219\n",
      "[045/300] train: 0.7825 - val: 0.8215\n",
      "[046/300] train: 0.7824 - val: 0.8208\n",
      "[047/300] train: 0.7811 - val: 0.8218\n",
      "[048/300] train: 0.7816 - val: 0.8218\n",
      "[049/300] train: 0.7809 - val: 0.8215\n",
      "loss improvement on epoch: 50\n",
      "[050/300] train: 0.7795 - val: 0.8206\n",
      "[051/300] train: 0.7781 - val: 0.8231\n",
      "loss improvement on epoch: 52\n",
      "[052/300] train: 0.7794 - val: 0.8204\n",
      "[053/300] train: 0.7791 - val: 0.8215\n",
      "[054/300] train: 0.7778 - val: 0.8230\n",
      "[055/300] train: 0.7773 - val: 0.8234\n",
      "[056/300] train: 0.7777 - val: 0.8231\n",
      "loss improvement on epoch: 57\n",
      "[057/300] train: 0.7764 - val: 0.8201\n",
      "[058/300] train: 0.7761 - val: 0.8226\n",
      "[059/300] train: 0.7758 - val: 0.8220\n",
      "[060/300] train: 0.7755 - val: 0.8215\n",
      "[061/300] train: 0.7753 - val: 0.8215\n",
      "[062/300] train: 0.7759 - val: 0.8240\n",
      "[063/300] train: 0.7741 - val: 0.8222\n",
      "[064/300] train: 0.7745 - val: 0.8222\n",
      "[065/300] train: 0.7742 - val: 0.8236\n",
      "[066/300] train: 0.7738 - val: 0.8211\n",
      "[067/300] train: 0.7735 - val: 0.8224\n",
      "[068/300] train: 0.7736 - val: 0.8254\n",
      "[069/300] train: 0.7744 - val: 0.8225\n",
      "[070/300] train: 0.7734 - val: 0.8234\n",
      "[071/300] train: 0.7733 - val: 0.8236\n",
      "[072/300] train: 0.7729 - val: 0.8244\n",
      "[073/300] train: 0.7720 - val: 0.8247\n",
      "[074/300] train: 0.7718 - val: 0.8241\n",
      "[075/300] train: 0.7720 - val: 0.8245\n",
      "[076/300] train: 0.7720 - val: 0.8232\n",
      "[077/300] train: 0.7716 - val: 0.8249\n",
      "[078/300] train: 0.7713 - val: 0.8253\n",
      "[079/300] train: 0.7712 - val: 0.8245\n",
      "[080/300] train: 0.7707 - val: 0.8234\n",
      "[081/300] train: 0.7711 - val: 0.8240\n",
      "[082/300] train: 0.7706 - val: 0.8245\n",
      "[083/300] train: 0.7714 - val: 0.8255\n",
      "[084/300] train: 0.7704 - val: 0.8239\n",
      "[085/300] train: 0.7704 - val: 0.8245\n",
      "[086/300] train: 0.7713 - val: 0.8243\n",
      "[087/300] train: 0.7695 - val: 0.8250\n",
      "[088/300] train: 0.7692 - val: 0.8264\n",
      "[089/300] train: 0.7699 - val: 0.8235\n",
      "[090/300] train: 0.7691 - val: 0.8264\n",
      "[091/300] train: 0.7689 - val: 0.8261\n",
      "[092/300] train: 0.7704 - val: 0.8258\n",
      "[093/300] train: 0.7697 - val: 0.8255\n",
      "[094/300] train: 0.7689 - val: 0.8267\n",
      "[095/300] train: 0.7683 - val: 0.8265\n",
      "[096/300] train: 0.7693 - val: 0.8261\n",
      "[097/300] train: 0.7700 - val: 0.8265\n",
      "[098/300] train: 0.7684 - val: 0.8265\n",
      "[099/300] train: 0.7685 - val: 0.8261\n",
      "[100/300] train: 0.7691 - val: 0.8254\n",
      "[101/300] train: 0.7682 - val: 0.8265\n",
      "[102/300] train: 0.7684 - val: 0.8281\n",
      "[103/300] train: 0.7686 - val: 0.8262\n",
      "[104/300] train: 0.7673 - val: 0.8254\n",
      "[105/300] train: 0.7685 - val: 0.8251\n",
      "[106/300] train: 0.7681 - val: 0.8272\n",
      "[107/300] train: 0.7677 - val: 0.8255\n",
      "[108/300] train: 0.7670 - val: 0.8264\n",
      "[109/300] train: 0.7684 - val: 0.8265\n",
      "[110/300] train: 0.7678 - val: 0.8259\n",
      "[111/300] train: 0.7675 - val: 0.8274\n",
      "[112/300] train: 0.7672 - val: 0.8270\n",
      "[113/300] train: 0.7672 - val: 0.8273\n",
      "[114/300] train: 0.7669 - val: 0.8254\n",
      "[115/300] train: 0.7664 - val: 0.8263\n",
      "[116/300] train: 0.7679 - val: 0.8262\n",
      "[117/300] train: 0.7676 - val: 0.8279\n",
      "[118/300] train: 0.7671 - val: 0.8270\n",
      "[119/300] train: 0.7676 - val: 0.8268\n",
      "[120/300] train: 0.7661 - val: 0.8271\n",
      "[121/300] train: 0.7673 - val: 0.8271\n",
      "[122/300] train: 0.7666 - val: 0.8277\n",
      "[123/300] train: 0.7668 - val: 0.8262\n",
      "[124/300] train: 0.7669 - val: 0.8288\n",
      "[125/300] train: 0.7660 - val: 0.8268\n",
      "[126/300] train: 0.7665 - val: 0.8273\n",
      "[127/300] train: 0.7659 - val: 0.8283\n",
      "[128/300] train: 0.7656 - val: 0.8265\n",
      "[129/300] train: 0.7657 - val: 0.8258\n",
      "[130/300] train: 0.7662 - val: 0.8286\n",
      "[131/300] train: 0.7656 - val: 0.8262\n",
      "[132/300] train: 0.7660 - val: 0.8288\n",
      "[133/300] train: 0.7664 - val: 0.8277\n",
      "[134/300] train: 0.7653 - val: 0.8267\n",
      "[135/300] train: 0.7658 - val: 0.8250\n",
      "[136/300] train: 0.7651 - val: 0.8278\n",
      "[137/300] train: 0.7659 - val: 0.8275\n",
      "[138/300] train: 0.7660 - val: 0.8285\n",
      "[139/300] train: 0.7655 - val: 0.8270\n",
      "[140/300] train: 0.7654 - val: 0.8274\n",
      "[141/300] train: 0.7648 - val: 0.8277\n",
      "[142/300] train: 0.7654 - val: 0.8274\n",
      "[143/300] train: 0.7652 - val: 0.8290\n",
      "[144/300] train: 0.7651 - val: 0.8262\n",
      "[145/300] train: 0.7652 - val: 0.8251\n",
      "[146/300] train: 0.7653 - val: 0.8270\n",
      "[147/300] train: 0.7646 - val: 0.8280\n",
      "[148/300] train: 0.7655 - val: 0.8271\n",
      "[149/300] train: 0.7653 - val: 0.8275\n",
      "[150/300] train: 0.7641 - val: 0.8280\n",
      "[151/300] train: 0.7648 - val: 0.8275\n",
      "[152/300] train: 0.7648 - val: 0.8264\n",
      "[153/300] train: 0.7644 - val: 0.8272\n",
      "[154/300] train: 0.7651 - val: 0.8283\n",
      "[155/300] train: 0.7651 - val: 0.8285\n",
      "[156/300] train: 0.7636 - val: 0.8286\n",
      "[157/300] train: 0.7642 - val: 0.8278\n",
      "[158/300] train: 0.7647 - val: 0.8270\n",
      "[159/300] train: 0.7646 - val: 0.8284\n",
      "[160/300] train: 0.7648 - val: 0.8279\n",
      "[161/300] train: 0.7641 - val: 0.8276\n",
      "[162/300] train: 0.7633 - val: 0.8297\n",
      "[163/300] train: 0.7643 - val: 0.8296\n",
      "[164/300] train: 0.7650 - val: 0.8272\n",
      "[165/300] train: 0.7638 - val: 0.8286\n",
      "[166/300] train: 0.7643 - val: 0.8299\n",
      "[167/300] train: 0.7647 - val: 0.8296\n",
      "[168/300] train: 0.7640 - val: 0.8275\n",
      "[169/300] train: 0.7650 - val: 0.8279\n",
      "[170/300] train: 0.7645 - val: 0.8277\n",
      "[171/300] train: 0.7634 - val: 0.8310\n",
      "[172/300] train: 0.7646 - val: 0.8280\n",
      "[173/300] train: 0.7641 - val: 0.8277\n",
      "[174/300] train: 0.7644 - val: 0.8266\n",
      "[175/300] train: 0.7632 - val: 0.8280\n",
      "[176/300] train: 0.7639 - val: 0.8256\n",
      "[177/300] train: 0.7636 - val: 0.8299\n",
      "[178/300] train: 0.7635 - val: 0.8290\n",
      "[179/300] train: 0.7646 - val: 0.8271\n",
      "[180/300] train: 0.7636 - val: 0.8296\n",
      "[181/300] train: 0.7634 - val: 0.8304\n",
      "[182/300] train: 0.7644 - val: 0.8293\n",
      "[183/300] train: 0.7632 - val: 0.8283\n",
      "[184/300] train: 0.7631 - val: 0.8281\n",
      "[185/300] train: 0.7627 - val: 0.8302\n",
      "[186/300] train: 0.7631 - val: 0.8281\n",
      "[187/300] train: 0.7633 - val: 0.8271\n",
      "[188/300] train: 0.7635 - val: 0.8281\n",
      "[189/300] train: 0.7634 - val: 0.8271\n",
      "[190/300] train: 0.7627 - val: 0.8289\n",
      "[191/300] train: 0.7632 - val: 0.8303\n",
      "[192/300] train: 0.7635 - val: 0.8295\n",
      "[193/300] train: 0.7636 - val: 0.8286\n",
      "[194/300] train: 0.7623 - val: 0.8283\n",
      "[195/300] train: 0.7632 - val: 0.8259\n",
      "[196/300] train: 0.7631 - val: 0.8274\n",
      "[197/300] train: 0.7619 - val: 0.8274\n",
      "[198/300] train: 0.7627 - val: 0.8303\n",
      "[199/300] train: 0.7633 - val: 0.8290\n",
      "[200/300] train: 0.7623 - val: 0.8282\n",
      "[201/300] train: 0.7623 - val: 0.8292\n",
      "[202/300] train: 0.7616 - val: 0.8293\n",
      "[203/300] train: 0.7621 - val: 0.8296\n",
      "[204/300] train: 0.7629 - val: 0.8303\n",
      "[205/300] train: 0.7632 - val: 0.8285\n",
      "[206/300] train: 0.7617 - val: 0.8300\n",
      "[207/300] train: 0.7624 - val: 0.8286\n",
      "[208/300] train: 0.7617 - val: 0.8265\n",
      "[209/300] train: 0.7625 - val: 0.8286\n",
      "[210/300] train: 0.7628 - val: 0.8301\n",
      "[211/300] train: 0.7625 - val: 0.8287\n",
      "[212/300] train: 0.7626 - val: 0.8294\n",
      "[213/300] train: 0.7631 - val: 0.8289\n",
      "[214/300] train: 0.7622 - val: 0.8299\n",
      "[215/300] train: 0.7621 - val: 0.8298\n",
      "[216/300] train: 0.7621 - val: 0.8279\n",
      "[217/300] train: 0.7620 - val: 0.8296\n",
      "[218/300] train: 0.7629 - val: 0.8291\n",
      "[219/300] train: 0.7618 - val: 0.8287\n",
      "[220/300] train: 0.7622 - val: 0.8292\n",
      "[221/300] train: 0.7619 - val: 0.8263\n",
      "[222/300] train: 0.7623 - val: 0.8277\n",
      "[223/300] train: 0.7621 - val: 0.8284\n",
      "[224/300] train: 0.7623 - val: 0.8289\n",
      "[225/300] train: 0.7606 - val: 0.8294\n",
      "[226/300] train: 0.7620 - val: 0.8281\n",
      "[227/300] train: 0.7617 - val: 0.8301\n",
      "[228/300] train: 0.7620 - val: 0.8284\n",
      "[229/300] train: 0.7623 - val: 0.8304\n",
      "[230/300] train: 0.7619 - val: 0.8288\n",
      "[231/300] train: 0.7615 - val: 0.8315\n",
      "[232/300] train: 0.7618 - val: 0.8284\n",
      "[233/300] train: 0.7612 - val: 0.8286\n",
      "[234/300] train: 0.7620 - val: 0.8294\n",
      "[235/300] train: 0.7609 - val: 0.8303\n",
      "[236/300] train: 0.7621 - val: 0.8290\n",
      "[237/300] train: 0.7616 - val: 0.8314\n",
      "[238/300] train: 0.7611 - val: 0.8291\n",
      "[239/300] train: 0.7618 - val: 0.8285\n",
      "[240/300] train: 0.7626 - val: 0.8310\n",
      "[241/300] train: 0.7614 - val: 0.8296\n",
      "[242/300] train: 0.7611 - val: 0.8295\n",
      "[243/300] train: 0.7623 - val: 0.8294\n",
      "[244/300] train: 0.7609 - val: 0.8300\n",
      "[245/300] train: 0.7609 - val: 0.8283\n",
      "[246/300] train: 0.7611 - val: 0.8303\n",
      "[247/300] train: 0.7617 - val: 0.8295\n",
      "[248/300] train: 0.7600 - val: 0.8300\n",
      "[249/300] train: 0.7614 - val: 0.8294\n",
      "[250/300] train: 0.7614 - val: 0.8313\n",
      "[251/300] train: 0.7612 - val: 0.8290\n",
      "[252/300] train: 0.7619 - val: 0.8309\n",
      "[253/300] train: 0.7601 - val: 0.8298\n",
      "[254/300] train: 0.7618 - val: 0.8299\n",
      "[255/300] train: 0.7614 - val: 0.8303\n",
      "[256/300] train: 0.7610 - val: 0.8294\n",
      "[257/300] train: 0.7611 - val: 0.8311\n",
      "[258/300] train: 0.7604 - val: 0.8299\n",
      "[259/300] train: 0.7611 - val: 0.8298\n",
      "[260/300] train: 0.7600 - val: 0.8283\n",
      "[261/300] train: 0.7614 - val: 0.8294\n",
      "[262/300] train: 0.7615 - val: 0.8284\n",
      "[263/300] train: 0.7608 - val: 0.8305\n",
      "[264/300] train: 0.7604 - val: 0.8294\n",
      "[265/300] train: 0.7609 - val: 0.8311\n",
      "[266/300] train: 0.7613 - val: 0.8292\n",
      "[267/300] train: 0.7604 - val: 0.8309\n",
      "[268/300] train: 0.7610 - val: 0.8275\n",
      "[269/300] train: 0.7606 - val: 0.8313\n",
      "[270/300] train: 0.7599 - val: 0.8310\n",
      "[271/300] train: 0.7611 - val: 0.8295\n",
      "[272/300] train: 0.7611 - val: 0.8306\n",
      "[273/300] train: 0.7607 - val: 0.8311\n",
      "[274/300] train: 0.7598 - val: 0.8302\n",
      "[275/300] train: 0.7605 - val: 0.8292\n",
      "[276/300] train: 0.7613 - val: 0.8296\n",
      "[277/300] train: 0.7613 - val: 0.8306\n",
      "[278/300] train: 0.7609 - val: 0.8312\n",
      "[279/300] train: 0.7615 - val: 0.8293\n",
      "[280/300] train: 0.7591 - val: 0.8300\n",
      "[281/300] train: 0.7606 - val: 0.8311\n",
      "[282/300] train: 0.7603 - val: 0.8292\n",
      "[283/300] train: 0.7611 - val: 0.8299\n",
      "[284/300] train: 0.7601 - val: 0.8282\n",
      "[285/300] train: 0.7607 - val: 0.8308\n",
      "[286/300] train: 0.7604 - val: 0.8290\n",
      "[287/300] train: 0.7602 - val: 0.8302\n",
      "[288/300] train: 0.7607 - val: 0.8297\n",
      "[289/300] train: 0.7594 - val: 0.8289\n",
      "[290/300] train: 0.7611 - val: 0.8318\n",
      "[291/300] train: 0.7612 - val: 0.8297\n",
      "[292/300] train: 0.7606 - val: 0.8299\n",
      "[293/300] train: 0.7607 - val: 0.8294\n",
      "[294/300] train: 0.7596 - val: 0.8320\n",
      "[295/300] train: 0.7604 - val: 0.8282\n",
      "[296/300] train: 0.7599 - val: 0.8302\n",
      "[297/300] train: 0.7604 - val: 0.8294\n",
      "[298/300] train: 0.7605 - val: 0.8289\n",
      "[299/300] train: 0.7602 - val: 0.8296\n",
      "[300/300] train: 0.7598 - val: 0.8298\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    #scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #lr_history.extend(scheduler.get_lr())\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# Plot decreasing learning rate\n",
    "\n",
    "#_ = plt.plot(lr_history[:2*iterations_per_epoch-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.9147\n"
     ]
    }
   ],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.9169\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "with open('best.weights.small', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 254.2543\n"
     ]
    }
   ],
   "source": [
    "print(f'duration: {round(time.time() - start, 4)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# medium net\n",
    "\n",
    "start = time.time()\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_movies=m,\n",
    "    n_factors=20, hidden=[10 ,10],\n",
    "    embedding_dropout=0.05, dropouts=[0.3, 0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "#iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "#scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 1.1055 - val: 0.9449\n",
      "loss improvement on epoch: 2\n",
      "[002/300] train: 0.9220 - val: 0.8881\n",
      "loss improvement on epoch: 3\n",
      "[003/300] train: 0.8831 - val: 0.8725\n",
      "loss improvement on epoch: 4\n",
      "[004/300] train: 0.8654 - val: 0.8633\n",
      "loss improvement on epoch: 5\n",
      "[005/300] train: 0.8539 - val: 0.8575\n",
      "loss improvement on epoch: 6\n",
      "[006/300] train: 0.8468 - val: 0.8554\n",
      "loss improvement on epoch: 7\n",
      "[007/300] train: 0.8427 - val: 0.8545\n",
      "loss improvement on epoch: 8\n",
      "[008/300] train: 0.8384 - val: 0.8521\n",
      "[009/300] train: 0.8363 - val: 0.8537\n",
      "[010/300] train: 0.8328 - val: 0.8527\n",
      "loss improvement on epoch: 11\n",
      "[011/300] train: 0.8297 - val: 0.8517\n",
      "[012/300] train: 0.8286 - val: 0.8525\n",
      "[013/300] train: 0.8265 - val: 0.8537\n",
      "[014/300] train: 0.8250 - val: 0.8544\n",
      "[015/300] train: 0.8234 - val: 0.8539\n",
      "[016/300] train: 0.8219 - val: 0.8536\n",
      "[017/300] train: 0.8200 - val: 0.8546\n",
      "[018/300] train: 0.8188 - val: 0.8538\n",
      "[019/300] train: 0.8173 - val: 0.8531\n",
      "[020/300] train: 0.8160 - val: 0.8543\n",
      "[021/300] train: 0.8154 - val: 0.8536\n",
      "[022/300] train: 0.8147 - val: 0.8533\n",
      "[023/300] train: 0.8130 - val: 0.8542\n",
      "[024/300] train: 0.8123 - val: 0.8552\n",
      "[025/300] train: 0.8111 - val: 0.8533\n",
      "[026/300] train: 0.8111 - val: 0.8541\n",
      "[027/300] train: 0.8099 - val: 0.8549\n",
      "[028/300] train: 0.8101 - val: 0.8529\n",
      "[029/300] train: 0.8085 - val: 0.8550\n",
      "[030/300] train: 0.8079 - val: 0.8554\n",
      "[031/300] train: 0.8073 - val: 0.8526\n",
      "[032/300] train: 0.8057 - val: 0.8555\n",
      "[033/300] train: 0.8046 - val: 0.8556\n",
      "[034/300] train: 0.8044 - val: 0.8559\n",
      "[035/300] train: 0.8060 - val: 0.8541\n",
      "[036/300] train: 0.8041 - val: 0.8562\n",
      "[037/300] train: 0.8039 - val: 0.8559\n",
      "[038/300] train: 0.8029 - val: 0.8544\n",
      "[039/300] train: 0.8025 - val: 0.8566\n",
      "[040/300] train: 0.8023 - val: 0.8556\n",
      "[041/300] train: 0.8014 - val: 0.8549\n",
      "[042/300] train: 0.8008 - val: 0.8568\n",
      "[043/300] train: 0.8016 - val: 0.8565\n",
      "[044/300] train: 0.8005 - val: 0.8548\n",
      "[045/300] train: 0.8003 - val: 0.8561\n",
      "[046/300] train: 0.8000 - val: 0.8551\n",
      "[047/300] train: 0.7987 - val: 0.8596\n",
      "[048/300] train: 0.7996 - val: 0.8560\n",
      "[049/300] train: 0.7991 - val: 0.8566\n",
      "[050/300] train: 0.7980 - val: 0.8588\n",
      "[051/300] train: 0.7990 - val: 0.8564\n",
      "[052/300] train: 0.7992 - val: 0.8579\n",
      "[053/300] train: 0.7978 - val: 0.8607\n",
      "[054/300] train: 0.7976 - val: 0.8583\n",
      "[055/300] train: 0.7967 - val: 0.8596\n",
      "[056/300] train: 0.7968 - val: 0.8581\n",
      "[057/300] train: 0.7947 - val: 0.8570\n",
      "[058/300] train: 0.7960 - val: 0.8596\n",
      "[059/300] train: 0.7954 - val: 0.8587\n",
      "[060/300] train: 0.7949 - val: 0.8580\n",
      "[061/300] train: 0.7949 - val: 0.8589\n",
      "[062/300] train: 0.7946 - val: 0.8594\n",
      "[063/300] train: 0.7954 - val: 0.8583\n",
      "[064/300] train: 0.7942 - val: 0.8571\n",
      "[065/300] train: 0.7936 - val: 0.8608\n",
      "[066/300] train: 0.7934 - val: 0.8597\n",
      "[067/300] train: 0.7927 - val: 0.8591\n",
      "[068/300] train: 0.7934 - val: 0.8597\n",
      "[069/300] train: 0.7931 - val: 0.8588\n",
      "[070/300] train: 0.7926 - val: 0.8616\n",
      "[071/300] train: 0.7919 - val: 0.8600\n",
      "[072/300] train: 0.7923 - val: 0.8585\n",
      "[073/300] train: 0.7916 - val: 0.8596\n",
      "[074/300] train: 0.7925 - val: 0.8601\n",
      "[075/300] train: 0.7923 - val: 0.8578\n",
      "[076/300] train: 0.7923 - val: 0.8602\n",
      "[077/300] train: 0.7921 - val: 0.8600\n",
      "[078/300] train: 0.7911 - val: 0.8614\n",
      "[079/300] train: 0.7902 - val: 0.8615\n",
      "[080/300] train: 0.7904 - val: 0.8610\n",
      "[081/300] train: 0.7911 - val: 0.8591\n",
      "[082/300] train: 0.7906 - val: 0.8595\n",
      "[083/300] train: 0.7903 - val: 0.8594\n",
      "[084/300] train: 0.7903 - val: 0.8592\n",
      "[085/300] train: 0.7894 - val: 0.8602\n",
      "[086/300] train: 0.7889 - val: 0.8616\n",
      "[087/300] train: 0.7899 - val: 0.8599\n",
      "[088/300] train: 0.7905 - val: 0.8613\n",
      "[089/300] train: 0.7901 - val: 0.8608\n",
      "[090/300] train: 0.7882 - val: 0.8622\n",
      "[091/300] train: 0.7882 - val: 0.8619\n",
      "[092/300] train: 0.7893 - val: 0.8625\n",
      "[093/300] train: 0.7890 - val: 0.8584\n",
      "[094/300] train: 0.7882 - val: 0.8611\n",
      "[095/300] train: 0.7889 - val: 0.8622\n",
      "[096/300] train: 0.7885 - val: 0.8614\n",
      "[097/300] train: 0.7876 - val: 0.8600\n",
      "[098/300] train: 0.7868 - val: 0.8632\n",
      "[099/300] train: 0.7896 - val: 0.8600\n",
      "[100/300] train: 0.7886 - val: 0.8582\n",
      "[101/300] train: 0.7879 - val: 0.8629\n",
      "[102/300] train: 0.7869 - val: 0.8624\n",
      "[103/300] train: 0.7870 - val: 0.8636\n",
      "[104/300] train: 0.7880 - val: 0.8638\n",
      "[105/300] train: 0.7878 - val: 0.8635\n",
      "[106/300] train: 0.7866 - val: 0.8634\n",
      "[107/300] train: 0.7866 - val: 0.8641\n",
      "[108/300] train: 0.7862 - val: 0.8627\n",
      "[109/300] train: 0.7869 - val: 0.8630\n",
      "[110/300] train: 0.7867 - val: 0.8615\n",
      "[111/300] train: 0.7865 - val: 0.8628\n",
      "[112/300] train: 0.7860 - val: 0.8627\n",
      "[113/300] train: 0.7865 - val: 0.8596\n",
      "[114/300] train: 0.7867 - val: 0.8610\n",
      "[115/300] train: 0.7856 - val: 0.8614\n",
      "[116/300] train: 0.7851 - val: 0.8614\n",
      "[117/300] train: 0.7856 - val: 0.8604\n",
      "[118/300] train: 0.7848 - val: 0.8603\n",
      "[119/300] train: 0.7847 - val: 0.8630\n",
      "[120/300] train: 0.7863 - val: 0.8629\n",
      "[121/300] train: 0.7864 - val: 0.8614\n",
      "[122/300] train: 0.7850 - val: 0.8615\n",
      "[123/300] train: 0.7838 - val: 0.8623\n",
      "[124/300] train: 0.7855 - val: 0.8656\n",
      "[125/300] train: 0.7839 - val: 0.8611\n",
      "[126/300] train: 0.7860 - val: 0.8634\n",
      "[127/300] train: 0.7858 - val: 0.8647\n",
      "[128/300] train: 0.7843 - val: 0.8634\n",
      "[129/300] train: 0.7850 - val: 0.8615\n",
      "[130/300] train: 0.7841 - val: 0.8619\n",
      "[131/300] train: 0.7851 - val: 0.8623\n",
      "[132/300] train: 0.7840 - val: 0.8599\n",
      "[133/300] train: 0.7833 - val: 0.8626\n",
      "[134/300] train: 0.7839 - val: 0.8606\n",
      "[135/300] train: 0.7843 - val: 0.8639\n",
      "[136/300] train: 0.7833 - val: 0.8643\n",
      "[137/300] train: 0.7832 - val: 0.8627\n",
      "[138/300] train: 0.7825 - val: 0.8640\n",
      "[139/300] train: 0.7829 - val: 0.8603\n",
      "[140/300] train: 0.7831 - val: 0.8602\n",
      "[141/300] train: 0.7831 - val: 0.8632\n",
      "[142/300] train: 0.7822 - val: 0.8627\n",
      "[143/300] train: 0.7822 - val: 0.8625\n",
      "[144/300] train: 0.7826 - val: 0.8632\n",
      "[145/300] train: 0.7828 - val: 0.8621\n",
      "[146/300] train: 0.7835 - val: 0.8646\n",
      "[147/300] train: 0.7838 - val: 0.8645\n",
      "[148/300] train: 0.7837 - val: 0.8623\n",
      "[149/300] train: 0.7815 - val: 0.8614\n",
      "[150/300] train: 0.7817 - val: 0.8662\n",
      "[151/300] train: 0.7823 - val: 0.8612\n",
      "[152/300] train: 0.7833 - val: 0.8617\n",
      "[153/300] train: 0.7824 - val: 0.8625\n",
      "[154/300] train: 0.7830 - val: 0.8630\n",
      "[155/300] train: 0.7823 - val: 0.8651\n",
      "[156/300] train: 0.7819 - val: 0.8622\n",
      "[157/300] train: 0.7818 - val: 0.8646\n",
      "[158/300] train: 0.7822 - val: 0.8643\n",
      "[159/300] train: 0.7818 - val: 0.8649\n",
      "[160/300] train: 0.7813 - val: 0.8630\n",
      "[161/300] train: 0.7817 - val: 0.8611\n",
      "[162/300] train: 0.7829 - val: 0.8626\n",
      "[163/300] train: 0.7816 - val: 0.8634\n",
      "[164/300] train: 0.7818 - val: 0.8638\n",
      "[165/300] train: 0.7816 - val: 0.8638\n",
      "[166/300] train: 0.7811 - val: 0.8642\n",
      "[167/300] train: 0.7814 - val: 0.8614\n",
      "[168/300] train: 0.7809 - val: 0.8643\n",
      "[169/300] train: 0.7821 - val: 0.8638\n",
      "[170/300] train: 0.7806 - val: 0.8646\n",
      "[171/300] train: 0.7806 - val: 0.8618\n",
      "[172/300] train: 0.7820 - val: 0.8623\n",
      "[173/300] train: 0.7811 - val: 0.8624\n",
      "[174/300] train: 0.7817 - val: 0.8638\n",
      "[175/300] train: 0.7818 - val: 0.8648\n",
      "[176/300] train: 0.7805 - val: 0.8640\n",
      "[177/300] train: 0.7813 - val: 0.8618\n",
      "[178/300] train: 0.7814 - val: 0.8625\n",
      "[179/300] train: 0.7812 - val: 0.8638\n",
      "[180/300] train: 0.7801 - val: 0.8622\n",
      "[181/300] train: 0.7808 - val: 0.8641\n",
      "[182/300] train: 0.7805 - val: 0.8653\n",
      "[183/300] train: 0.7814 - val: 0.8627\n",
      "[184/300] train: 0.7823 - val: 0.8642\n",
      "[185/300] train: 0.7812 - val: 0.8669\n",
      "[186/300] train: 0.7808 - val: 0.8639\n",
      "[187/300] train: 0.7805 - val: 0.8651\n",
      "[188/300] train: 0.7809 - val: 0.8627\n",
      "[189/300] train: 0.7807 - val: 0.8657\n",
      "[190/300] train: 0.7798 - val: 0.8652\n",
      "[191/300] train: 0.7801 - val: 0.8637\n",
      "[192/300] train: 0.7796 - val: 0.8628\n",
      "[193/300] train: 0.7797 - val: 0.8635\n",
      "[194/300] train: 0.7804 - val: 0.8654\n",
      "[195/300] train: 0.7797 - val: 0.8610\n",
      "[196/300] train: 0.7788 - val: 0.8671\n",
      "[197/300] train: 0.7797 - val: 0.8635\n",
      "[198/300] train: 0.7795 - val: 0.8621\n",
      "[199/300] train: 0.7801 - val: 0.8632\n",
      "[200/300] train: 0.7803 - val: 0.8672\n",
      "[201/300] train: 0.7790 - val: 0.8649\n",
      "[202/300] train: 0.7802 - val: 0.8650\n",
      "[203/300] train: 0.7790 - val: 0.8655\n",
      "[204/300] train: 0.7790 - val: 0.8639\n",
      "[205/300] train: 0.7782 - val: 0.8656\n",
      "[206/300] train: 0.7796 - val: 0.8667\n",
      "[207/300] train: 0.7791 - val: 0.8671\n",
      "[208/300] train: 0.7796 - val: 0.8653\n",
      "[209/300] train: 0.7780 - val: 0.8657\n",
      "[210/300] train: 0.7796 - val: 0.8646\n",
      "[211/300] train: 0.7786 - val: 0.8627\n",
      "[212/300] train: 0.7793 - val: 0.8658\n",
      "[213/300] train: 0.7791 - val: 0.8641\n",
      "[214/300] train: 0.7780 - val: 0.8656\n",
      "[215/300] train: 0.7790 - val: 0.8647\n",
      "[216/300] train: 0.7794 - val: 0.8654\n",
      "[217/300] train: 0.7800 - val: 0.8665\n",
      "[218/300] train: 0.7789 - val: 0.8664\n",
      "[219/300] train: 0.7784 - val: 0.8645\n",
      "[220/300] train: 0.7790 - val: 0.8667\n",
      "[221/300] train: 0.7778 - val: 0.8657\n",
      "[222/300] train: 0.7788 - val: 0.8659\n",
      "[223/300] train: 0.7782 - val: 0.8649\n",
      "[224/300] train: 0.7789 - val: 0.8662\n",
      "[225/300] train: 0.7793 - val: 0.8645\n",
      "[226/300] train: 0.7800 - val: 0.8639\n",
      "[227/300] train: 0.7788 - val: 0.8638\n",
      "[228/300] train: 0.7784 - val: 0.8671\n",
      "[229/300] train: 0.7790 - val: 0.8667\n",
      "[230/300] train: 0.7789 - val: 0.8648\n",
      "[231/300] train: 0.7797 - val: 0.8637\n",
      "[232/300] train: 0.7785 - val: 0.8654\n",
      "[233/300] train: 0.7790 - val: 0.8622\n",
      "[234/300] train: 0.7782 - val: 0.8676\n",
      "[235/300] train: 0.7778 - val: 0.8656\n",
      "[236/300] train: 0.7783 - val: 0.8644\n",
      "[237/300] train: 0.7787 - val: 0.8645\n",
      "[238/300] train: 0.7771 - val: 0.8661\n",
      "[239/300] train: 0.7779 - val: 0.8658\n",
      "[240/300] train: 0.7783 - val: 0.8632\n",
      "[241/300] train: 0.7784 - val: 0.8646\n",
      "[242/300] train: 0.7775 - val: 0.8640\n",
      "[243/300] train: 0.7778 - val: 0.8673\n",
      "[244/300] train: 0.7775 - val: 0.8642\n",
      "[245/300] train: 0.7770 - val: 0.8642\n",
      "[246/300] train: 0.7770 - val: 0.8654\n",
      "[247/300] train: 0.7773 - val: 0.8678\n",
      "[248/300] train: 0.7777 - val: 0.8671\n",
      "[249/300] train: 0.7774 - val: 0.8647\n",
      "[250/300] train: 0.7762 - val: 0.8667\n",
      "[251/300] train: 0.7768 - val: 0.8688\n",
      "[252/300] train: 0.7771 - val: 0.8665\n",
      "[253/300] train: 0.7762 - val: 0.8672\n",
      "[254/300] train: 0.7772 - val: 0.8660\n",
      "[255/300] train: 0.7773 - val: 0.8647\n",
      "[256/300] train: 0.7766 - val: 0.8659\n",
      "[257/300] train: 0.7779 - val: 0.8640\n",
      "[258/300] train: 0.7765 - val: 0.8651\n",
      "[259/300] train: 0.7759 - val: 0.8655\n",
      "[260/300] train: 0.7761 - val: 0.8646\n",
      "[261/300] train: 0.7783 - val: 0.8681\n",
      "[262/300] train: 0.7774 - val: 0.8660\n",
      "[263/300] train: 0.7774 - val: 0.8636\n",
      "[264/300] train: 0.7785 - val: 0.8661\n",
      "[265/300] train: 0.7767 - val: 0.8632\n",
      "[266/300] train: 0.7757 - val: 0.8662\n",
      "[267/300] train: 0.7777 - val: 0.8660\n",
      "[268/300] train: 0.7759 - val: 0.8662\n",
      "[269/300] train: 0.7768 - val: 0.8658\n",
      "[270/300] train: 0.7763 - val: 0.8651\n",
      "[271/300] train: 0.7758 - val: 0.8661\n",
      "[272/300] train: 0.7765 - val: 0.8665\n",
      "[273/300] train: 0.7768 - val: 0.8660\n",
      "[274/300] train: 0.7766 - val: 0.8691\n",
      "[275/300] train: 0.7769 - val: 0.8671\n",
      "[276/300] train: 0.7760 - val: 0.8671\n",
      "[277/300] train: 0.7766 - val: 0.8673\n",
      "[278/300] train: 0.7763 - val: 0.8641\n",
      "[279/300] train: 0.7771 - val: 0.8658\n",
      "[280/300] train: 0.7771 - val: 0.8663\n",
      "[281/300] train: 0.7759 - val: 0.8656\n",
      "[282/300] train: 0.7769 - val: 0.8660\n",
      "[283/300] train: 0.7764 - val: 0.8680\n",
      "[284/300] train: 0.7751 - val: 0.8678\n",
      "[285/300] train: 0.7761 - val: 0.8661\n",
      "[286/300] train: 0.7763 - val: 0.8659\n",
      "[287/300] train: 0.7770 - val: 0.8653\n",
      "[288/300] train: 0.7753 - val: 0.8666\n",
      "[289/300] train: 0.7761 - val: 0.8648\n",
      "[290/300] train: 0.7771 - val: 0.8679\n",
      "[291/300] train: 0.7752 - val: 0.8675\n",
      "[292/300] train: 0.7758 - val: 0.8677\n",
      "[293/300] train: 0.7762 - val: 0.8662\n",
      "[294/300] train: 0.7766 - val: 0.8685\n",
      "[295/300] train: 0.7758 - val: 0.8677\n",
      "[296/300] train: 0.7770 - val: 0.8639\n",
      "[297/300] train: 0.7764 - val: 0.8637\n",
      "[298/300] train: 0.7753 - val: 0.8664\n",
      "[299/300] train: 0.7761 - val: 0.8657\n",
      "[300/300] train: 0.7772 - val: 0.8655\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    #scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #lr_history.extend(scheduler.get_lr())\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# Plot decreasing learning rate\n",
    "\n",
    "#_ = plt.plot(lr_history[:2*iterations_per_epoch-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.9315\n"
     ]
    }
   ],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.9349\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "with open('best.weights.medium', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 278.7559\n"
     ]
    }
   ],
   "source": [
    "print(f'duration: {round(time.time() - start, 4)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "# big net\n",
    "\n",
    "start = time.time()\n",
    "net = RecommenderNet(\n",
    "    n_users=n, n_movies=m,\n",
    "    n_factors=50, hidden=[100, 100, 100],\n",
    "    embedding_dropout=0.05, dropouts=[0.3, 0.3, 0.3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "bs = 4096\n",
    "n_epochs = 300\n",
    "patience = 300\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "#iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "#scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/300] train: 0.9864 - val: 0.8408\n",
      "loss improvement on epoch: 2\n",
      "[002/300] train: 0.8318 - val: 0.8165\n",
      "loss improvement on epoch: 3\n",
      "[003/300] train: 0.8082 - val: 0.8081\n",
      "loss improvement on epoch: 4\n",
      "[004/300] train: 0.7936 - val: 0.8054\n",
      "loss improvement on epoch: 5\n",
      "[005/300] train: 0.7791 - val: 0.7989\n",
      "loss improvement on epoch: 6\n",
      "[006/300] train: 0.7636 - val: 0.7945\n",
      "[007/300] train: 0.7475 - val: 0.7953\n",
      "[008/300] train: 0.7325 - val: 0.7976\n",
      "[009/300] train: 0.7187 - val: 0.8000\n",
      "[010/300] train: 0.7058 - val: 0.8058\n",
      "[011/300] train: 0.6915 - val: 0.8111\n",
      "[012/300] train: 0.6804 - val: 0.8113\n",
      "[013/300] train: 0.6689 - val: 0.8231\n",
      "[014/300] train: 0.6582 - val: 0.8233\n",
      "[015/300] train: 0.6491 - val: 0.8237\n",
      "[016/300] train: 0.6405 - val: 0.8326\n",
      "[017/300] train: 0.6310 - val: 0.8363\n",
      "[018/300] train: 0.6252 - val: 0.8385\n",
      "[019/300] train: 0.6183 - val: 0.8384\n",
      "[020/300] train: 0.6117 - val: 0.8452\n",
      "[021/300] train: 0.6066 - val: 0.8458\n",
      "[022/300] train: 0.6010 - val: 0.8553\n",
      "[023/300] train: 0.5959 - val: 0.8512\n",
      "[024/300] train: 0.5918 - val: 0.8503\n",
      "[025/300] train: 0.5882 - val: 0.8540\n",
      "[026/300] train: 0.5827 - val: 0.8599\n",
      "[027/300] train: 0.5799 - val: 0.8554\n",
      "[028/300] train: 0.5761 - val: 0.8627\n",
      "[029/300] train: 0.5709 - val: 0.8603\n",
      "[030/300] train: 0.5694 - val: 0.8584\n",
      "[031/300] train: 0.5661 - val: 0.8583\n",
      "[032/300] train: 0.5633 - val: 0.8747\n",
      "[033/300] train: 0.5613 - val: 0.8636\n",
      "[034/300] train: 0.5577 - val: 0.8653\n",
      "[035/300] train: 0.5551 - val: 0.8623\n",
      "[036/300] train: 0.5536 - val: 0.8646\n",
      "[037/300] train: 0.5491 - val: 0.8740\n",
      "[038/300] train: 0.5482 - val: 0.8682\n",
      "[039/300] train: 0.5474 - val: 0.8706\n",
      "[040/300] train: 0.5449 - val: 0.8709\n",
      "[041/300] train: 0.5426 - val: 0.8765\n",
      "[042/300] train: 0.5418 - val: 0.8710\n",
      "[043/300] train: 0.5387 - val: 0.8759\n",
      "[044/300] train: 0.5375 - val: 0.8703\n",
      "[045/300] train: 0.5364 - val: 0.8766\n",
      "[046/300] train: 0.5338 - val: 0.8792\n",
      "[047/300] train: 0.5334 - val: 0.8806\n",
      "[048/300] train: 0.5324 - val: 0.8755\n",
      "[049/300] train: 0.5300 - val: 0.8792\n",
      "[050/300] train: 0.5297 - val: 0.8811\n",
      "[051/300] train: 0.5283 - val: 0.8758\n",
      "[052/300] train: 0.5266 - val: 0.8745\n",
      "[053/300] train: 0.5266 - val: 0.8775\n",
      "[054/300] train: 0.5248 - val: 0.8775\n",
      "[055/300] train: 0.5226 - val: 0.8844\n",
      "[056/300] train: 0.5218 - val: 0.8766\n",
      "[057/300] train: 0.5207 - val: 0.8761\n",
      "[058/300] train: 0.5191 - val: 0.8802\n",
      "[059/300] train: 0.5175 - val: 0.8871\n",
      "[060/300] train: 0.5168 - val: 0.8783\n",
      "[061/300] train: 0.5169 - val: 0.8827\n",
      "[062/300] train: 0.5147 - val: 0.8796\n",
      "[063/300] train: 0.5150 - val: 0.8851\n",
      "[064/300] train: 0.5134 - val: 0.8860\n",
      "[065/300] train: 0.5124 - val: 0.8840\n",
      "[066/300] train: 0.5126 - val: 0.8832\n",
      "[067/300] train: 0.5116 - val: 0.8825\n",
      "[068/300] train: 0.5100 - val: 0.8878\n",
      "[069/300] train: 0.5101 - val: 0.8908\n",
      "[070/300] train: 0.5086 - val: 0.8788\n",
      "[071/300] train: 0.5083 - val: 0.8864\n",
      "[072/300] train: 0.5068 - val: 0.8910\n",
      "[073/300] train: 0.5058 - val: 0.8902\n",
      "[074/300] train: 0.5058 - val: 0.8901\n",
      "[075/300] train: 0.5047 - val: 0.8854\n",
      "[076/300] train: 0.5047 - val: 0.8874\n",
      "[077/300] train: 0.5038 - val: 0.8871\n",
      "[078/300] train: 0.5025 - val: 0.8835\n",
      "[079/300] train: 0.5030 - val: 0.8835\n",
      "[080/300] train: 0.5008 - val: 0.8988\n",
      "[081/300] train: 0.5009 - val: 0.8893\n",
      "[082/300] train: 0.5008 - val: 0.8895\n",
      "[083/300] train: 0.4998 - val: 0.8890\n",
      "[084/300] train: 0.4998 - val: 0.8872\n",
      "[085/300] train: 0.4978 - val: 0.8911\n",
      "[086/300] train: 0.4969 - val: 0.8922\n",
      "[087/300] train: 0.4978 - val: 0.8869\n",
      "[088/300] train: 0.4963 - val: 0.8866\n",
      "[089/300] train: 0.4967 - val: 0.8888\n",
      "[090/300] train: 0.4952 - val: 0.8854\n",
      "[091/300] train: 0.4950 - val: 0.8898\n",
      "[092/300] train: 0.4949 - val: 0.8901\n",
      "[093/300] train: 0.4944 - val: 0.8949\n",
      "[094/300] train: 0.4936 - val: 0.8931\n",
      "[095/300] train: 0.4932 - val: 0.8939\n",
      "[096/300] train: 0.4916 - val: 0.8962\n",
      "[097/300] train: 0.4908 - val: 0.8941\n",
      "[098/300] train: 0.4909 - val: 0.8841\n",
      "[099/300] train: 0.4921 - val: 0.8993\n",
      "[100/300] train: 0.4896 - val: 0.8936\n",
      "[101/300] train: 0.4888 - val: 0.9011\n",
      "[102/300] train: 0.4892 - val: 0.8984\n",
      "[103/300] train: 0.4896 - val: 0.8961\n",
      "[104/300] train: 0.4875 - val: 0.8949\n",
      "[105/300] train: 0.4887 - val: 0.8922\n",
      "[106/300] train: 0.4876 - val: 0.9089\n",
      "[107/300] train: 0.4875 - val: 0.8939\n",
      "[108/300] train: 0.4862 - val: 0.8913\n",
      "[109/300] train: 0.4861 - val: 0.8969\n",
      "[110/300] train: 0.4847 - val: 0.8998\n",
      "[111/300] train: 0.4842 - val: 0.8954\n",
      "[112/300] train: 0.4843 - val: 0.9011\n",
      "[113/300] train: 0.4845 - val: 0.9006\n",
      "[114/300] train: 0.4827 - val: 0.8960\n",
      "[115/300] train: 0.4851 - val: 0.8980\n",
      "[116/300] train: 0.4835 - val: 0.8972\n",
      "[117/300] train: 0.4830 - val: 0.8941\n",
      "[118/300] train: 0.4821 - val: 0.8959\n",
      "[119/300] train: 0.4827 - val: 0.8918\n",
      "[120/300] train: 0.4821 - val: 0.9035\n",
      "[121/300] train: 0.4817 - val: 0.8975\n",
      "[122/300] train: 0.4810 - val: 0.9010\n",
      "[123/300] train: 0.4810 - val: 0.8966\n",
      "[124/300] train: 0.4801 - val: 0.9011\n",
      "[125/300] train: 0.4804 - val: 0.9030\n",
      "[126/300] train: 0.4787 - val: 0.9119\n",
      "[127/300] train: 0.4784 - val: 0.9017\n",
      "[128/300] train: 0.4785 - val: 0.8997\n",
      "[129/300] train: 0.4777 - val: 0.8973\n",
      "[130/300] train: 0.4777 - val: 0.9001\n",
      "[131/300] train: 0.4778 - val: 0.9038\n",
      "[132/300] train: 0.4776 - val: 0.8959\n",
      "[133/300] train: 0.4766 - val: 0.8995\n",
      "[134/300] train: 0.4762 - val: 0.8973\n",
      "[135/300] train: 0.4757 - val: 0.9008\n",
      "[136/300] train: 0.4752 - val: 0.8997\n",
      "[137/300] train: 0.4753 - val: 0.9028\n",
      "[138/300] train: 0.4767 - val: 0.8986\n",
      "[139/300] train: 0.4754 - val: 0.8922\n",
      "[140/300] train: 0.4747 - val: 0.9037\n",
      "[141/300] train: 0.4750 - val: 0.9081\n",
      "[142/300] train: 0.4725 - val: 0.9097\n",
      "[143/300] train: 0.4742 - val: 0.9048\n",
      "[144/300] train: 0.4729 - val: 0.9013\n",
      "[145/300] train: 0.4736 - val: 0.8991\n",
      "[146/300] train: 0.4721 - val: 0.8938\n",
      "[147/300] train: 0.4727 - val: 0.9059\n",
      "[148/300] train: 0.4726 - val: 0.9018\n",
      "[149/300] train: 0.4720 - val: 0.9006\n",
      "[150/300] train: 0.4714 - val: 0.9042\n",
      "[151/300] train: 0.4710 - val: 0.9029\n",
      "[152/300] train: 0.4709 - val: 0.9017\n",
      "[153/300] train: 0.4712 - val: 0.9080\n",
      "[154/300] train: 0.4708 - val: 0.9035\n",
      "[155/300] train: 0.4705 - val: 0.9007\n",
      "[156/300] train: 0.4690 - val: 0.9100\n",
      "[157/300] train: 0.4700 - val: 0.9044\n",
      "[158/300] train: 0.4691 - val: 0.9080\n",
      "[159/300] train: 0.4691 - val: 0.9000\n",
      "[160/300] train: 0.4692 - val: 0.9078\n",
      "[161/300] train: 0.4693 - val: 0.8993\n",
      "[162/300] train: 0.4697 - val: 0.9044\n",
      "[163/300] train: 0.4680 - val: 0.9071\n",
      "[164/300] train: 0.4672 - val: 0.9005\n",
      "[165/300] train: 0.4682 - val: 0.9041\n",
      "[166/300] train: 0.4667 - val: 0.9038\n",
      "[167/300] train: 0.4671 - val: 0.9025\n",
      "[168/300] train: 0.4671 - val: 0.9006\n",
      "[169/300] train: 0.4665 - val: 0.9014\n",
      "[170/300] train: 0.4665 - val: 0.9098\n",
      "[171/300] train: 0.4662 - val: 0.9089\n",
      "[172/300] train: 0.4663 - val: 0.9057\n",
      "[173/300] train: 0.4660 - val: 0.9087\n",
      "[174/300] train: 0.4665 - val: 0.9118\n",
      "[175/300] train: 0.4643 - val: 0.9085\n",
      "[176/300] train: 0.4647 - val: 0.9081\n",
      "[177/300] train: 0.4644 - val: 0.9070\n",
      "[178/300] train: 0.4654 - val: 0.9074\n",
      "[179/300] train: 0.4631 - val: 0.9110\n",
      "[180/300] train: 0.4639 - val: 0.9091\n",
      "[181/300] train: 0.4640 - val: 0.9117\n",
      "[182/300] train: 0.4641 - val: 0.9112\n",
      "[183/300] train: 0.4643 - val: 0.9109\n",
      "[184/300] train: 0.4625 - val: 0.9095\n",
      "[185/300] train: 0.4618 - val: 0.9078\n",
      "[186/300] train: 0.4623 - val: 0.9020\n",
      "[187/300] train: 0.4630 - val: 0.9102\n",
      "[188/300] train: 0.4617 - val: 0.9145\n",
      "[189/300] train: 0.4620 - val: 0.9073\n",
      "[190/300] train: 0.4623 - val: 0.9067\n",
      "[191/300] train: 0.4614 - val: 0.9067\n",
      "[192/300] train: 0.4617 - val: 0.9028\n",
      "[193/300] train: 0.4618 - val: 0.9124\n",
      "[194/300] train: 0.4607 - val: 0.9120\n",
      "[195/300] train: 0.4587 - val: 0.9101\n",
      "[196/300] train: 0.4615 - val: 0.9050\n",
      "[197/300] train: 0.4608 - val: 0.9217\n",
      "[198/300] train: 0.4606 - val: 0.9034\n",
      "[199/300] train: 0.4600 - val: 0.9120\n",
      "[200/300] train: 0.4606 - val: 0.9142\n",
      "[201/300] train: 0.4592 - val: 0.9125\n",
      "[202/300] train: 0.4585 - val: 0.9083\n",
      "[203/300] train: 0.4597 - val: 0.9101\n",
      "[204/300] train: 0.4596 - val: 0.9110\n",
      "[205/300] train: 0.4590 - val: 0.9134\n",
      "[206/300] train: 0.4589 - val: 0.9081\n",
      "[207/300] train: 0.4589 - val: 0.9187\n",
      "[208/300] train: 0.4581 - val: 0.9081\n",
      "[209/300] train: 0.4576 - val: 0.9136\n",
      "[210/300] train: 0.4582 - val: 0.9159\n",
      "[211/300] train: 0.4578 - val: 0.9079\n",
      "[212/300] train: 0.4578 - val: 0.9125\n",
      "[213/300] train: 0.4574 - val: 0.9095\n",
      "[214/300] train: 0.4570 - val: 0.9064\n",
      "[215/300] train: 0.4556 - val: 0.9084\n",
      "[216/300] train: 0.4567 - val: 0.9039\n",
      "[217/300] train: 0.4565 - val: 0.9016\n",
      "[218/300] train: 0.4561 - val: 0.9087\n",
      "[219/300] train: 0.4559 - val: 0.9034\n",
      "[220/300] train: 0.4547 - val: 0.9141\n",
      "[221/300] train: 0.4563 - val: 0.9159\n",
      "[222/300] train: 0.4549 - val: 0.9153\n",
      "[223/300] train: 0.4558 - val: 0.9099\n",
      "[224/300] train: 0.4557 - val: 0.9136\n",
      "[225/300] train: 0.4547 - val: 0.9134\n",
      "[226/300] train: 0.4549 - val: 0.9094\n",
      "[227/300] train: 0.4536 - val: 0.9151\n",
      "[228/300] train: 0.4548 - val: 0.9102\n",
      "[229/300] train: 0.4553 - val: 0.9149\n",
      "[230/300] train: 0.4546 - val: 0.9097\n",
      "[231/300] train: 0.4541 - val: 0.9140\n",
      "[232/300] train: 0.4552 - val: 0.9140\n",
      "[233/300] train: 0.4542 - val: 0.9187\n",
      "[234/300] train: 0.4541 - val: 0.9148\n",
      "[235/300] train: 0.4515 - val: 0.9145\n",
      "[236/300] train: 0.4529 - val: 0.9169\n",
      "[237/300] train: 0.4529 - val: 0.9166\n",
      "[238/300] train: 0.4536 - val: 0.9109\n",
      "[239/300] train: 0.4543 - val: 0.9091\n",
      "[240/300] train: 0.4535 - val: 0.9168\n",
      "[241/300] train: 0.4525 - val: 0.9117\n",
      "[242/300] train: 0.4531 - val: 0.9151\n",
      "[243/300] train: 0.4530 - val: 0.9102\n",
      "[244/300] train: 0.4521 - val: 0.9049\n",
      "[245/300] train: 0.4530 - val: 0.9147\n",
      "[246/300] train: 0.4520 - val: 0.9143\n",
      "[247/300] train: 0.4517 - val: 0.9108\n",
      "[248/300] train: 0.4521 - val: 0.9157\n",
      "[249/300] train: 0.4517 - val: 0.9134\n",
      "[250/300] train: 0.4509 - val: 0.9150\n",
      "[251/300] train: 0.4513 - val: 0.9185\n",
      "[252/300] train: 0.4510 - val: 0.9089\n",
      "[253/300] train: 0.4515 - val: 0.9205\n",
      "[254/300] train: 0.4509 - val: 0.9106\n",
      "[255/300] train: 0.4498 - val: 0.9108\n",
      "[256/300] train: 0.4517 - val: 0.9278\n",
      "[257/300] train: 0.4506 - val: 0.9128\n",
      "[258/300] train: 0.4502 - val: 0.9172\n",
      "[259/300] train: 0.4515 - val: 0.9227\n",
      "[260/300] train: 0.4495 - val: 0.9175\n",
      "[261/300] train: 0.4509 - val: 0.9183\n",
      "[262/300] train: 0.4496 - val: 0.9169\n",
      "[263/300] train: 0.4503 - val: 0.9152\n",
      "[264/300] train: 0.4495 - val: 0.9223\n",
      "[265/300] train: 0.4497 - val: 0.9148\n",
      "[266/300] train: 0.4493 - val: 0.9084\n",
      "[267/300] train: 0.4496 - val: 0.9127\n",
      "[268/300] train: 0.4491 - val: 0.9159\n",
      "[269/300] train: 0.4487 - val: 0.9145\n",
      "[270/300] train: 0.4482 - val: 0.9176\n",
      "[271/300] train: 0.4494 - val: 0.9194\n",
      "[272/300] train: 0.4489 - val: 0.9161\n",
      "[273/300] train: 0.4486 - val: 0.9172\n",
      "[274/300] train: 0.4489 - val: 0.9199\n",
      "[275/300] train: 0.4475 - val: 0.9240\n",
      "[276/300] train: 0.4480 - val: 0.9184\n",
      "[277/300] train: 0.4477 - val: 0.9079\n",
      "[278/300] train: 0.4473 - val: 0.9136\n",
      "[279/300] train: 0.4479 - val: 0.9235\n",
      "[280/300] train: 0.4473 - val: 0.9179\n",
      "[281/300] train: 0.4482 - val: 0.9135\n",
      "[282/300] train: 0.4475 - val: 0.9198\n",
      "[283/300] train: 0.4463 - val: 0.9214\n",
      "[284/300] train: 0.4472 - val: 0.9172\n",
      "[285/300] train: 0.4470 - val: 0.9210\n",
      "[286/300] train: 0.4469 - val: 0.9176\n",
      "[287/300] train: 0.4472 - val: 0.9152\n",
      "[288/300] train: 0.4460 - val: 0.9163\n",
      "[289/300] train: 0.4464 - val: 0.9169\n",
      "[290/300] train: 0.4456 - val: 0.9249\n",
      "[291/300] train: 0.4453 - val: 0.9214\n",
      "[292/300] train: 0.4461 - val: 0.9180\n",
      "[293/300] train: 0.4466 - val: 0.9201\n",
      "[294/300] train: 0.4446 - val: 0.9194\n",
      "[295/300] train: 0.4459 - val: 0.9242\n",
      "[296/300] train: 0.4454 - val: 0.9166\n",
      "[297/300] train: 0.4451 - val: 0.9182\n",
      "[298/300] train: 0.4462 - val: 0.9109\n",
      "[299/300] train: 0.4445 - val: 0.9139\n",
      "[300/300] train: 0.4458 - val: 0.9204\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch in batches(*datasets[phase], shuffle=training, bs=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = net(x_batch[:,0], x_batch[:,1], minmax)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    #scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #lr_history.extend(scheduler.get_lr())\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(net.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.DataFrame(history).drop(columns='total').plot(x='epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# Plot decreasing learning rate\n",
    "\n",
    "#_ = plt.plot(lr_history[:2*iterations_per_epoch-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(best_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['val'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.8993\n"
     ]
    }
   ],
   "source": [
    "valid_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Validation RMSE: {valid_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "groud_truth, predictions = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in batches(*datasets['test'], shuffle=False, bs=bs):\n",
    "        x_batch, y_batch = [b.to(device) for b in batch]\n",
    "        outputs = net(x_batch[:, 0], x_batch[:, 1], minmax)\n",
    "        groud_truth.extend(y_batch.tolist())\n",
    "        predictions.extend(outputs.tolist())\n",
    "\n",
    "groud_truth = np.asarray(groud_truth).ravel()\n",
    "predictions = np.asarray(predictions).ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.9013\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.sqrt(np.mean((predictions - groud_truth)**2))\n",
    "print(f'Final RMSE: {final_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "with open('best.weights.big', 'wb') as file:\n",
    "    pickle.dump(best_weights, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 549.0049\n"
     ]
    }
   ],
   "source": [
    "print(f'duration: {round(time.time() - start, 4)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f57757a1",
   "language": "python",
   "display_name": "PyCharm (rs-via-gnn)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}